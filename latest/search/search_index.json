{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"kiara plugin: tabular \u00b6 This package contains a set of commonly used/useful modules, pipelines, types and metadata schemas for Kiara . Description \u00b6 kiara data-types and modules for working with tables and databases. Package content \u00b6 data_types \u00b6 array : An array, in most cases used as a column within a table. database : A database, containing one or several tables. table : Tabular data (table, spreadsheet, data_frame, what have you). module_types \u00b6 load.array : -- n/a -- load.database : -- n/a -- load.table : -- n/a -- parse.date_array : -- n/a -- create.database : -- n/a -- create.table : -- n/a -- query.database : -- n/a -- export.table : Export network data items. table.cut_column : Cut off one column from a table, returning an array. table.merge : Create a table from other tables and/or arrays. query.table : Execute a sql query against an (Arrow) table. kiara_model_types \u00b6 instance.render_instruction.table : -- n/a -- database_metadata : Database and table properties. kiara_table_metadata : File stats. table_metadata : Describes properties for the 'table' data type. kiara_database : -- n/a -- kiara_array : -- n/a -- kiara_table : A wrapper class to manage tabular data in a hopefully memory efficient way. operations \u00b6 create.database.from.csv_file : -- n/a -- create.database.from.csv_file_bundle : -- n/a -- create.database.from.table : -- n/a -- create.table.from.csv_file : -- n/a -- create.table.from.text_file_bundle : -- n/a -- deserialize.array.as.python_object : -- n/a -- deserialize.database.as.python_object : -- n/a -- deserialize.table.as.python_object : -- n/a -- export.table.as.csv_file : -- n/a -- extract.date_array.from.table : Extract a date array from a table column. import.table.from.csv_file : Load a table from a csv file. import.table.from.text_file_bundle : Load a table from a bundle of text files. parse.date_array : -- n/a -- query.database : -- n/a -- query.table : Execute a sql query against an (Arrow) table. table.cut_column : Cut off one column from a table, returning an array. Links \u00b6 Documentation: https://DHARPA-Project.github.io/kiara_plugin.tabular Code: https://github.com/DHARPA-Project/kiara_plugin.tabular","title":"Home"},{"location":"#kiara-plugin-tabular","text":"This package contains a set of commonly used/useful modules, pipelines, types and metadata schemas for Kiara .","title":"kiara plugin: tabular"},{"location":"#description","text":"kiara data-types and modules for working with tables and databases.","title":"Description"},{"location":"#package-content","text":"","title":"Package content"},{"location":"#data_types","text":"array : An array, in most cases used as a column within a table. database : A database, containing one or several tables. table : Tabular data (table, spreadsheet, data_frame, what have you).","title":"data_types"},{"location":"#module_types","text":"load.array : -- n/a -- load.database : -- n/a -- load.table : -- n/a -- parse.date_array : -- n/a -- create.database : -- n/a -- create.table : -- n/a -- query.database : -- n/a -- export.table : Export network data items. table.cut_column : Cut off one column from a table, returning an array. table.merge : Create a table from other tables and/or arrays. query.table : Execute a sql query against an (Arrow) table.","title":"module_types"},{"location":"#kiara_model_types","text":"instance.render_instruction.table : -- n/a -- database_metadata : Database and table properties. kiara_table_metadata : File stats. table_metadata : Describes properties for the 'table' data type. kiara_database : -- n/a -- kiara_array : -- n/a -- kiara_table : A wrapper class to manage tabular data in a hopefully memory efficient way.","title":"kiara_model_types"},{"location":"#operations","text":"create.database.from.csv_file : -- n/a -- create.database.from.csv_file_bundle : -- n/a -- create.database.from.table : -- n/a -- create.table.from.csv_file : -- n/a -- create.table.from.text_file_bundle : -- n/a -- deserialize.array.as.python_object : -- n/a -- deserialize.database.as.python_object : -- n/a -- deserialize.table.as.python_object : -- n/a -- export.table.as.csv_file : -- n/a -- extract.date_array.from.table : Extract a date array from a table column. import.table.from.csv_file : Load a table from a csv file. import.table.from.text_file_bundle : Load a table from a bundle of text files. parse.date_array : -- n/a -- query.database : -- n/a -- query.table : Execute a sql query against an (Arrow) table. table.cut_column : Cut off one column from a table, returning an array.","title":"operations"},{"location":"#links","text":"Documentation: https://DHARPA-Project.github.io/kiara_plugin.tabular Code: https://github.com/DHARPA-Project/kiara_plugin.tabular","title":"Links"},{"location":"SUMMARY/","text":"Home Usage Development Package contents API reference","title":"SUMMARY"},{"location":"usage/","text":"Usage \u00b6 TO BE DONE","title":"Usage"},{"location":"usage/#usage","text":"TO BE DONE","title":"Usage"},{"location":"info/SUMMARY/","text":"data_types module_types kiara_model_types operations","title":"SUMMARY"},{"location":"info/data_types/","text":"array \u00b6 lineage array any qualifier profile(s) -- n/a -- Documentation An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name ArrayType python_module_name kiara_plugin.tabular.data_types.array full_name kiara_plugin.tabular.data_types.array.ArrayType Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraArray python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.KiaraArray database \u00b6 lineage database any qualifier profile(s) -- n/a -- Documentation A database, containing one or several tables. This is backed by a sqlite database file. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name DatabaseType python_module_name kiara_plugin.tabular.data_types.db full_name kiara_plugin.tabular.data_types.db.DatabaseType Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraDatabase python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.KiaraDatabase table \u00b6 lineage table any qualifier profile(s) -- n/a -- Documentation Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the [ KiaraTable ][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: \u2022 KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name TableType python_module_name kiara_plugin.tabular.data_types.table full_name kiara_plugin.tabular.data_types.table.TableType Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraTable python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.KiaraTable","title":"data_types"},{"location":"info/data_types/#kiara_info.data_types.array","text":"lineage array any qualifier profile(s) -- n/a -- Documentation An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name ArrayType python_module_name kiara_plugin.tabular.data_types.array full_name kiara_plugin.tabular.data_types.array.ArrayType Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraArray python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.KiaraArray","title":"array"},{"location":"info/data_types/#kiara_info.data_types.database","text":"lineage database any qualifier profile(s) -- n/a -- Documentation A database, containing one or several tables. This is backed by a sqlite database file. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name DatabaseType python_module_name kiara_plugin.tabular.data_types.db full_name kiara_plugin.tabular.data_types.db.DatabaseType Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraDatabase python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.KiaraDatabase","title":"database"},{"location":"info/data_types/#kiara_info.data_types.table","text":"lineage table any qualifier profile(s) -- n/a -- Documentation Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the [ KiaraTable ][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: \u2022 KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name TableType python_module_name kiara_plugin.tabular.data_types.table full_name kiara_plugin.tabular.data_types.table.TableType Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraTable python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.KiaraTable","title":"table"},{"location":"info/kiara_model_types/","text":"instance.render_instruction.table \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name RenderTableInstruction python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.RenderTableInstruction metadata_schema { \"title\" : \"RenderTableInstruction\" , \"description\" : \"Base class that all models in kiara inherit from.\\n\\nThis class provides utility functions for things l\u2026 \"type\" : \"object\" , \"properties\" : { \"number_of_rows\" : { \"title\" : \"Number Of Rows\" , \"description\" : \"How many rows to display.\" , \"default\" : 20 , \"type\" : \"integer\" }, \"row_offset\" : { \"title\" : \"Row Offset\" , \"description\" : \"From which row to start.\" , \"default\" : 0 , \"type\" : \"integer\" }, \"columns\" : { \"title\" : \"Columns\" , \"description\" : \"Which rows do display.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"additionalProperties\" : false } database_metadata \u00b6 Documentation Database and table properties. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name DatabaseMetadata python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.DatabaseMetadata metadata_schema { \"title\" : \"DatabaseMetadata\" , \"description\" : \"Database and table properties.\" , \"type\" : \"object\" , \"properties\" : { \"tables\" : { \"title\" : \"Tables\" , \"description\" : \"The table schema.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/TableMetadata\" } } }, \"required\" : [ \"tables\" ], \"additionalProperties\" : false , \"definitions\" : { \"ColumnSchema\" : { \"title\" : \"ColumnSchema\" , \"description\" : \"Describes properties of a single column of the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"type_name\" : { \"title\" : \"Type Name\" , \"description\" : \"The type name of the column (backend-specific).\" , \"type\" : \"string\" }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Other metadata for the column.\" , \"type\" : \"object\" } }, \"required\" : [ \"type_name\" ] }, \"TableMetadata\" : { \"title\" : \"TableMetadata\" , \"description\" : \"Describes properties for the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"column_names\" : { \"title\" : \"Column Names\" , \"description\" : \"The name of the columns of the table.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"column_schema\" : { \"title\" : \"Column Schema\" , \"description\" : \"The schema description of the table.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/ColumnSchema\" } }, \"rows\" : { \"title\" : \"Rows\" , \"description\" : \"The number of rows the table contains.\" , \"type\" : \"integer\" }, \"size\" : { \"title\" : \"Size\" , \"description\" : \"The tables size in bytes.\" , \"type\" : \"integer\" } }, \"required\" : [ \"column_names\" , \"column_schema\" , \"rows\" ], \"additionalProperties\" : false } } } kiara_table_metadata \u00b6 Documentation File stats. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name KiaraTableMetadata python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.KiaraTableMetadata metadata_schema { \"title\" : \"KiaraTableMetadata\" , \"description\" : \"File stats.\" , \"type\" : \"object\" , \"properties\" : { \"table\" : { \"title\" : \"Table\" , \"description\" : \"The table schema.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/TableMetadata\" } ] } }, \"required\" : [ \"table\" ], \"additionalProperties\" : false , \"definitions\" : { \"ColumnSchema\" : { \"title\" : \"ColumnSchema\" , \"description\" : \"Describes properties of a single column of the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"type_name\" : { \"title\" : \"Type Name\" , \"description\" : \"The type name of the column (backend-specific).\" , \"type\" : \"string\" }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Other metadata for the column.\" , \"type\" : \"object\" } }, \"required\" : [ \"type_name\" ] }, \"TableMetadata\" : { \"title\" : \"TableMetadata\" , \"description\" : \"Describes properties for the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"column_names\" : { \"title\" : \"Column Names\" , \"description\" : \"The name of the columns of the table.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"column_schema\" : { \"title\" : \"Column Schema\" , \"description\" : \"The schema description of the table.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/ColumnSchema\" } }, \"rows\" : { \"title\" : \"Rows\" , \"description\" : \"The number of rows the table contains.\" , \"type\" : \"integer\" }, \"size\" : { \"title\" : \"Size\" , \"description\" : \"The tables size in bytes.\" , \"type\" : \"integer\" } }, \"required\" : [ \"column_names\" , \"column_schema\" , \"rows\" ], \"additionalProperties\" : false } } } table_metadata \u00b6 Documentation Describes properties for the 'table' data type. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name TableMetadata python_module_name kiara_plugin.tabular.models full_name kiara_plugin.tabular.models.TableMetadata metadata_schema { \"title\" : \"TableMetadata\" , \"description\" : \"Describes properties for the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"column_names\" : { \"title\" : \"Column Names\" , \"description\" : \"The name of the columns of the table.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"column_schema\" : { \"title\" : \"Column Schema\" , \"description\" : \"The schema description of the table.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/ColumnSchema\" } }, \"rows\" : { \"title\" : \"Rows\" , \"description\" : \"The number of rows the table contains.\" , \"type\" : \"integer\" }, \"size\" : { \"title\" : \"Size\" , \"description\" : \"The tables size in bytes.\" , \"type\" : \"integer\" } }, \"required\" : [ \"column_names\" , \"column_schema\" , \"rows\" ], \"additionalProperties\" : false , \"definitions\" : { \"ColumnSchema\" : { \"title\" : \"ColumnSchema\" , \"description\" : \"Describes properties of a single column of the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"type_name\" : { \"title\" : \"Type Name\" , \"description\" : \"The type name of the column (backend-specific).\" , \"type\" : \"string\" }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Other metadata for the column.\" , \"type\" : \"object\" } }, \"required\" : [ \"type_name\" ] } } } kiara_database \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name KiaraDatabase python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.KiaraDatabase metadata_schema { \"title\" : \"KiaraDatabase\" , \"description\" : \"Base class that all models in kiara inherit from.\\n\\nThis class provides utility functions for things l\u2026 \"type\" : \"object\" , \"properties\" : { \"db_file_path\" : { \"title\" : \"Db File Path\" , \"description\" : \"The path to the sqlite database file.\" , \"type\" : \"string\" } }, \"required\" : [ \"db_file_path\" ], \"additionalProperties\" : false } kiara_array \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name KiaraArray python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.KiaraArray metadata_schema { \"title\" : \"KiaraArray\" , \"description\" : \"Base class that all models in kiara inherit from.\\n\\nThis class provides utility functions for things l\u2026 \"type\" : \"object\" , \"properties\" : { \"data_path\" : { \"title\" : \"Data Path\" , \"description\" : \"The path to the (feather) file backing this array.\" , \"type\" : \"string\" } }, \"additionalProperties\" : false } kiara_table \u00b6 Documentation A wrapper class to manage tabular data in a hopefully memory efficient way. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name KiaraTable python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.KiaraTable metadata_schema { \"title\" : \"KiaraTable\" , \"description\" : \"A wrapper class to manage tabular data in a hopefully memory efficient way.\" , \"type\" : \"object\" , \"properties\" : { \"data_path\" : { \"title\" : \"Data Path\" , \"description\" : \"The path to the (feather) file backing this array.\" , \"type\" : \"string\" } }, \"additionalProperties\" : false }","title":"kiara_model_types"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.instance.render_instruction.table","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name RenderTableInstruction python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.RenderTableInstruction metadata_schema { \"title\" : \"RenderTableInstruction\" , \"description\" : \"Base class that all models in kiara inherit from.\\n\\nThis class provides utility functions for things l\u2026 \"type\" : \"object\" , \"properties\" : { \"number_of_rows\" : { \"title\" : \"Number Of Rows\" , \"description\" : \"How many rows to display.\" , \"default\" : 20 , \"type\" : \"integer\" }, \"row_offset\" : { \"title\" : \"Row Offset\" , \"description\" : \"From which row to start.\" , \"default\" : 0 , \"type\" : \"integer\" }, \"columns\" : { \"title\" : \"Columns\" , \"description\" : \"Which rows do display.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"additionalProperties\" : false }","title":"instance.render_instruction.table"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.database_metadata","text":"Documentation Database and table properties. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name DatabaseMetadata python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.DatabaseMetadata metadata_schema { \"title\" : \"DatabaseMetadata\" , \"description\" : \"Database and table properties.\" , \"type\" : \"object\" , \"properties\" : { \"tables\" : { \"title\" : \"Tables\" , \"description\" : \"The table schema.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/TableMetadata\" } } }, \"required\" : [ \"tables\" ], \"additionalProperties\" : false , \"definitions\" : { \"ColumnSchema\" : { \"title\" : \"ColumnSchema\" , \"description\" : \"Describes properties of a single column of the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"type_name\" : { \"title\" : \"Type Name\" , \"description\" : \"The type name of the column (backend-specific).\" , \"type\" : \"string\" }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Other metadata for the column.\" , \"type\" : \"object\" } }, \"required\" : [ \"type_name\" ] }, \"TableMetadata\" : { \"title\" : \"TableMetadata\" , \"description\" : \"Describes properties for the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"column_names\" : { \"title\" : \"Column Names\" , \"description\" : \"The name of the columns of the table.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"column_schema\" : { \"title\" : \"Column Schema\" , \"description\" : \"The schema description of the table.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/ColumnSchema\" } }, \"rows\" : { \"title\" : \"Rows\" , \"description\" : \"The number of rows the table contains.\" , \"type\" : \"integer\" }, \"size\" : { \"title\" : \"Size\" , \"description\" : \"The tables size in bytes.\" , \"type\" : \"integer\" } }, \"required\" : [ \"column_names\" , \"column_schema\" , \"rows\" ], \"additionalProperties\" : false } } }","title":"database_metadata"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.kiara_table_metadata","text":"Documentation File stats. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name KiaraTableMetadata python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.KiaraTableMetadata metadata_schema { \"title\" : \"KiaraTableMetadata\" , \"description\" : \"File stats.\" , \"type\" : \"object\" , \"properties\" : { \"table\" : { \"title\" : \"Table\" , \"description\" : \"The table schema.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/TableMetadata\" } ] } }, \"required\" : [ \"table\" ], \"additionalProperties\" : false , \"definitions\" : { \"ColumnSchema\" : { \"title\" : \"ColumnSchema\" , \"description\" : \"Describes properties of a single column of the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"type_name\" : { \"title\" : \"Type Name\" , \"description\" : \"The type name of the column (backend-specific).\" , \"type\" : \"string\" }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Other metadata for the column.\" , \"type\" : \"object\" } }, \"required\" : [ \"type_name\" ] }, \"TableMetadata\" : { \"title\" : \"TableMetadata\" , \"description\" : \"Describes properties for the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"column_names\" : { \"title\" : \"Column Names\" , \"description\" : \"The name of the columns of the table.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"column_schema\" : { \"title\" : \"Column Schema\" , \"description\" : \"The schema description of the table.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/ColumnSchema\" } }, \"rows\" : { \"title\" : \"Rows\" , \"description\" : \"The number of rows the table contains.\" , \"type\" : \"integer\" }, \"size\" : { \"title\" : \"Size\" , \"description\" : \"The tables size in bytes.\" , \"type\" : \"integer\" } }, \"required\" : [ \"column_names\" , \"column_schema\" , \"rows\" ], \"additionalProperties\" : false } } }","title":"kiara_table_metadata"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.table_metadata","text":"Documentation Describes properties for the 'table' data type. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name TableMetadata python_module_name kiara_plugin.tabular.models full_name kiara_plugin.tabular.models.TableMetadata metadata_schema { \"title\" : \"TableMetadata\" , \"description\" : \"Describes properties for the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"column_names\" : { \"title\" : \"Column Names\" , \"description\" : \"The name of the columns of the table.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"column_schema\" : { \"title\" : \"Column Schema\" , \"description\" : \"The schema description of the table.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/ColumnSchema\" } }, \"rows\" : { \"title\" : \"Rows\" , \"description\" : \"The number of rows the table contains.\" , \"type\" : \"integer\" }, \"size\" : { \"title\" : \"Size\" , \"description\" : \"The tables size in bytes.\" , \"type\" : \"integer\" } }, \"required\" : [ \"column_names\" , \"column_schema\" , \"rows\" ], \"additionalProperties\" : false , \"definitions\" : { \"ColumnSchema\" : { \"title\" : \"ColumnSchema\" , \"description\" : \"Describes properties of a single column of the 'table' data type.\" , \"type\" : \"object\" , \"properties\" : { \"type_name\" : { \"title\" : \"Type Name\" , \"description\" : \"The type name of the column (backend-specific).\" , \"type\" : \"string\" }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Other metadata for the column.\" , \"type\" : \"object\" } }, \"required\" : [ \"type_name\" ] } } }","title":"table_metadata"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.kiara_database","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name KiaraDatabase python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.KiaraDatabase metadata_schema { \"title\" : \"KiaraDatabase\" , \"description\" : \"Base class that all models in kiara inherit from.\\n\\nThis class provides utility functions for things l\u2026 \"type\" : \"object\" , \"properties\" : { \"db_file_path\" : { \"title\" : \"Db File Path\" , \"description\" : \"The path to the sqlite database file.\" , \"type\" : \"string\" } }, \"required\" : [ \"db_file_path\" ], \"additionalProperties\" : false }","title":"kiara_database"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.kiara_array","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name KiaraArray python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.KiaraArray metadata_schema { \"title\" : \"KiaraArray\" , \"description\" : \"Base class that all models in kiara inherit from.\\n\\nThis class provides utility functions for things l\u2026 \"type\" : \"object\" , \"properties\" : { \"data_path\" : { \"title\" : \"Data Path\" , \"description\" : \"The path to the (feather) file backing this array.\" , \"type\" : \"string\" } }, \"additionalProperties\" : false }","title":"kiara_array"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.kiara_table","text":"Documentation A wrapper class to manage tabular data in a hopefully memory efficient way. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Python class python_class_name KiaraTable python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.KiaraTable metadata_schema { \"title\" : \"KiaraTable\" , \"description\" : \"A wrapper class to manage tabular data in a hopefully memory efficient way.\" , \"type\" : \"object\" , \"properties\" : { \"data_path\" : { \"title\" : \"Data Path\" , \"description\" : \"The path to the (feather) file backing this array.\" , \"type\" : \"string\" } }, \"additionalProperties\" : false }","title":"kiara_table"},{"location":"info/module_types/","text":"load.array \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no serialization_profile string The name of the serialization profile used to serialize yes the source value. target_profile string The profile name of the de-serialization result data. yes value_type string The value type of the actual (unserialized) value. yes Python class python_class_name DeserializeArrayModule python_module_name kiara_plugin.tabular.modules.array full_name kiara_plugin.tabular.modules.array.DeserializeArrayModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : value_type = self . get_config_value( \"value_type\" ) serialized_value = inputs . get_value_obj(value_type) config = inputs . get_value_obj( \"deserialization_config\" ) target_profile = self . get_config_value( \"target_profile\" ) func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . serialized_data, ** _config) outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load.database \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no serialization_profile string The name of the serialization profile used to serialize yes the source value. target_profile string The profile name of the de-serialization result data. yes value_type string The value type of the actual (unserialized) value. yes Python class python_class_name LoadDatabaseFromDiskModule python_module_name kiara_plugin.tabular.modules.db full_name kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : value_type = self . get_config_value( \"value_type\" ) serialized_value = inputs . get_value_obj(value_type) config = inputs . get_value_obj( \"deserialization_config\" ) target_profile = self . get_config_value( \"target_profile\" ) func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . serialized_data, ** _config) outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load.table \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no serialization_profile string The name of the serialization profile used to serialize yes the source value. target_profile string The profile name of the de-serialization result data. yes value_type string The value type of the actual (unserialized) value. yes Python class python_class_name DeserializeTableModule python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.DeserializeTableModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : value_type = self . get_config_value( \"value_type\" ) serialized_value = inputs . get_value_obj(value_type) config = inputs . get_value_obj( \"deserialization_config\" ) target_profile = self . get_config_value( \"target_profile\" ) func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . serialized_data, ** _config) outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 parse.date_array \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 add_inputs boolean If set to 'True', parse options will be available as inputs. no true constants object Value constants for this module. no defaults object Value defaults for this module. no force_non_null boolean If set to 'True', raise an error if any of the strings in the no true array can't be parsed. input_fields array If not empty, only add the fields specified in here to the no module inputs schema. max_index integer The maximum index until whic to parse the string(s). no min_index integer The minimum index from where to start parsing the string(s). no remove_tokens array A list of tokens/characters to replace with a single no white-space before parsing the input. Python class python_class_name ExtractDateModule python_module_name kiara_plugin.tabular.modules.array full_name kiara_plugin.tabular.modules.array.ExtractDateModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap, job_log: JobLog): import polars as pl import pyarrow as pa from dateutil import parser force_non_null: bool = self . get_data_for_field( field_name = \"force_non_null\" , inputs = inputs ) min_pos: Union[ None , int] = self . get_data_for_field( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos: Union[ None , int] = self . get_data_for_field( field_name = \"max_index\" , inputs = inputs ) remove_tokens: Iterable[str] = self . get_data_for_field( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date (_text: str): text = _text if min_pos: try : text = text[min_pos:] # type: ignore except Exception : return None if max_pos: try : text = text[ 0 : max_pos - min_pos] # type: ignore # noqa except Exception : pass if remove_tokens: for t in remove_tokens: text = text . replace(t, \" \" ) try : d_obj = parser . parse(text, fuzzy = True ) except Exception as e: if force_non_null: raise KiaraProcessingException(e) return None if d_obj is None : if force_non_null: raise KiaraProcessingException( f\"Can't parse date from string: { text }\" ) return None return d_obj value = inputs . get_value_obj( \"array\" ) array: KiaraArray = value . data series = pl . Series(name = \"tokens\" , values = array . arrow_array) job_log . add_log( f\"start parsing date for { len(array) } items\" ) result = series . apply(parse_date) job_log . add_log( f\"finished parsing date for { len(array) } items\" ) result_array = result . to_arrow() # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array(result_array) outputs . set_values(date_array = chunked) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 create.database \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no ignore_errors boolean Whether to ignore convert errors and omit the no false failed items. include_source_file_content boolean When including source metadata, whether to also no false include the original raw (string) content. include_source_metadata boolean Whether to include a table with metadata about the no source files. merge_into_single_table boolean Whether to merge all csv files into a single no false table. source_type string The value type of the source value. yes target_type string The value type of the target. yes Python class python_class_name CreateDatabaseModule python_module_name kiara_plugin.tabular.modules.db full_name kiara_plugin.tabular.modules.db.CreateDatabaseModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : source_type = self . get_config_value( \"source_type\" ) target_type = self . get_config_value( \"target_type\" ) func_name = f\"create__{ target_type }__from__{ source_type }\" func = getattr(self, func_name) source_value = inputs . get_value_obj(source_type) signature = inspect . signature(func) if \"optional\" in signature . parameters: optional: Dict[str, Value] = {} op_schemas = {} for field, schema in self . inputs_schema . items(): if field == source_type: continue optional[field] = inputs . get_value_obj(field) op_schemas[field] = schema result = func( source_value = source_value, optional = ValueMapReadOnly( value_items = optional, values_schema = op_schemas ), ) else : result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 create.table \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no ignore_errors boolean Whether to ignore convert errors and omit the failed items. no false source_type string The value type of the source value. yes target_type string The value type of the target. yes Python class python_class_name CreateTableModule python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.CreateTableModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : source_type = self . get_config_value( \"source_type\" ) target_type = self . get_config_value( \"target_type\" ) func_name = f\"create__{ target_type }__from__{ source_type }\" func = getattr(self, func_name) source_value = inputs . get_value_obj(source_type) signature = inspect . signature(func) if \"optional\" in signature . parameters: optional: Dict[str, Value] = {} op_schemas = {} for field, schema in self . inputs_schema . items(): if field == source_type: continue optional[field] = inputs . get_value_obj(field) op_schemas[field] = schema result = func( source_value = source_value, optional = ValueMapReadOnly( value_items = optional, values_schema = op_schemas ), ) else : result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query.database \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no query string The query. no Python class python_class_name QueryDatabaseModule python_module_name kiara_plugin.tabular.modules.db full_name kiara_plugin.tabular.modules.db.QueryDatabaseModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap): import pyarrow as pa database: KiaraDatabase = inputs . get_value_data( \"database\" ) query = self . get_config_value( \"query\" ) if query is None : query = inputs . get_value_data( \"query\" ) # TODO: make this memory efficent result_columns: Dict[str, List[Any]] = {} with database . get_sqlalchemy_engine() . connect() as con: result = con . execute(text(query)) for r in result: for k, v in dict(r) . items(): result_columns . setdefault(k, []) . append(v) table = pa . Table . from_pydict(result_columns) outputs . set_value( \"query_result\" , table) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 export.table \u00b6 Documentation Export network data items. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no source_type string The type of the source data that is going to be exported. yes target_profile string The name of the target profile. Used to distinguish different yes target formats for the same data type. Python class python_class_name ExportTableModule python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.ExportTableModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : target_profile: str = self . get_config_value( \"target_profile\" ) source_type: str = self . get_config_value( \"source_type\" ) export_metadata = inputs . get_value_data( \"export_metadata\" ) source_obj = inputs . get_value_obj(source_type) source = source_obj . data func_name = f\"export__{ source_type }__as__{ target_profile }\" if not hasattr(self, func_name): raise Exception ( f\"Can't export '{ source_type }' value: missing function '{ func_name }' in class '{ self . __class__ . __n\u2026 ) base_path = inputs . get_value_data( \"base_path\" ) if base_path is None : base_path = os . getcwd() name = inputs . get_value_data( \"name\" ) if not name: name = str(source_obj . value_id) func = getattr(self, func_name) # TODO: check signature? base_path = os . path . abspath(base_path) os . makedirs(base_path, exist_ok = True ) result = func(value = source, base_path = base_path, name = name) if isinstance(result, Mapping): result = DataExportResult( ** result) elif isinstance(result, str): result = DataExportResult(files = [result]) if not isinstance(result, DataExportResult): raise KiaraProcessingException( f\"Can't export value: invalid result type '{ type(result) }' from internal method. This is most like\u2026 ) if export_metadata: metadata_file = Path(os . path . join(base_path, f\"{ name }.metadata\" )) value_info = source_obj . create_info() value_json = value_info . json() metadata_file . write_text(value_json) result . files . append(metadata_file . as_posix()) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value( \"export_details\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table.cut_column \u00b6 Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no Python class python_class_name CutColumnModule python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.CutColumnModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : import pyarrow as pa column_name: str = inputs . get_value_data( \"column_name\" ) table_value: Value = inputs . get_value_obj( \"table\" ) table_metadata: KiaraTableMetadata = table_value . get_property_data( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available: raise KiaraProcessingException( f\"Invalid column name '{ column_name }'. Available column names: {', ' . join(available) }\" ) table: pa . Table = table_value . data . arrow_table column = table . column(column_name) outputs . set_value( \"array\" , column) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table.merge \u00b6 Documentation Create a table from other tables and/or arrays. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 column_map object A map describing no constants object Value constants for this module. no defaults object Value defaults for this module. no inputs_schema object A dict describing the inputs for this merge process. yes Python class python_class_name MergeTableModule python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.MergeTableModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap, job_log: JobLog) -> None : import pyarrow as pa inputs_schema: Dict[str, Any] = self . get_config_value( \"inputs_schema\" ) column_map: Dict[str, str] = self . get_config_value( \"column_map\" ) sources = {} for field_name in inputs_schema . keys(): sources[field_name] = inputs . get_value_data(field_name) len_dict = {} arrays = {} column_map_final = dict(column_map) for source_key, table_or_array in sources . items(): if isinstance(table_or_array, KiaraTable): rows = table_or_array . num_rows for name in table_or_array . column_names: array_name = f\"{ source_key }.{ name }\" if column_map and array_name not in column_map . values(): job_log . add_log( f\"Ignoring column '{ name }' of input table '{ source_key }': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column(name) arrays[array_name] = column if not column_map: if name in column_map_final: raise Exception ( f\"Can't merge table, duplicate column name: { name }.\" ) column_map_final[name] = array_name elif isinstance(table_or_array, KiaraArray): if column_map and source_key not in column_map . values(): job_log . add_log( f\"Ignoring array '{ source_key }': not listed in column_map.\" ) continue rows = len(table_or_array) arrays[source_key] = table_or_array . arrow_array if not column_map: if source_key in column_map_final . keys(): raise Exception ( f\"Can't merge table, duplicate column name: { source_key }.\" ) column_map_final[source_key] = source_key else : raise KiaraProcessingException( f\"Can't merge table: invalid type '{ type(table_or_array) }' for source '{ source_key }'.\" ) len_dict[source_key] = rows all_rows = None for source_key, rows in len_dict . items(): if all_rows is None : all_rows = rows else : if all_rows != rows: all_rows = None break if all_rows is None : len_str = \"\" for name, rows in len_dict . items(): len_str = f\" { name } ({ rows })\" raise KiaraProcessingException( f\"Can't merge table, sources have different lengths: { len_str }\" ) column_names = [] columns = [] for column_name, ref in column_map_final . items(): column_names . append(column_name) column = arrays[ref] columns . append(column) table = pa . Table . from_arrays(arrays = columns, names = column_names) outputs . set_value( \"table\" , table) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query.table \u00b6 Documentation Execute a sql query against an (Arrow) table. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no query string The query to execute. If not specified, the user will be able to no provide their own. relation_name string The name the table is referred to in the sql query. If not no \"data\" specified, the user will be able to provide their own. Python class python_class_name QueryTableSQL python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.QueryTableSQL Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : import duckdb if self . get_config_value( \"query\" ) is None : _query: str = inputs . get_value_data( \"query\" ) _relation_name: str = inputs . get_value_data( \"relation_name\" ) else : _query = self . get_config_value( \"query\" ) _relation_name = self . get_config_value( \"relation_name\" ) if _relation_name . upper() in RESERVED_SQL_KEYWORDS: raise KiaraProcessingException( f\"Invalid relation name '{ _relation_name }': this is a reserved sql keyword, please select a differ\u2026 ) _table: KiaraTable = inputs . get_value_data( \"table\" ) rel_from_arrow = duckdb . arrow(_table . arrow_table) result: duckdb . DuckDBPyResult = rel_from_arrow . query(_relation_name, _query) outputs . set_value( \"query_result\" , result . fetch_arrow_table()) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"module_types"},{"location":"info/module_types/#kiara_info.module_types.load.array","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no serialization_profile string The name of the serialization profile used to serialize yes the source value. target_profile string The profile name of the de-serialization result data. yes value_type string The value type of the actual (unserialized) value. yes Python class python_class_name DeserializeArrayModule python_module_name kiara_plugin.tabular.modules.array full_name kiara_plugin.tabular.modules.array.DeserializeArrayModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : value_type = self . get_config_value( \"value_type\" ) serialized_value = inputs . get_value_obj(value_type) config = inputs . get_value_obj( \"deserialization_config\" ) target_profile = self . get_config_value( \"target_profile\" ) func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . serialized_data, ** _config) outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"load.array"},{"location":"info/module_types/#kiara_info.module_types.load.database","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no serialization_profile string The name of the serialization profile used to serialize yes the source value. target_profile string The profile name of the de-serialization result data. yes value_type string The value type of the actual (unserialized) value. yes Python class python_class_name LoadDatabaseFromDiskModule python_module_name kiara_plugin.tabular.modules.db full_name kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : value_type = self . get_config_value( \"value_type\" ) serialized_value = inputs . get_value_obj(value_type) config = inputs . get_value_obj( \"deserialization_config\" ) target_profile = self . get_config_value( \"target_profile\" ) func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . serialized_data, ** _config) outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"load.database"},{"location":"info/module_types/#kiara_info.module_types.load.table","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no serialization_profile string The name of the serialization profile used to serialize yes the source value. target_profile string The profile name of the de-serialization result data. yes value_type string The value type of the actual (unserialized) value. yes Python class python_class_name DeserializeTableModule python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.DeserializeTableModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : value_type = self . get_config_value( \"value_type\" ) serialized_value = inputs . get_value_obj(value_type) config = inputs . get_value_obj( \"deserialization_config\" ) target_profile = self . get_config_value( \"target_profile\" ) func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . serialized_data, ** _config) outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"load.table"},{"location":"info/module_types/#kiara_info.module_types.parse.date_array","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 add_inputs boolean If set to 'True', parse options will be available as inputs. no true constants object Value constants for this module. no defaults object Value defaults for this module. no force_non_null boolean If set to 'True', raise an error if any of the strings in the no true array can't be parsed. input_fields array If not empty, only add the fields specified in here to the no module inputs schema. max_index integer The maximum index until whic to parse the string(s). no min_index integer The minimum index from where to start parsing the string(s). no remove_tokens array A list of tokens/characters to replace with a single no white-space before parsing the input. Python class python_class_name ExtractDateModule python_module_name kiara_plugin.tabular.modules.array full_name kiara_plugin.tabular.modules.array.ExtractDateModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap, job_log: JobLog): import polars as pl import pyarrow as pa from dateutil import parser force_non_null: bool = self . get_data_for_field( field_name = \"force_non_null\" , inputs = inputs ) min_pos: Union[ None , int] = self . get_data_for_field( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos: Union[ None , int] = self . get_data_for_field( field_name = \"max_index\" , inputs = inputs ) remove_tokens: Iterable[str] = self . get_data_for_field( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date (_text: str): text = _text if min_pos: try : text = text[min_pos:] # type: ignore except Exception : return None if max_pos: try : text = text[ 0 : max_pos - min_pos] # type: ignore # noqa except Exception : pass if remove_tokens: for t in remove_tokens: text = text . replace(t, \" \" ) try : d_obj = parser . parse(text, fuzzy = True ) except Exception as e: if force_non_null: raise KiaraProcessingException(e) return None if d_obj is None : if force_non_null: raise KiaraProcessingException( f\"Can't parse date from string: { text }\" ) return None return d_obj value = inputs . get_value_obj( \"array\" ) array: KiaraArray = value . data series = pl . Series(name = \"tokens\" , values = array . arrow_array) job_log . add_log( f\"start parsing date for { len(array) } items\" ) result = series . apply(parse_date) job_log . add_log( f\"finished parsing date for { len(array) } items\" ) result_array = result . to_arrow() # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array(result_array) outputs . set_values(date_array = chunked) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"parse.date_array"},{"location":"info/module_types/#kiara_info.module_types.create.database","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no ignore_errors boolean Whether to ignore convert errors and omit the no false failed items. include_source_file_content boolean When including source metadata, whether to also no false include the original raw (string) content. include_source_metadata boolean Whether to include a table with metadata about the no source files. merge_into_single_table boolean Whether to merge all csv files into a single no false table. source_type string The value type of the source value. yes target_type string The value type of the target. yes Python class python_class_name CreateDatabaseModule python_module_name kiara_plugin.tabular.modules.db full_name kiara_plugin.tabular.modules.db.CreateDatabaseModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : source_type = self . get_config_value( \"source_type\" ) target_type = self . get_config_value( \"target_type\" ) func_name = f\"create__{ target_type }__from__{ source_type }\" func = getattr(self, func_name) source_value = inputs . get_value_obj(source_type) signature = inspect . signature(func) if \"optional\" in signature . parameters: optional: Dict[str, Value] = {} op_schemas = {} for field, schema in self . inputs_schema . items(): if field == source_type: continue optional[field] = inputs . get_value_obj(field) op_schemas[field] = schema result = func( source_value = source_value, optional = ValueMapReadOnly( value_items = optional, values_schema = op_schemas ), ) else : result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"create.database"},{"location":"info/module_types/#kiara_info.module_types.create.table","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no ignore_errors boolean Whether to ignore convert errors and omit the failed items. no false source_type string The value type of the source value. yes target_type string The value type of the target. yes Python class python_class_name CreateTableModule python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.CreateTableModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : source_type = self . get_config_value( \"source_type\" ) target_type = self . get_config_value( \"target_type\" ) func_name = f\"create__{ target_type }__from__{ source_type }\" func = getattr(self, func_name) source_value = inputs . get_value_obj(source_type) signature = inspect . signature(func) if \"optional\" in signature . parameters: optional: Dict[str, Value] = {} op_schemas = {} for field, schema in self . inputs_schema . items(): if field == source_type: continue optional[field] = inputs . get_value_obj(field) op_schemas[field] = schema result = func( source_value = source_value, optional = ValueMapReadOnly( value_items = optional, values_schema = op_schemas ), ) else : result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"create.table"},{"location":"info/module_types/#kiara_info.module_types.query.database","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no query string The query. no Python class python_class_name QueryDatabaseModule python_module_name kiara_plugin.tabular.modules.db full_name kiara_plugin.tabular.modules.db.QueryDatabaseModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap): import pyarrow as pa database: KiaraDatabase = inputs . get_value_data( \"database\" ) query = self . get_config_value( \"query\" ) if query is None : query = inputs . get_value_data( \"query\" ) # TODO: make this memory efficent result_columns: Dict[str, List[Any]] = {} with database . get_sqlalchemy_engine() . connect() as con: result = con . execute(text(query)) for r in result: for k, v in dict(r) . items(): result_columns . setdefault(k, []) . append(v) table = pa . Table . from_pydict(result_columns) outputs . set_value( \"query_result\" , table) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"query.database"},{"location":"info/module_types/#kiara_info.module_types.export.table","text":"Documentation Export network data items. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no source_type string The type of the source data that is going to be exported. yes target_profile string The name of the target profile. Used to distinguish different yes target formats for the same data type. Python class python_class_name ExportTableModule python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.ExportTableModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : target_profile: str = self . get_config_value( \"target_profile\" ) source_type: str = self . get_config_value( \"source_type\" ) export_metadata = inputs . get_value_data( \"export_metadata\" ) source_obj = inputs . get_value_obj(source_type) source = source_obj . data func_name = f\"export__{ source_type }__as__{ target_profile }\" if not hasattr(self, func_name): raise Exception ( f\"Can't export '{ source_type }' value: missing function '{ func_name }' in class '{ self . __class__ . __n\u2026 ) base_path = inputs . get_value_data( \"base_path\" ) if base_path is None : base_path = os . getcwd() name = inputs . get_value_data( \"name\" ) if not name: name = str(source_obj . value_id) func = getattr(self, func_name) # TODO: check signature? base_path = os . path . abspath(base_path) os . makedirs(base_path, exist_ok = True ) result = func(value = source, base_path = base_path, name = name) if isinstance(result, Mapping): result = DataExportResult( ** result) elif isinstance(result, str): result = DataExportResult(files = [result]) if not isinstance(result, DataExportResult): raise KiaraProcessingException( f\"Can't export value: invalid result type '{ type(result) }' from internal method. This is most like\u2026 ) if export_metadata: metadata_file = Path(os . path . join(base_path, f\"{ name }.metadata\" )) value_info = source_obj . create_info() value_json = value_info . json() metadata_file . write_text(value_json) result . files . append(metadata_file . as_posix()) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value( \"export_details\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"export.table"},{"location":"info/module_types/#kiara_info.module_types.table.cut_column","text":"Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no Python class python_class_name CutColumnModule python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.CutColumnModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : import pyarrow as pa column_name: str = inputs . get_value_data( \"column_name\" ) table_value: Value = inputs . get_value_obj( \"table\" ) table_metadata: KiaraTableMetadata = table_value . get_property_data( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available: raise KiaraProcessingException( f\"Invalid column name '{ column_name }'. Available column names: {', ' . join(available) }\" ) table: pa . Table = table_value . data . arrow_table column = table . column(column_name) outputs . set_value( \"array\" , column) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"table.cut_column"},{"location":"info/module_types/#kiara_info.module_types.table.merge","text":"Documentation Create a table from other tables and/or arrays. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 column_map object A map describing no constants object Value constants for this module. no defaults object Value defaults for this module. no inputs_schema object A dict describing the inputs for this merge process. yes Python class python_class_name MergeTableModule python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.MergeTableModule Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap, job_log: JobLog) -> None : import pyarrow as pa inputs_schema: Dict[str, Any] = self . get_config_value( \"inputs_schema\" ) column_map: Dict[str, str] = self . get_config_value( \"column_map\" ) sources = {} for field_name in inputs_schema . keys(): sources[field_name] = inputs . get_value_data(field_name) len_dict = {} arrays = {} column_map_final = dict(column_map) for source_key, table_or_array in sources . items(): if isinstance(table_or_array, KiaraTable): rows = table_or_array . num_rows for name in table_or_array . column_names: array_name = f\"{ source_key }.{ name }\" if column_map and array_name not in column_map . values(): job_log . add_log( f\"Ignoring column '{ name }' of input table '{ source_key }': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column(name) arrays[array_name] = column if not column_map: if name in column_map_final: raise Exception ( f\"Can't merge table, duplicate column name: { name }.\" ) column_map_final[name] = array_name elif isinstance(table_or_array, KiaraArray): if column_map and source_key not in column_map . values(): job_log . add_log( f\"Ignoring array '{ source_key }': not listed in column_map.\" ) continue rows = len(table_or_array) arrays[source_key] = table_or_array . arrow_array if not column_map: if source_key in column_map_final . keys(): raise Exception ( f\"Can't merge table, duplicate column name: { source_key }.\" ) column_map_final[source_key] = source_key else : raise KiaraProcessingException( f\"Can't merge table: invalid type '{ type(table_or_array) }' for source '{ source_key }'.\" ) len_dict[source_key] = rows all_rows = None for source_key, rows in len_dict . items(): if all_rows is None : all_rows = rows else : if all_rows != rows: all_rows = None break if all_rows is None : len_str = \"\" for name, rows in len_dict . items(): len_str = f\" { name } ({ rows })\" raise KiaraProcessingException( f\"Can't merge table, sources have different lengths: { len_str }\" ) column_names = [] columns = [] for column_name, ref in column_map_final . items(): column_names . append(column_name) column = arrays[ref] columns . append(column) table = pa . Table . from_arrays(arrays = columns, names = column_names) outputs . set_value( \"table\" , table) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"table.merge"},{"location":"info/module_types/#kiara_info.module_types.query.table","text":"Documentation Execute a sql query against an (Arrow) table. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Module config schema Field Type Description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants for this module. no defaults object Value defaults for this module. no query string The query to execute. If not specified, the user will be able to no provide their own. relation_name string The name the table is referred to in the sql query. If not no \"data\" specified, the user will be able to provide their own. Python class python_class_name QueryTableSQL python_module_name kiara_plugin.tabular.modules.table full_name kiara_plugin.tabular.modules.table.QueryTableSQL Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: ValueMap) -> None : import duckdb if self . get_config_value( \"query\" ) is None : _query: str = inputs . get_value_data( \"query\" ) _relation_name: str = inputs . get_value_data( \"relation_name\" ) else : _query = self . get_config_value( \"query\" ) _relation_name = self . get_config_value( \"relation_name\" ) if _relation_name . upper() in RESERVED_SQL_KEYWORDS: raise KiaraProcessingException( f\"Invalid relation name '{ _relation_name }': this is a reserved sql keyword, please select a differ\u2026 ) _table: KiaraTable = inputs . get_value_data( \"table\" ) rel_from_arrow = duckdb . arrow(_table . arrow_table) result: duckdb . DuckDBPyResult = rel_from_arrow . query(_relation_name, _query) outputs . set_value( \"query_result\" , result . fetch_arrow_table()) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"query.table"},{"location":"info/operations/","text":"create.database.from.csv_file \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_file csv_file The source value. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value. create.database.from.csv_file_bundle \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_file_bundle csv_file_bundle The source value. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value. create.database.from.table \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The source value. yes -- no default -- table_name string The name of the table in the new database. no imported_table Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value. create.table.from.csv_file \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_file csv_file The source value. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value. create.table.from.text_file_bundle \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 text_file_bundle text_file_bundle The source value. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value. deserialize.array.as.python_object \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value array The value to de-serialize. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_object python_object The de-serialized python object instance. deserialize.database.as.python_object \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value database The value to de-serialize. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_object python_object The de-serialized python object instance. deserialize.table.as.python_object \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table The value to de-serialize. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_object python_object The de-serialized python object instance. export.table.as.csv_file \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The source value. yes -- no default -- base_path string The directory to export the file(s) no -- no default -- to. name string The (base) name of the exported no -- no default -- file(s). export_metadata boolean Whether to also export the value no False metadata. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 export_details dict Details about the exported data/files. extract.date_array.from.table \u00b6 Documentation Extract a date array from a table column. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Extract a date array from a table column. Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table A table. yes -- no default -- column_name string The name of the column to yes -- no default -- extract. parse_date_array.force_non_ boolean If set to 'True', raise an no True null error if any of the strings in the array can't be parsed. parse_date_array.min_index integer The minimum index from no -- no default -- where to start parsing the string(s). parse_date_array.max_index integer The maximum index until no -- no default -- whic to parse the string(s). parse_date_array.remove_tok list A list of no [] ens tokens/characters to replace with a single white-space before parsing the input. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 date_array array The resulting array with items of a date data type. import.table.from.csv_file \u00b6 Documentation Load a table from a csv file. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Load a table from a csv file. Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 path string The local path to the file. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 imported_file file The loaded files. table table The result value. import.table.from.text_file_bundle \u00b6 Documentation Load a table from a bundle of text files. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Load a table from a bundle of text files. Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 path string The local path of the folder to import. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 imported_file_bundle file_bundle The imported file bundle. table table The result value. parse.date_array \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The input array. yes -- no default -- force_non_null boolean If set to 'True', raise an error if any no True of the strings in the array can't be parsed. min_index integer The minimum index from where to start no -- no default -- parsing the string(s). max_index integer The maximum index until whic to parse no -- no default -- the string(s). remove_tokens list A list of tokens/characters to replace no [] with a single white-space before parsing the input. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 date_array array The resulting array with items of a date data type. query.database \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The database to query. yes -- no default -- query string The query to execute. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query_result table The query result. query.table \u00b6 Documentation Execute a sql query against an (Arrow) table. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation Execute a sql query against an (Arrow) table. Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The table to query yes -- no default -- query string The query. yes -- no default -- relation_name string The name the table is referred to in the no data sql query. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query_result table The query result. table.cut_column \u00b6 Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation Cut off one column from a table, returning an array. Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table A table. yes -- no default -- column_name string The name of the column to extract. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The column.","title":"operations"},{"location":"info/operations/#kiara_info.operations.create.database.from.csv_file","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_file csv_file The source value. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value.","title":"create.database.from.csv_file"},{"location":"info/operations/#kiara_info.operations.create.database.from.csv_file_bundle","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_file_bundle csv_file_bundle The source value. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value.","title":"create.database.from.csv_file_bundle"},{"location":"info/operations/#kiara_info.operations.create.database.from.table","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The source value. yes -- no default -- table_name string The name of the table in the new database. no imported_table Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value.","title":"create.database.from.table"},{"location":"info/operations/#kiara_info.operations.create.table.from.csv_file","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_file csv_file The source value. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value.","title":"create.table.from.csv_file"},{"location":"info/operations/#kiara_info.operations.create.table.from.text_file_bundle","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 text_file_bundle text_file_bundle The source value. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value.","title":"create.table.from.text_file_bundle"},{"location":"info/operations/#kiara_info.operations.deserialize.array.as.python_object","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value array The value to de-serialize. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_object python_object The de-serialized python object instance.","title":"deserialize.array.as.python_object"},{"location":"info/operations/#kiara_info.operations.deserialize.database.as.python_object","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value database The value to de-serialize. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_object python_object The de-serialized python object instance.","title":"deserialize.database.as.python_object"},{"location":"info/operations/#kiara_info.operations.deserialize.table.as.python_object","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table The value to de-serialize. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_object python_object The de-serialized python object instance.","title":"deserialize.table.as.python_object"},{"location":"info/operations/#kiara_info.operations.export.table.as.csv_file","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The source value. yes -- no default -- base_path string The directory to export the file(s) no -- no default -- to. name string The (base) name of the exported no -- no default -- file(s). export_metadata boolean Whether to also export the value no False metadata. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 export_details dict Details about the exported data/files.","title":"export.table.as.csv_file"},{"location":"info/operations/#kiara_info.operations.extract.date_array.from.table","text":"Documentation Extract a date array from a table column. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Extract a date array from a table column. Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table A table. yes -- no default -- column_name string The name of the column to yes -- no default -- extract. parse_date_array.force_non_ boolean If set to 'True', raise an no True null error if any of the strings in the array can't be parsed. parse_date_array.min_index integer The minimum index from no -- no default -- where to start parsing the string(s). parse_date_array.max_index integer The maximum index until no -- no default -- whic to parse the string(s). parse_date_array.remove_tok list A list of no [] ens tokens/characters to replace with a single white-space before parsing the input. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 date_array array The resulting array with items of a date data type.","title":"extract.date_array.from.table"},{"location":"info/operations/#kiara_info.operations.import.table.from.csv_file","text":"Documentation Load a table from a csv file. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Load a table from a csv file. Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 path string The local path to the file. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 imported_file file The loaded files. table table The result value.","title":"import.table.from.csv_file"},{"location":"info/operations/#kiara_info.operations.import.table.from.text_file_bundle","text":"Documentation Load a table from a bundle of text files. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Load a table from a bundle of text files. Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 path string The local path of the folder to import. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 imported_file_bundle file_bundle The imported file bundle. table table The result value.","title":"import.table.from.text_file_bundle"},{"location":"info/operations/#kiara_info.operations.parse.date_array","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The input array. yes -- no default -- force_non_null boolean If set to 'True', raise an error if any no True of the strings in the array can't be parsed. min_index integer The minimum index from where to start no -- no default -- parsing the string(s). max_index integer The maximum index until whic to parse no -- no default -- the string(s). remove_tokens list A list of tokens/characters to replace no [] with a single white-space before parsing the input. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 date_array array The resulting array with items of a date data type.","title":"parse.date_array"},{"location":"info/operations/#kiara_info.operations.query.database","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation -- n/a -- Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The database to query. yes -- no default -- query string The query to execute. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query_result table The query result.","title":"query.database"},{"location":"info/operations/#kiara_info.operations.query.table","text":"Documentation Execute a sql query against an (Arrow) table. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation Execute a sql query against an (Arrow) table. Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The table to query yes -- no default -- query string The query. yes -- no default -- relation_name string The name the table is referred to in the no data sql query. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query_result table The query result.","title":"query.table"},{"location":"info/operations/#kiara_info.operations.table.cut_column","text":"Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin.tabular documentation : https://DHARPA-Project.github.io/kiara_plugin.tabular/ Operation details Documentation Cut off one column from a table, returning an array. Inputs field name type description Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table A table. yes -- no default -- column_name string The name of the column to extract. yes -- no default -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The column.","title":"table.cut_column"},{"location":"reference/SUMMARY/","text":"kiara_plugin tabular data_types array db table defaults models db table modules array db table pipelines utils","title":"SUMMARY"},{"location":"reference/kiara_plugin/tabular/__init__/","text":"Top-level package for kiara_plugin.tabular. KIARA_METADATA \u00b6 find_data_types : Union [ Type , Tuple , Callable ] \u00b6 find_model_classes : Union [ Type , Tuple , Callable ] \u00b6 find_modules : Union [ Type , Tuple , Callable ] \u00b6 find_pipelines : Union [ Type , Tuple , Callable ] \u00b6 get_version () \u00b6 Source code in tabular/__init__.py def get_version (): from pkg_resources import DistributionNotFound , get_distribution try : # Change here if project is renamed and does not equal the package name dist_name = __name__ __version__ = get_distribution ( dist_name ) . version except DistributionNotFound : try : version_file = os . path . join ( os . path . dirname ( __file__ ), \"version.txt\" ) if os . path . exists ( version_file ): with open ( version_file , encoding = \"utf-8\" ) as vf : __version__ = vf . read () else : __version__ = \"unknown\" except ( Exception ): pass if __version__ is None : __version__ = \"unknown\" return __version__ Modules \u00b6 data_types special \u00b6 This module contains the value type classes that are used in the kiara_plugin.tabular package. Modules \u00b6 array \u00b6 Classes \u00b6 ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray serialize ( self , data ) \u00b6 Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized Functions \u00b6 store_array ( array_obj , file_name , column_name = 'array' ) \u00b6 Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch ) db \u00b6 Classes \u00b6 DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase serialize ( self , data ) \u00b6 Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized table \u00b6 Classes \u00b6 TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Calculate the size of the value. Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable serialize ( self , data ) \u00b6 Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized defaults \u00b6 Attributes \u00b6 DEFAULT_TABULAR_DATA_CHUNK_SIZE \u00b6 KIARA_PLUGIN_TABULAR_BASE_FOLDER \u00b6 Marker to indicate the base folder for the kiara network module package. KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER \u00b6 Default resources folder for this package. RESERVED_SQL_KEYWORDS \u00b6 SQLALCHEMY_SQLITE_TYPE_MAP : Dict [ Type , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] \u00b6 SQLITE_DATA_TYPE : Tuple [ Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ], ... ] \u00b6 SQLITE_SQLALCHEMY_TYPE_MAP : Dict [ Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ], Type ] \u00b6 SqliteDataType \u00b6 TEMPLATES_FOLDER \u00b6 models special \u00b6 This module contains the metadata (and other) models that are used in the kiara_plugin.tabular package. Those models are convenience wrappers that make it easier for kiara to find, create, manage and version metadata -- but also other type of models -- that is attached to data, as well as kiara modules. Metadata models must be a sub-class of kiara.metadata.MetadataModel . Other models usually sub-class a pydantic BaseModel or implement custom base classes. Classes \u00b6 ColumnSchema ( BaseModel ) pydantic-model \u00b6 Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict ) Attributes \u00b6 metadata : Dict [ str , Any ] pydantic-field \u00b6 Other metadata for the column. type_name : str pydantic-field required \u00b6 The type name of the column (backend-specific). TableMetadata ( KiaraModel ) pydantic-model \u00b6 Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , } Attributes \u00b6 column_names : List [ str ] pydantic-field required \u00b6 The name of the columns of the table. column_schema : Dict [ str , kiara_plugin . tabular . models . ColumnSchema ] pydantic-field required \u00b6 The schema description of the table. rows : int pydantic-field required \u00b6 The number of rows the table contains. size : int pydantic-field \u00b6 The tables size in bytes. Modules \u00b6 db \u00b6 Classes \u00b6 DatabaseMetadata ( ValueMetadata ) pydantic-model \u00b6 Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" ) Attributes \u00b6 tables : Dict [ str , kiara_plugin . tabular . models . TableMetadata ] pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_cid : CID property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector get_sqlalchemy_metadata ( self ) \u00b6 Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj get_sqlalchemy_table ( self , table_name ) \u00b6 Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table SqliteTableSchema ( BaseModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table Attributes \u00b6 columns : Dict [ str , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] pydantic-field required \u00b6 The table columns and their attributes. index_columns : List [ str ] pydantic-field \u00b6 The columns to index nullable_columns : List [ str ] pydantic-field \u00b6 The columns that are nullable. primary_key : str pydantic-field \u00b6 The primary key for this table. unique_columns : List [ str ] pydantic-field \u00b6 The columns that should be marked 'UNIQUE'. Methods \u00b6 create_table ( self , table_name , engine ) \u00b6 Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table create_table_metadata ( self , table_name ) \u00b6 Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table table \u00b6 Classes \u00b6 KiaraArray ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist () KiaraTable ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage tabular data in a hopefully memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a hopefully memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 Return the data as an Apache Arrow Table instance. column_names : Iterable [ str ] property readonly \u00b6 Retrieve the names of all the columns of this table. data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 Return the number of rows in this table. Methods \u00b6 create_table ( data ) classmethod \u00b6 Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] RenderTableInstruction ( RenderInstruction ) pydantic-model \u00b6 Source code in tabular/models/table.py class RenderTableInstruction ( RenderInstruction ): @classmethod def retrieve_source_type ( cls ) -> str : return \"table\" _kiara_model_id = \"instance.render_instruction.table\" number_of_rows : int = Field ( description = \"How many rows to display.\" , default = 20 ) row_offset : int = Field ( description = \"From which row to start.\" , default = 0 ) columns : Optional [ List [ str ]] = Field ( description = \"Which rows do display.\" , default = None ) def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata ) Attributes \u00b6 columns : List [ str ] pydantic-field \u00b6 Which rows do display. number_of_rows : int pydantic-field \u00b6 How many rows to display. row_offset : int pydantic-field \u00b6 From which row to start. render_as__terminal_renderable ( self , value ) \u00b6 Source code in tabular/models/table.py def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata ) retrieve_source_type () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_source_type ( cls ) -> str : return \"table\" modules special \u00b6 Modules \u00b6 array special \u00b6 FORCE_NON_NULL_DOC \u00b6 MAX_INDEX_DOC \u00b6 MIN_INDEX_DOC \u00b6 REMOVE_TOKENS_DOC \u00b6 Classes \u00b6 DeserializeArrayModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array ExtractDateConfig ( KiaraInputsConfig ) pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. ExtractDateModule ( AutoInputsKiaraModule ) \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) Classes \u00b6 _config_cls ( KiaraInputsConfig ) private pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) db special \u00b6 Classes \u00b6 CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. create__database__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__table ( self , source_value , optional ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db create_optional_inputs ( self , source_type , target_type ) \u00b6 Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db QueryDatabaseConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. QueryDatabaseModule ( KiaraModule ) \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) table special \u00b6 EMPTY_COLUMN_NAME_MARKER \u00b6 Classes \u00b6 CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. create__table__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__table__from__text_file_bundle ( self , source_value ) \u00b6 Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) DeserializeTableModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table ExportTableModule ( DataExportModule ) \u00b6 Export network data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export network data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path} export__table__as__csv_file ( self , value , base_path , name ) \u00b6 Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } MergeTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. MergeTableModule ( KiaraModule ) \u00b6 Create a table from other tables and/or arrays. Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays.\"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) QueryTableSQL ( KiaraModule ) \u00b6 Execute a sql query against an (Arrow) table. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table.\"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ()) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ()) QueryTableSQLModuleConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. pipelines special \u00b6 Default (empty) module that is used as a base path for pipelines contained in this package. utils \u00b6 Functions \u00b6 convert_arrow_column_types_to_sqlite ( table ) \u00b6 Source code in tabular/utils.py def convert_arrow_column_types_to_sqlite ( table : \"pa.Table\" , ) -> Dict [ str , SqliteDataType ]: result : Dict [ str , SqliteDataType ] = {} for column_name in table . column_names : field = table . field ( column_name ) sqlite_type = convert_arrow_type_to_sqlite ( str ( field . type )) result [ column_name ] = sqlite_type return result convert_arrow_type_to_sqlite ( data_type ) \u00b6 Source code in tabular/utils.py def convert_arrow_type_to_sqlite ( data_type : str ) -> SqliteDataType : if data_type . startswith ( \"int\" ) or data_type . startswith ( \"uint\" ): return \"INTEGER\" if ( data_type . startswith ( \"float\" ) or data_type . startswith ( \"decimal\" ) or data_type . startswith ( \"double\" ) ): return \"REAL\" if data_type . startswith ( \"time\" ) or data_type . startswith ( \"date\" ): return \"TEXT\" if data_type == \"bool\" : return \"INTEGER\" if data_type in [ \"string\" , \"utf8\" , \"large_string\" , \"large_utf8\" ]: return \"TEXT\" if data_type in [ \"binary\" , \"large_binary\" ]: return \"BLOB\" raise Exception ( f \"Can't convert to sqlite type: { data_type } \" ) create_sqlite_schema_data_from_arrow_table ( table , column_map = None , index_columns = None , nullable_columns = None , unique_columns = None , primary_key = None ) \u00b6 Create a sql schema statement from an Arrow table object. Parameters: Name Type Description Default table pa.Table the Arrow table object required column_map Optional[Mapping[str, str]] a map that contains column names that should be changed in the new table None index_columns Optional[Iterable[str]] a list of column names (after mapping) to create module_indexes for None extra_column_info a list of extra schema instructions per column name (after mapping) required Source code in tabular/utils.py def create_sqlite_schema_data_from_arrow_table ( table : \"pa.Table\" , column_map : Optional [ Mapping [ str , str ]] = None , index_columns : Optional [ Iterable [ str ]] = None , nullable_columns : Optional [ Iterable [ str ]] = None , unique_columns : Optional [ Iterable [ str ]] = None , primary_key : Optional [ str ] = None , ) -> SqliteTableSchema : \"\"\"Create a sql schema statement from an Arrow table object. Arguments: table: the Arrow table object column_map: a map that contains column names that should be changed in the new table index_columns: a list of column names (after mapping) to create module_indexes for extra_column_info: a list of extra schema instructions per column name (after mapping) \"\"\" columns = convert_arrow_column_types_to_sqlite ( table = table ) if column_map is None : column_map = {} temp : Dict [ str , SqliteDataType ] = {} if index_columns is None : index_columns = [] if nullable_columns is None : nullable_columns = [] if unique_columns is None : unique_columns = [] for cn , sqlite_data_type in columns . items (): if cn in column_map . keys (): new_key = column_map [ cn ] index_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in index_columns ] unique_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in unique_columns ] nullable_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in nullable_columns ] else : new_key = cn temp [ new_key ] = sqlite_data_type columns = temp if not columns : raise Exception ( \"Resulting table schema has no columns.\" ) else : for ic in index_columns : if ic not in columns . keys (): raise Exception ( f \"Can't create schema, requested index column name not available: { ic } \" ) schema = SqliteTableSchema ( columns = columns , index_columns = index_columns , nullable_columns = nullable_columns , unique_columns = unique_columns , primary_key = primary_key , ) return schema create_sqlite_table_from_tabular_file ( target_db_file , file_item , table_name = None , is_csv = True , is_tsv = False , is_nl = False , primary_key_column_names = None , flatten_nested_json_objects = False , csv_delimiter = None , quotechar = None , sniff = True , no_headers = False , encoding = 'utf-8' , batch_size = 100 , detect_types = True ) \u00b6 Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : str = None , quotechar : str = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension with open ( file_item . path , \"rb\" ) as f : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , ) insert_db_table_from_file_bundle ( database , file_bundle , table_name = 'file_items' , include_content = True ) \u00b6 Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import ( Column , DateTime , Integer , MetaData , String , Table , Text , insert , ) from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"import_time\" , DateTime (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"import_time\" : f . import_time , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"tabular"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.KIARA_METADATA","text":"","title":"KIARA_METADATA"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_data_types","text":"","title":"find_data_types"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_model_classes","text":"","title":"find_model_classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_modules","text":"","title":"find_modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_pipelines","text":"","title":"find_pipelines"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.get_version","text":"Source code in tabular/__init__.py def get_version (): from pkg_resources import DistributionNotFound , get_distribution try : # Change here if project is renamed and does not equal the package name dist_name = __name__ __version__ = get_distribution ( dist_name ) . version except DistributionNotFound : try : version_file = os . path . join ( os . path . dirname ( __file__ ), \"version.txt\" ) if os . path . exists ( version_file ): with open ( version_file , encoding = \"utf-8\" ) as vf : __version__ = vf . read () else : __version__ = \"unknown\" except ( Exception ): pass if __version__ is None : __version__ = \"unknown\" return __version__","title":"get_version()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types","text":"This module contains the value type classes that are used in the kiara_plugin.tabular package.","title":"data_types"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.array","text":"","title":"array"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.array-classes","text":"ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray serialize ( self , data ) \u00b6 Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.array-functions","text":"store_array ( array_obj , file_name , column_name = 'array' ) \u00b6 Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch )","title":"Functions"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.db-classes","text":"DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase serialize ( self , data ) \u00b6 Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.table-classes","text":"TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Calculate the size of the value. Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable serialize ( self , data ) \u00b6 Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults","text":"","title":"defaults"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.DEFAULT_TABULAR_DATA_CHUNK_SIZE","text":"","title":"DEFAULT_TABULAR_DATA_CHUNK_SIZE"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.KIARA_PLUGIN_TABULAR_BASE_FOLDER","text":"Marker to indicate the base folder for the kiara network module package.","title":"KIARA_PLUGIN_TABULAR_BASE_FOLDER"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER","text":"Default resources folder for this package.","title":"KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.RESERVED_SQL_KEYWORDS","text":"","title":"RESERVED_SQL_KEYWORDS"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.SQLALCHEMY_SQLITE_TYPE_MAP","text":"","title":"SQLALCHEMY_SQLITE_TYPE_MAP"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.SQLITE_DATA_TYPE","text":"","title":"SQLITE_DATA_TYPE"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.SQLITE_SQLALCHEMY_TYPE_MAP","text":"","title":"SQLITE_SQLALCHEMY_TYPE_MAP"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.SqliteDataType","text":"","title":"SqliteDataType"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.TEMPLATES_FOLDER","text":"","title":"TEMPLATES_FOLDER"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models","text":"This module contains the metadata (and other) models that are used in the kiara_plugin.tabular package. Those models are convenience wrappers that make it easier for kiara to find, create, manage and version metadata -- but also other type of models -- that is attached to data, as well as kiara modules. Metadata models must be a sub-class of kiara.metadata.MetadataModel . Other models usually sub-class a pydantic BaseModel or implement custom base classes.","title":"models"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.ColumnSchema","text":"Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict )","title":"ColumnSchema"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.ColumnSchema-attributes","text":"metadata : Dict [ str , Any ] pydantic-field \u00b6 Other metadata for the column. type_name : str pydantic-field required \u00b6 The type name of the column (backend-specific).","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.TableMetadata","text":"Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , }","title":"TableMetadata"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.TableMetadata-attributes","text":"column_names : List [ str ] pydantic-field required \u00b6 The name of the columns of the table. column_schema : Dict [ str , kiara_plugin . tabular . models . ColumnSchema ] pydantic-field required \u00b6 The schema description of the table. rows : int pydantic-field required \u00b6 The number of rows the table contains. size : int pydantic-field \u00b6 The tables size in bytes.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.db-classes","text":"DatabaseMetadata ( ValueMetadata ) pydantic-model \u00b6 Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" ) Attributes \u00b6 tables : Dict [ str , kiara_plugin . tabular . models . TableMetadata ] pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_cid : CID property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector get_sqlalchemy_metadata ( self ) \u00b6 Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj get_sqlalchemy_table ( self , table_name ) \u00b6 Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table SqliteTableSchema ( BaseModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table Attributes \u00b6 columns : Dict [ str , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] pydantic-field required \u00b6 The table columns and their attributes. index_columns : List [ str ] pydantic-field \u00b6 The columns to index nullable_columns : List [ str ] pydantic-field \u00b6 The columns that are nullable. primary_key : str pydantic-field \u00b6 The primary key for this table. unique_columns : List [ str ] pydantic-field \u00b6 The columns that should be marked 'UNIQUE'. Methods \u00b6 create_table ( self , table_name , engine ) \u00b6 Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table create_table_metadata ( self , table_name ) \u00b6 Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.table-classes","text":"KiaraArray ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist () KiaraTable ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage tabular data in a hopefully memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a hopefully memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 Return the data as an Apache Arrow Table instance. column_names : Iterable [ str ] property readonly \u00b6 Retrieve the names of all the columns of this table. data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 Return the number of rows in this table. Methods \u00b6 create_table ( data ) classmethod \u00b6 Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] RenderTableInstruction ( RenderInstruction ) pydantic-model \u00b6 Source code in tabular/models/table.py class RenderTableInstruction ( RenderInstruction ): @classmethod def retrieve_source_type ( cls ) -> str : return \"table\" _kiara_model_id = \"instance.render_instruction.table\" number_of_rows : int = Field ( description = \"How many rows to display.\" , default = 20 ) row_offset : int = Field ( description = \"From which row to start.\" , default = 0 ) columns : Optional [ List [ str ]] = Field ( description = \"Which rows do display.\" , default = None ) def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata ) Attributes \u00b6 columns : List [ str ] pydantic-field \u00b6 Which rows do display. number_of_rows : int pydantic-field \u00b6 How many rows to display. row_offset : int pydantic-field \u00b6 From which row to start. render_as__terminal_renderable ( self , value ) \u00b6 Source code in tabular/models/table.py def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata ) retrieve_source_type () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_source_type ( cls ) -> str : return \"table\"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules","text":"","title":"modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.array","text":"FORCE_NON_NULL_DOC \u00b6 MAX_INDEX_DOC \u00b6 MIN_INDEX_DOC \u00b6 REMOVE_TOKENS_DOC \u00b6","title":"array"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.array-classes","text":"DeserializeArrayModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array ExtractDateConfig ( KiaraInputsConfig ) pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. ExtractDateModule ( AutoInputsKiaraModule ) \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) Classes \u00b6 _config_cls ( KiaraInputsConfig ) private pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.db-classes","text":"CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. create__database__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__table ( self , source_value , optional ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db create_optional_inputs ( self , source_type , target_type ) \u00b6 Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db QueryDatabaseConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. QueryDatabaseModule ( KiaraModule ) \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table )","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.table","text":"EMPTY_COLUMN_NAME_MARKER \u00b6","title":"table"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.table-classes","text":"CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. create__table__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__table__from__text_file_bundle ( self , source_value ) \u00b6 Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) DeserializeTableModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table ExportTableModule ( DataExportModule ) \u00b6 Export network data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export network data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path} export__table__as__csv_file ( self , value , base_path , name ) \u00b6 Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } MergeTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. MergeTableModule ( KiaraModule ) \u00b6 Create a table from other tables and/or arrays. Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays.\"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) QueryTableSQL ( KiaraModule ) \u00b6 Execute a sql query against an (Arrow) table. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table.\"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ()) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ()) QueryTableSQLModuleConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.pipelines","text":"Default (empty) module that is used as a base path for pipelines contained in this package.","title":"pipelines"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils","text":"","title":"utils"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils-functions","text":"","title":"Functions"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.convert_arrow_column_types_to_sqlite","text":"Source code in tabular/utils.py def convert_arrow_column_types_to_sqlite ( table : \"pa.Table\" , ) -> Dict [ str , SqliteDataType ]: result : Dict [ str , SqliteDataType ] = {} for column_name in table . column_names : field = table . field ( column_name ) sqlite_type = convert_arrow_type_to_sqlite ( str ( field . type )) result [ column_name ] = sqlite_type return result","title":"convert_arrow_column_types_to_sqlite()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.convert_arrow_type_to_sqlite","text":"Source code in tabular/utils.py def convert_arrow_type_to_sqlite ( data_type : str ) -> SqliteDataType : if data_type . startswith ( \"int\" ) or data_type . startswith ( \"uint\" ): return \"INTEGER\" if ( data_type . startswith ( \"float\" ) or data_type . startswith ( \"decimal\" ) or data_type . startswith ( \"double\" ) ): return \"REAL\" if data_type . startswith ( \"time\" ) or data_type . startswith ( \"date\" ): return \"TEXT\" if data_type == \"bool\" : return \"INTEGER\" if data_type in [ \"string\" , \"utf8\" , \"large_string\" , \"large_utf8\" ]: return \"TEXT\" if data_type in [ \"binary\" , \"large_binary\" ]: return \"BLOB\" raise Exception ( f \"Can't convert to sqlite type: { data_type } \" )","title":"convert_arrow_type_to_sqlite()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.create_sqlite_schema_data_from_arrow_table","text":"Create a sql schema statement from an Arrow table object. Parameters: Name Type Description Default table pa.Table the Arrow table object required column_map Optional[Mapping[str, str]] a map that contains column names that should be changed in the new table None index_columns Optional[Iterable[str]] a list of column names (after mapping) to create module_indexes for None extra_column_info a list of extra schema instructions per column name (after mapping) required Source code in tabular/utils.py def create_sqlite_schema_data_from_arrow_table ( table : \"pa.Table\" , column_map : Optional [ Mapping [ str , str ]] = None , index_columns : Optional [ Iterable [ str ]] = None , nullable_columns : Optional [ Iterable [ str ]] = None , unique_columns : Optional [ Iterable [ str ]] = None , primary_key : Optional [ str ] = None , ) -> SqliteTableSchema : \"\"\"Create a sql schema statement from an Arrow table object. Arguments: table: the Arrow table object column_map: a map that contains column names that should be changed in the new table index_columns: a list of column names (after mapping) to create module_indexes for extra_column_info: a list of extra schema instructions per column name (after mapping) \"\"\" columns = convert_arrow_column_types_to_sqlite ( table = table ) if column_map is None : column_map = {} temp : Dict [ str , SqliteDataType ] = {} if index_columns is None : index_columns = [] if nullable_columns is None : nullable_columns = [] if unique_columns is None : unique_columns = [] for cn , sqlite_data_type in columns . items (): if cn in column_map . keys (): new_key = column_map [ cn ] index_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in index_columns ] unique_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in unique_columns ] nullable_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in nullable_columns ] else : new_key = cn temp [ new_key ] = sqlite_data_type columns = temp if not columns : raise Exception ( \"Resulting table schema has no columns.\" ) else : for ic in index_columns : if ic not in columns . keys (): raise Exception ( f \"Can't create schema, requested index column name not available: { ic } \" ) schema = SqliteTableSchema ( columns = columns , index_columns = index_columns , nullable_columns = nullable_columns , unique_columns = unique_columns , primary_key = primary_key , ) return schema","title":"create_sqlite_schema_data_from_arrow_table()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.create_sqlite_table_from_tabular_file","text":"Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : str = None , quotechar : str = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension with open ( file_item . path , \"rb\" ) as f : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , )","title":"create_sqlite_table_from_tabular_file()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.insert_db_table_from_file_bundle","text":"Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import ( Column , DateTime , Integer , MetaData , String , Table , Text , insert , ) from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"import_time\" , DateTime (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"import_time\" : f . import_time , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"insert_db_table_from_file_bundle()"},{"location":"reference/kiara_plugin/tabular/defaults/","text":"Attributes \u00b6 DEFAULT_TABULAR_DATA_CHUNK_SIZE \u00b6 KIARA_PLUGIN_TABULAR_BASE_FOLDER \u00b6 Marker to indicate the base folder for the kiara network module package. KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER \u00b6 Default resources folder for this package. RESERVED_SQL_KEYWORDS \u00b6 SQLALCHEMY_SQLITE_TYPE_MAP : Dict [ Type , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] \u00b6 SQLITE_DATA_TYPE : Tuple [ Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ], ... ] \u00b6 SQLITE_SQLALCHEMY_TYPE_MAP : Dict [ Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ], Type ] \u00b6 SqliteDataType \u00b6 TEMPLATES_FOLDER \u00b6","title":"defaults"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.DEFAULT_TABULAR_DATA_CHUNK_SIZE","text":"","title":"DEFAULT_TABULAR_DATA_CHUNK_SIZE"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.KIARA_PLUGIN_TABULAR_BASE_FOLDER","text":"Marker to indicate the base folder for the kiara network module package.","title":"KIARA_PLUGIN_TABULAR_BASE_FOLDER"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER","text":"Default resources folder for this package.","title":"KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.RESERVED_SQL_KEYWORDS","text":"","title":"RESERVED_SQL_KEYWORDS"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.SQLALCHEMY_SQLITE_TYPE_MAP","text":"","title":"SQLALCHEMY_SQLITE_TYPE_MAP"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.SQLITE_DATA_TYPE","text":"","title":"SQLITE_DATA_TYPE"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.SQLITE_SQLALCHEMY_TYPE_MAP","text":"","title":"SQLITE_SQLALCHEMY_TYPE_MAP"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.SqliteDataType","text":"","title":"SqliteDataType"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.TEMPLATES_FOLDER","text":"","title":"TEMPLATES_FOLDER"},{"location":"reference/kiara_plugin/tabular/utils/","text":"Functions \u00b6 convert_arrow_column_types_to_sqlite ( table ) \u00b6 Source code in tabular/utils.py def convert_arrow_column_types_to_sqlite ( table : \"pa.Table\" , ) -> Dict [ str , SqliteDataType ]: result : Dict [ str , SqliteDataType ] = {} for column_name in table . column_names : field = table . field ( column_name ) sqlite_type = convert_arrow_type_to_sqlite ( str ( field . type )) result [ column_name ] = sqlite_type return result convert_arrow_type_to_sqlite ( data_type ) \u00b6 Source code in tabular/utils.py def convert_arrow_type_to_sqlite ( data_type : str ) -> SqliteDataType : if data_type . startswith ( \"int\" ) or data_type . startswith ( \"uint\" ): return \"INTEGER\" if ( data_type . startswith ( \"float\" ) or data_type . startswith ( \"decimal\" ) or data_type . startswith ( \"double\" ) ): return \"REAL\" if data_type . startswith ( \"time\" ) or data_type . startswith ( \"date\" ): return \"TEXT\" if data_type == \"bool\" : return \"INTEGER\" if data_type in [ \"string\" , \"utf8\" , \"large_string\" , \"large_utf8\" ]: return \"TEXT\" if data_type in [ \"binary\" , \"large_binary\" ]: return \"BLOB\" raise Exception ( f \"Can't convert to sqlite type: { data_type } \" ) create_sqlite_schema_data_from_arrow_table ( table , column_map = None , index_columns = None , nullable_columns = None , unique_columns = None , primary_key = None ) \u00b6 Create a sql schema statement from an Arrow table object. Parameters: Name Type Description Default table pa.Table the Arrow table object required column_map Optional[Mapping[str, str]] a map that contains column names that should be changed in the new table None index_columns Optional[Iterable[str]] a list of column names (after mapping) to create module_indexes for None extra_column_info a list of extra schema instructions per column name (after mapping) required Source code in tabular/utils.py def create_sqlite_schema_data_from_arrow_table ( table : \"pa.Table\" , column_map : Optional [ Mapping [ str , str ]] = None , index_columns : Optional [ Iterable [ str ]] = None , nullable_columns : Optional [ Iterable [ str ]] = None , unique_columns : Optional [ Iterable [ str ]] = None , primary_key : Optional [ str ] = None , ) -> SqliteTableSchema : \"\"\"Create a sql schema statement from an Arrow table object. Arguments: table: the Arrow table object column_map: a map that contains column names that should be changed in the new table index_columns: a list of column names (after mapping) to create module_indexes for extra_column_info: a list of extra schema instructions per column name (after mapping) \"\"\" columns = convert_arrow_column_types_to_sqlite ( table = table ) if column_map is None : column_map = {} temp : Dict [ str , SqliteDataType ] = {} if index_columns is None : index_columns = [] if nullable_columns is None : nullable_columns = [] if unique_columns is None : unique_columns = [] for cn , sqlite_data_type in columns . items (): if cn in column_map . keys (): new_key = column_map [ cn ] index_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in index_columns ] unique_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in unique_columns ] nullable_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in nullable_columns ] else : new_key = cn temp [ new_key ] = sqlite_data_type columns = temp if not columns : raise Exception ( \"Resulting table schema has no columns.\" ) else : for ic in index_columns : if ic not in columns . keys (): raise Exception ( f \"Can't create schema, requested index column name not available: { ic } \" ) schema = SqliteTableSchema ( columns = columns , index_columns = index_columns , nullable_columns = nullable_columns , unique_columns = unique_columns , primary_key = primary_key , ) return schema create_sqlite_table_from_tabular_file ( target_db_file , file_item , table_name = None , is_csv = True , is_tsv = False , is_nl = False , primary_key_column_names = None , flatten_nested_json_objects = False , csv_delimiter = None , quotechar = None , sniff = True , no_headers = False , encoding = 'utf-8' , batch_size = 100 , detect_types = True ) \u00b6 Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : str = None , quotechar : str = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension with open ( file_item . path , \"rb\" ) as f : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , ) insert_db_table_from_file_bundle ( database , file_bundle , table_name = 'file_items' , include_content = True ) \u00b6 Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import ( Column , DateTime , Integer , MetaData , String , Table , Text , insert , ) from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"import_time\" , DateTime (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"import_time\" : f . import_time , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"utils"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils-functions","text":"","title":"Functions"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.convert_arrow_column_types_to_sqlite","text":"Source code in tabular/utils.py def convert_arrow_column_types_to_sqlite ( table : \"pa.Table\" , ) -> Dict [ str , SqliteDataType ]: result : Dict [ str , SqliteDataType ] = {} for column_name in table . column_names : field = table . field ( column_name ) sqlite_type = convert_arrow_type_to_sqlite ( str ( field . type )) result [ column_name ] = sqlite_type return result","title":"convert_arrow_column_types_to_sqlite()"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.convert_arrow_type_to_sqlite","text":"Source code in tabular/utils.py def convert_arrow_type_to_sqlite ( data_type : str ) -> SqliteDataType : if data_type . startswith ( \"int\" ) or data_type . startswith ( \"uint\" ): return \"INTEGER\" if ( data_type . startswith ( \"float\" ) or data_type . startswith ( \"decimal\" ) or data_type . startswith ( \"double\" ) ): return \"REAL\" if data_type . startswith ( \"time\" ) or data_type . startswith ( \"date\" ): return \"TEXT\" if data_type == \"bool\" : return \"INTEGER\" if data_type in [ \"string\" , \"utf8\" , \"large_string\" , \"large_utf8\" ]: return \"TEXT\" if data_type in [ \"binary\" , \"large_binary\" ]: return \"BLOB\" raise Exception ( f \"Can't convert to sqlite type: { data_type } \" )","title":"convert_arrow_type_to_sqlite()"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.create_sqlite_schema_data_from_arrow_table","text":"Create a sql schema statement from an Arrow table object. Parameters: Name Type Description Default table pa.Table the Arrow table object required column_map Optional[Mapping[str, str]] a map that contains column names that should be changed in the new table None index_columns Optional[Iterable[str]] a list of column names (after mapping) to create module_indexes for None extra_column_info a list of extra schema instructions per column name (after mapping) required Source code in tabular/utils.py def create_sqlite_schema_data_from_arrow_table ( table : \"pa.Table\" , column_map : Optional [ Mapping [ str , str ]] = None , index_columns : Optional [ Iterable [ str ]] = None , nullable_columns : Optional [ Iterable [ str ]] = None , unique_columns : Optional [ Iterable [ str ]] = None , primary_key : Optional [ str ] = None , ) -> SqliteTableSchema : \"\"\"Create a sql schema statement from an Arrow table object. Arguments: table: the Arrow table object column_map: a map that contains column names that should be changed in the new table index_columns: a list of column names (after mapping) to create module_indexes for extra_column_info: a list of extra schema instructions per column name (after mapping) \"\"\" columns = convert_arrow_column_types_to_sqlite ( table = table ) if column_map is None : column_map = {} temp : Dict [ str , SqliteDataType ] = {} if index_columns is None : index_columns = [] if nullable_columns is None : nullable_columns = [] if unique_columns is None : unique_columns = [] for cn , sqlite_data_type in columns . items (): if cn in column_map . keys (): new_key = column_map [ cn ] index_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in index_columns ] unique_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in unique_columns ] nullable_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in nullable_columns ] else : new_key = cn temp [ new_key ] = sqlite_data_type columns = temp if not columns : raise Exception ( \"Resulting table schema has no columns.\" ) else : for ic in index_columns : if ic not in columns . keys (): raise Exception ( f \"Can't create schema, requested index column name not available: { ic } \" ) schema = SqliteTableSchema ( columns = columns , index_columns = index_columns , nullable_columns = nullable_columns , unique_columns = unique_columns , primary_key = primary_key , ) return schema","title":"create_sqlite_schema_data_from_arrow_table()"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.create_sqlite_table_from_tabular_file","text":"Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : str = None , quotechar : str = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension with open ( file_item . path , \"rb\" ) as f : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , )","title":"create_sqlite_table_from_tabular_file()"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.insert_db_table_from_file_bundle","text":"Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import ( Column , DateTime , Integer , MetaData , String , Table , Text , insert , ) from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"import_time\" , DateTime (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"import_time\" : f . import_time , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"insert_db_table_from_file_bundle()"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/","text":"This module contains the value type classes that are used in the kiara_plugin.tabular package. Modules \u00b6 array \u00b6 Classes \u00b6 ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray serialize ( self , data ) \u00b6 Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized Functions \u00b6 store_array ( array_obj , file_name , column_name = 'array' ) \u00b6 Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch ) db \u00b6 Classes \u00b6 DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase serialize ( self , data ) \u00b6 Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized table \u00b6 Classes \u00b6 TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Calculate the size of the value. Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable serialize ( self , data ) \u00b6 Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"data_types"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array","text":"","title":"array"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array.ArrayType","text":"An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result","title":"ArrayType"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array.ArrayType-methods","text":"parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray serialize ( self , data ) \u00b6 Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array-functions","text":"","title":"Functions"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array.store_array","text":"Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch )","title":"store_array()"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db.DatabaseType","text":"A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"DatabaseType"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db.DatabaseType-methods","text":"parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase serialize ( self , data ) \u00b6 Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table.TableType","text":"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"TableType"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table.TableType-methods","text":"calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Calculate the size of the value. Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable serialize ( self , data ) \u00b6 Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/array/","text":"Classes \u00b6 ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray serialize ( self , data ) \u00b6 Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized Functions \u00b6 store_array ( array_obj , file_name , column_name = 'array' ) \u00b6 Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch )","title":"array"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType","text":"An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result","title":"ArrayType"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType.parse_python_obj","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data )","title":"parse_python_obj()"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType.pretty_print_as__terminal_renderable","text":"Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result","title":"pretty_print_as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType.python_class","text":"Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray","title":"python_class()"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType.serialize","text":"Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"serialize()"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array-functions","text":"","title":"Functions"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.store_array","text":"Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch )","title":"store_array()"},{"location":"reference/kiara_plugin/tabular/data_types/db/","text":"Classes \u00b6 DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase serialize ( self , data ) \u00b6 Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"db"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType","text":"A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"DatabaseType"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.parse_python_obj","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data","title":"parse_python_obj()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.pretty_print_as__terminal_renderable","text":"Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"pretty_print_as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.python_class","text":"Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase","title":"python_class()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.serialize","text":"Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"serialize()"},{"location":"reference/kiara_plugin/tabular/data_types/table/","text":"Classes \u00b6 TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Calculate the size of the value. Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable serialize ( self , data ) \u00b6 Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"table"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType","text":"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"TableType"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.calculate_hash","text":"Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_cid ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array))","title":"calculate_hash()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.calculate_size","text":"Calculate the size of the value. Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table )","title":"calculate_size()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.parse_python_obj","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data )","title":"parse_python_obj()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.pretty_print_as__terminal_renderable","text":"Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"pretty_print_as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.python_class","text":"Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable","title":"python_class()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.serialize","text":"Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"serialize()"},{"location":"reference/kiara_plugin/tabular/models/__init__/","text":"This module contains the metadata (and other) models that are used in the kiara_plugin.tabular package. Those models are convenience wrappers that make it easier for kiara to find, create, manage and version metadata -- but also other type of models -- that is attached to data, as well as kiara modules. Metadata models must be a sub-class of kiara.metadata.MetadataModel . Other models usually sub-class a pydantic BaseModel or implement custom base classes. Classes \u00b6 ColumnSchema ( BaseModel ) pydantic-model \u00b6 Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict ) Attributes \u00b6 metadata : Dict [ str , Any ] pydantic-field \u00b6 Other metadata for the column. type_name : str pydantic-field required \u00b6 The type name of the column (backend-specific). TableMetadata ( KiaraModel ) pydantic-model \u00b6 Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , } Attributes \u00b6 column_names : List [ str ] pydantic-field required \u00b6 The name of the columns of the table. column_schema : Dict [ str , kiara_plugin . tabular . models . ColumnSchema ] pydantic-field required \u00b6 The schema description of the table. rows : int pydantic-field required \u00b6 The number of rows the table contains. size : int pydantic-field \u00b6 The tables size in bytes. Modules \u00b6 db \u00b6 Classes \u00b6 DatabaseMetadata ( ValueMetadata ) pydantic-model \u00b6 Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" ) Attributes \u00b6 tables : Dict [ str , kiara_plugin . tabular . models . TableMetadata ] pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_cid : CID property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector get_sqlalchemy_metadata ( self ) \u00b6 Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj get_sqlalchemy_table ( self , table_name ) \u00b6 Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table SqliteTableSchema ( BaseModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table Attributes \u00b6 columns : Dict [ str , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] pydantic-field required \u00b6 The table columns and their attributes. index_columns : List [ str ] pydantic-field \u00b6 The columns to index nullable_columns : List [ str ] pydantic-field \u00b6 The columns that are nullable. primary_key : str pydantic-field \u00b6 The primary key for this table. unique_columns : List [ str ] pydantic-field \u00b6 The columns that should be marked 'UNIQUE'. Methods \u00b6 create_table ( self , table_name , engine ) \u00b6 Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table create_table_metadata ( self , table_name ) \u00b6 Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table table \u00b6 Classes \u00b6 KiaraArray ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist () KiaraTable ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage tabular data in a hopefully memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a hopefully memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 Return the data as an Apache Arrow Table instance. column_names : Iterable [ str ] property readonly \u00b6 Retrieve the names of all the columns of this table. data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 Return the number of rows in this table. Methods \u00b6 create_table ( data ) classmethod \u00b6 Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] RenderTableInstruction ( RenderInstruction ) pydantic-model \u00b6 Source code in tabular/models/table.py class RenderTableInstruction ( RenderInstruction ): @classmethod def retrieve_source_type ( cls ) -> str : return \"table\" _kiara_model_id = \"instance.render_instruction.table\" number_of_rows : int = Field ( description = \"How many rows to display.\" , default = 20 ) row_offset : int = Field ( description = \"From which row to start.\" , default = 0 ) columns : Optional [ List [ str ]] = Field ( description = \"Which rows do display.\" , default = None ) def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata ) Attributes \u00b6 columns : List [ str ] pydantic-field \u00b6 Which rows do display. number_of_rows : int pydantic-field \u00b6 How many rows to display. row_offset : int pydantic-field \u00b6 From which row to start. render_as__terminal_renderable ( self , value ) \u00b6 Source code in tabular/models/table.py def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata ) retrieve_source_type () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_source_type ( cls ) -> str : return \"table\"","title":"models"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema","text":"Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict )","title":"ColumnSchema"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema.metadata","text":"Other metadata for the column.","title":"metadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema.type_name","text":"The type name of the column (backend-specific).","title":"type_name"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata","text":"Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , }","title":"TableMetadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.column_names","text":"The name of the columns of the table.","title":"column_names"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.column_schema","text":"The schema description of the table.","title":"column_schema"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.rows","text":"The number of rows the table contains.","title":"rows"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.size","text":"The tables size in bytes.","title":"size"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.DatabaseMetadata","text":"Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" )","title":"DatabaseMetadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.DatabaseMetadata-attributes","text":"tables : Dict [ str , kiara_plugin . tabular . models . TableMetadata ] pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ]","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.KiaraDatabase","text":"Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table","title":"KiaraDatabase"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.KiaraDatabase-attributes","text":"db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_cid : CID property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.KiaraDatabase-methods","text":"copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector get_sqlalchemy_metadata ( self ) \u00b6 Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj get_sqlalchemy_table ( self , table_name ) \u00b6 Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.SqliteTableSchema","text":"Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table","title":"SqliteTableSchema"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.SqliteTableSchema-attributes","text":"columns : Dict [ str , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] pydantic-field required \u00b6 The table columns and their attributes. index_columns : List [ str ] pydantic-field \u00b6 The columns to index nullable_columns : List [ str ] pydantic-field \u00b6 The columns that are nullable. primary_key : str pydantic-field \u00b6 The primary key for this table. unique_columns : List [ str ] pydantic-field \u00b6 The columns that should be marked 'UNIQUE'.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.SqliteTableSchema-methods","text":"create_table ( self , table_name , engine ) \u00b6 Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table create_table_metadata ( self , table_name ) \u00b6 Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraArray","text":"Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas ()","title":"KiaraArray"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraArray-attributes","text":"arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist ()","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable","text":"A wrapper class to manage tabular data in a hopefully memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a hopefully memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas ()","title":"KiaraTable"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable-attributes","text":"arrow_table : Table property readonly \u00b6 Return the data as an Apache Arrow Table instance. column_names : Iterable [ str ] property readonly \u00b6 Retrieve the names of all the columns of this table. data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 Return the number of rows in this table.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable-methods","text":"create_table ( data ) classmethod \u00b6 Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist ()","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTableMetadata","text":"File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" )","title":"KiaraTableMetadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTableMetadata-attributes","text":"table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.RenderTableInstruction","text":"Source code in tabular/models/table.py class RenderTableInstruction ( RenderInstruction ): @classmethod def retrieve_source_type ( cls ) -> str : return \"table\" _kiara_model_id = \"instance.render_instruction.table\" number_of_rows : int = Field ( description = \"How many rows to display.\" , default = 20 ) row_offset : int = Field ( description = \"From which row to start.\" , default = 0 ) columns : Optional [ List [ str ]] = Field ( description = \"Which rows do display.\" , default = None ) def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata )","title":"RenderTableInstruction"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.RenderTableInstruction-attributes","text":"columns : List [ str ] pydantic-field \u00b6 Which rows do display. number_of_rows : int pydantic-field \u00b6 How many rows to display. row_offset : int pydantic-field \u00b6 From which row to start. render_as__terminal_renderable ( self , value ) \u00b6 Source code in tabular/models/table.py def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata ) retrieve_source_type () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_source_type ( cls ) -> str : return \"table\"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/db/","text":"Classes \u00b6 DatabaseMetadata ( ValueMetadata ) pydantic-model \u00b6 Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" ) Attributes \u00b6 tables : Dict [ str , kiara_plugin . tabular . models . TableMetadata ] pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_cid : CID property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector get_sqlalchemy_metadata ( self ) \u00b6 Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj get_sqlalchemy_table ( self , table_name ) \u00b6 Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table SqliteTableSchema ( BaseModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table Attributes \u00b6 columns : Dict [ str , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] pydantic-field required \u00b6 The table columns and their attributes. index_columns : List [ str ] pydantic-field \u00b6 The columns to index nullable_columns : List [ str ] pydantic-field \u00b6 The columns that are nullable. primary_key : str pydantic-field \u00b6 The primary key for this table. unique_columns : List [ str ] pydantic-field \u00b6 The columns that should be marked 'UNIQUE'. Methods \u00b6 create_table ( self , table_name , engine ) \u00b6 Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table create_table_metadata ( self , table_name ) \u00b6 Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table","title":"db"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.DatabaseMetadata","text":"Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" )","title":"DatabaseMetadata"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.DatabaseMetadata-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.DatabaseMetadata.tables","text":"The table schema.","title":"tables"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.DatabaseMetadata.create_value_metadata","text":"Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds )","title":"create_value_metadata()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.DatabaseMetadata.retrieve_supported_data_types","text":"Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ]","title":"retrieve_supported_data_types()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase","text":"Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table","title":"KiaraDatabase"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.db_file_path","text":"The path to the sqlite database file.","title":"db_file_path"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.db_url","text":"","title":"db_url"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.file_cid","text":"","title":"file_cid"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.table_names","text":"","title":"table_names"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.copy_database_file","text":"Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db","title":"copy_database_file()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.create_if_not_exists","text":"Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url )","title":"create_if_not_exists()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.create_in_temp_dir","text":"Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db","title":"create_in_temp_dir()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.ensure_absolute_path","text":"Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path","title":"ensure_absolute_path()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.execute_sql","text":"Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate ()","title":"execute_sql()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_sqlalchemy_engine","text":"Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine","title":"get_sqlalchemy_engine()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_sqlalchemy_inspector","text":"Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector","title":"get_sqlalchemy_inspector()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_sqlalchemy_metadata","text":"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj","title":"get_sqlalchemy_metadata()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_sqlalchemy_table","text":"Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table","title":"get_sqlalchemy_table()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema","text":"Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table","title":"SqliteTableSchema"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.columns","text":"The table columns and their attributes.","title":"columns"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.index_columns","text":"The columns to index","title":"index_columns"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.nullable_columns","text":"The columns that are nullable.","title":"nullable_columns"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.primary_key","text":"The primary key for this table.","title":"primary_key"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.unique_columns","text":"The columns that should be marked 'UNIQUE'.","title":"unique_columns"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.create_table","text":"Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table","title":"create_table()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.create_table_metadata","text":"Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table","title":"create_table_metadata()"},{"location":"reference/kiara_plugin/tabular/models/table/","text":"Classes \u00b6 KiaraArray ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist () KiaraTable ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage tabular data in a hopefully memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a hopefully memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 Return the data as an Apache Arrow Table instance. column_names : Iterable [ str ] property readonly \u00b6 Retrieve the names of all the columns of this table. data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 Return the number of rows in this table. Methods \u00b6 create_table ( data ) classmethod \u00b6 Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] RenderTableInstruction ( RenderInstruction ) pydantic-model \u00b6 Source code in tabular/models/table.py class RenderTableInstruction ( RenderInstruction ): @classmethod def retrieve_source_type ( cls ) -> str : return \"table\" _kiara_model_id = \"instance.render_instruction.table\" number_of_rows : int = Field ( description = \"How many rows to display.\" , default = 20 ) row_offset : int = Field ( description = \"From which row to start.\" , default = 0 ) columns : Optional [ List [ str ]] = Field ( description = \"Which rows do display.\" , default = None ) def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata ) Attributes \u00b6 columns : List [ str ] pydantic-field \u00b6 Which rows do display. number_of_rows : int pydantic-field \u00b6 How many rows to display. row_offset : int pydantic-field \u00b6 From which row to start. render_as__terminal_renderable ( self , value ) \u00b6 Source code in tabular/models/table.py def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata ) retrieve_source_type () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_source_type ( cls ) -> str : return \"table\"","title":"table"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray","text":"Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas ()","title":"KiaraArray"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray.arrow_array","text":"","title":"arrow_array"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray.data_path","text":"The path to the (feather) file backing this array.","title":"data_path"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray.create_array","text":"Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj","title":"create_array()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray.to_pandas","text":"Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas ()","title":"to_pandas()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray.to_pylist","text":"Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist ()","title":"to_pylist()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable","text":"A wrapper class to manage tabular data in a hopefully memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a hopefully memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas ()","title":"KiaraTable"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.arrow_table","text":"Return the data as an Apache Arrow Table instance.","title":"arrow_table"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.column_names","text":"Retrieve the names of all the columns of this table.","title":"column_names"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.data_path","text":"The path to the (feather) file backing this array.","title":"data_path"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.num_rows","text":"Return the number of rows in this table.","title":"num_rows"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.create_table","text":"Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj","title":"create_table()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.to_pandas","text":"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas ()","title":"to_pandas()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.to_pydict","text":"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict ()","title":"to_pydict()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.to_pylist","text":"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist ()","title":"to_pylist()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata","text":"File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" )","title":"KiaraTableMetadata"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata.table","text":"The table schema.","title":"table"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata.create_value_metadata","text":"Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md )","title":"create_value_metadata()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata.retrieve_supported_data_types","text":"Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"retrieve_supported_data_types()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.RenderTableInstruction","text":"Source code in tabular/models/table.py class RenderTableInstruction ( RenderInstruction ): @classmethod def retrieve_source_type ( cls ) -> str : return \"table\" _kiara_model_id = \"instance.render_instruction.table\" number_of_rows : int = Field ( description = \"How many rows to display.\" , default = 20 ) row_offset : int = Field ( description = \"From which row to start.\" , default = 0 ) columns : Optional [ List [ str ]] = Field ( description = \"Which rows do display.\" , default = None ) def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata )","title":"RenderTableInstruction"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.RenderTableInstruction-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.RenderTableInstruction.columns","text":"Which rows do display.","title":"columns"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.RenderTableInstruction.number_of_rows","text":"How many rows to display.","title":"number_of_rows"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.RenderTableInstruction.row_offset","text":"From which row to start.","title":"row_offset"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.RenderTableInstruction.render_as__terminal_renderable","text":"Source code in tabular/models/table.py def render_as__terminal_renderable ( self , value : Value ): import duckdb table : KiaraTable = value . data columnns : Iterable [ str ] = self . columns # type: ignore if not columnns : columnns = table . column_names assert columnns query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data ORDER by { ', ' . join ( columnns ) } LIMIT { self . number_of_rows } OFFSET { self . row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( table . arrow_table ) query_result : duckdb . DuckDBPyResult = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . fetch_arrow_table () wrap = ArrowTabularWrap ( table = result_table ) pretty = wrap . pretty_print () related_instructions = {} related_instructions [ \"first\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : 0 , \"columns\" : self . columns } ) if self . row_offset > 0 : p_offset = self . row_offset - self . number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"columns\" : self . columns } related_instructions [ \"previous\" ] = RenderTableInstruction . construct ( ** previous ) n_offset = self . row_offset + self . number_of_rows if n_offset < table . num_rows : next = { \"row_offset\" : n_offset , \"columns\" : self . columns } related_instructions [ \"next\" ] = RenderTableInstruction . construct ( ** next ) row_offset = table . num_rows - self . number_of_rows if row_offset < 0 : row_offset = 0 related_instructions [ \"last\" ] = RenderTableInstruction . construct ( ** { \"row_offset\" : row_offset , \"columns\" : columnns } ) render_metadata = RenderMetadata ( related_instructions = related_instructions ) return RenderValueResult ( rendered = pretty , metadata = render_metadata )","title":"render_as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.RenderTableInstruction.retrieve_source_type","text":"Source code in tabular/models/table.py @classmethod def retrieve_source_type ( cls ) -> str : return \"table\"","title":"retrieve_source_type()"},{"location":"reference/kiara_plugin/tabular/modules/__init__/","text":"Modules \u00b6 array special \u00b6 FORCE_NON_NULL_DOC \u00b6 MAX_INDEX_DOC \u00b6 MIN_INDEX_DOC \u00b6 REMOVE_TOKENS_DOC \u00b6 Classes \u00b6 DeserializeArrayModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array ExtractDateConfig ( KiaraInputsConfig ) pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. ExtractDateModule ( AutoInputsKiaraModule ) \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) Classes \u00b6 _config_cls ( KiaraInputsConfig ) private pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) db special \u00b6 Classes \u00b6 CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. create__database__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__table ( self , source_value , optional ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db create_optional_inputs ( self , source_type , target_type ) \u00b6 Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db QueryDatabaseConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. QueryDatabaseModule ( KiaraModule ) \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) table special \u00b6 EMPTY_COLUMN_NAME_MARKER \u00b6 Classes \u00b6 CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. create__table__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__table__from__text_file_bundle ( self , source_value ) \u00b6 Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) DeserializeTableModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table ExportTableModule ( DataExportModule ) \u00b6 Export network data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export network data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path} export__table__as__csv_file ( self , value , base_path , name ) \u00b6 Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } MergeTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. MergeTableModule ( KiaraModule ) \u00b6 Create a table from other tables and/or arrays. Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays.\"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) QueryTableSQL ( KiaraModule ) \u00b6 Execute a sql query against an (Arrow) table. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table.\"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ()) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ()) QueryTableSQLModuleConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"modules"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array","text":"","title":"array"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.FORCE_NON_NULL_DOC","text":"","title":"FORCE_NON_NULL_DOC"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.MAX_INDEX_DOC","text":"","title":"MAX_INDEX_DOC"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.MIN_INDEX_DOC","text":"","title":"MIN_INDEX_DOC"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.REMOVE_TOKENS_DOC","text":"","title":"REMOVE_TOKENS_DOC"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule","text":"Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array","title":"DeserializeArrayModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig","text":"Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list )","title":"ExtractDateConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig-attributes","text":"force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule","text":"Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"ExtractDateModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule-classes","text":"_config_cls ( KiaraInputsConfig ) private pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule","text":"Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db","title":"CreateDatabaseModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule-classes","text":"_config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. create__database__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__table ( self , source_value , optional ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db create_optional_inputs ( self , source_type , target_type ) \u00b6 Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig","text":"Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , )","title":"CreateDatabaseModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule","text":"Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db","title":"LoadDatabaseFromDiskModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseConfig","text":"Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None )","title":"QueryDatabaseConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseConfig-attributes","text":"query : str pydantic-field \u00b6 The query.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule","text":"Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table )","title":"QueryDatabaseModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule-classes","text":"_config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.EMPTY_COLUMN_NAME_MARKER","text":"","title":"EMPTY_COLUMN_NAME_MARKER"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule","text":"Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"CreateTableModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule-classes","text":"_config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. create__table__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__table__from__text_file_bundle ( self , source_value ) \u00b6 Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CreateTableModuleConfig","text":"Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , )","title":"CreateTableModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CreateTableModuleConfig-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule","text":"Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"CutColumnModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule","text":"Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table","title":"DeserializeTableModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.ExportTableModule","text":"Export network data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export network data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path} export__table__as__csv_file ( self , value , base_path , name ) \u00b6 Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path }","title":"ExportTableModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig","text":"Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict )","title":"MergeTableConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig-attributes","text":"column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule","text":"Create a table from other tables and/or arrays. Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays.\"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table )","title":"MergeTableModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule-classes","text":"_config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL","text":"Execute a sql query against an (Arrow) table. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table.\"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ())","title":"QueryTableSQL"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL-classes","text":"_config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ())","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig","text":"Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , )","title":"QueryTableSQLModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig-attributes","text":"query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/","text":"FORCE_NON_NULL_DOC \u00b6 MAX_INDEX_DOC \u00b6 MIN_INDEX_DOC \u00b6 REMOVE_TOKENS_DOC \u00b6 Classes \u00b6 DeserializeArrayModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array ExtractDateConfig ( KiaraInputsConfig ) pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. ExtractDateModule ( AutoInputsKiaraModule ) \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) Classes \u00b6 _config_cls ( KiaraInputsConfig ) private pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"array"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.FORCE_NON_NULL_DOC","text":"","title":"FORCE_NON_NULL_DOC"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.MAX_INDEX_DOC","text":"","title":"MAX_INDEX_DOC"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.MIN_INDEX_DOC","text":"","title":"MIN_INDEX_DOC"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.REMOVE_TOKENS_DOC","text":"","title":"REMOVE_TOKENS_DOC"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule","text":"Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array","title":"DeserializeArrayModule"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule.retrieve_serialized_value_type","text":"Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\"","title":"retrieve_serialized_value_type()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule.retrieve_supported_serialization_profile","text":"Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\"","title":"retrieve_supported_serialization_profile()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule.retrieve_supported_target_profiles","text":"Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray }","title":"retrieve_supported_target_profiles()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule.to__python_object","text":"Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array","title":"to__python_object()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig","text":"Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list )","title":"ExtractDateConfig"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig.force_non_null","text":"If set to 'True', raise an error if any of the strings in the array can't be parsed.","title":"force_non_null"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig.max_index","text":"The maximum index until whic to parse the string(s).","title":"max_index"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig.min_index","text":"The minimum index from where to start parsing the string(s).","title":"min_index"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig.remove_tokens","text":"A list of tokens/characters to replace with a single white-space before parsing the input.","title":"remove_tokens"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule","text":"Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"ExtractDateModule"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule._config_cls","text":"Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule._config_cls-attributes","text":"force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } }","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule.process","text":"Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"process()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/","text":"Classes \u00b6 CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. create__database__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__table ( self , source_value , optional ) \u00b6 Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db create_optional_inputs ( self , source_type , target_type ) \u00b6 Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db QueryDatabaseConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. QueryDatabaseModule ( KiaraModule ) \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table )","title":"db"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule","text":"Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db","title":"CreateDatabaseModule"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule._config_cls","text":"Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule._config_cls-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule.create__database__from__csv_file","text":"Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path","title":"create__database__from__csv_file()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule.create__database__from__csv_file_bundle","text":"Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path","title":"create__database__from__csv_file_bundle()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule.create__database__from__table","text":"Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db","title":"create__database__from__table()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule.create_optional_inputs","text":"Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None","title":"create_optional_inputs()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig","text":"Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , )","title":"CreateDatabaseModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig.ignore_errors","text":"Whether to ignore convert errors and omit the failed items.","title":"ignore_errors"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig.include_source_file_content","text":"When including source metadata, whether to also include the original raw (string) content.","title":"include_source_file_content"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig.include_source_metadata","text":"Whether to include a table with metadata about the source files.","title":"include_source_metadata"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig.merge_into_single_table","text":"Whether to merge all csv files into a single table.","title":"merge_into_single_table"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule","text":"Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db","title":"LoadDatabaseFromDiskModule"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule.retrieve_serialized_value_type","text":"Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\"","title":"retrieve_serialized_value_type()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule.retrieve_supported_serialization_profile","text":"Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\"","title":"retrieve_supported_serialization_profile()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule.retrieve_supported_target_profiles","text":"Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase }","title":"retrieve_supported_target_profiles()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule.to__python_object","text":"Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db","title":"to__python_object()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseConfig","text":"Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None )","title":"QueryDatabaseConfig"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseConfig.query","text":"The query.","title":"query"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule","text":"Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table )","title":"QueryDatabaseModule"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule._config_cls","text":"Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule._config_cls-attributes","text":"query : str pydantic-field \u00b6 The query.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }}","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule.process","text":"Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table )","title":"process()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/","text":"EMPTY_COLUMN_NAME_MARKER \u00b6 Classes \u00b6 CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. create__table__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__table__from__text_file_bundle ( self , source_value ) \u00b6 Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) DeserializeTableModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table ExportTableModule ( DataExportModule ) \u00b6 Export network data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export network data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path} export__table__as__csv_file ( self , value , base_path , name ) \u00b6 Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } MergeTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. MergeTableModule ( KiaraModule ) \u00b6 Create a table from other tables and/or arrays. Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays.\"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) QueryTableSQL ( KiaraModule ) \u00b6 Execute a sql query against an (Arrow) table. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table.\"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ()) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ()) QueryTableSQLModuleConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"table"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.EMPTY_COLUMN_NAME_MARKER","text":"","title":"EMPTY_COLUMN_NAME_MARKER"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule","text":"Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"CreateTableModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule._config_cls","text":"Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule._config_cls-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule.create__table__from__csv_file","text":"Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data","title":"create__table__from__csv_file()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule.create__table__from__text_file_bundle","text":"Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"create__table__from__text_file_bundle()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModuleConfig","text":"Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , )","title":"CreateTableModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModuleConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModuleConfig.ignore_errors","text":"Whether to ignore convert errors and omit the failed items.","title":"ignore_errors"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule","text":"Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"CutColumnModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule.process","text":"Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"process()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule","text":"Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table","title":"DeserializeTableModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule.retrieve_serialized_value_type","text":"Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\"","title":"retrieve_serialized_value_type()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule.retrieve_supported_serialization_profile","text":"Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\"","title":"retrieve_supported_serialization_profile()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule.retrieve_supported_target_profiles","text":"Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable }","title":"retrieve_supported_target_profiles()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule.to__python_object","text":"Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table","title":"to__python_object()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.ExportTableModule","text":"Export network data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export network data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path}","title":"ExportTableModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.ExportTableModule.export__table__as__csv_file","text":"Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path }","title":"export__table__as__csv_file()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig","text":"Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict )","title":"MergeTableConfig"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig.column_map","text":"A map describing","title":"column_map"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig.inputs_schema","text":"A dict describing the inputs for this merge process.","title":"inputs_schema"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule","text":"Create a table from other tables and/or arrays. Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays.\"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table )","title":"MergeTableModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule._config_cls","text":"Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule._config_cls-attributes","text":"column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule.process","text":"Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table )","title":"process()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL","text":"Execute a sql query against an (Arrow) table. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table.\"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ())","title":"QueryTableSQL"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL._config_cls","text":"Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL._config_cls-attributes","text":"query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }}","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL.process","text":"Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyResult = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . fetch_arrow_table ())","title":"process()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig","text":"Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , )","title":"QueryTableSQLModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig.query","text":"The query to execute. If not specified, the user will be able to provide their own.","title":"query"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig.relation_name","text":"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"relation_name"},{"location":"reference/kiara_plugin/tabular/pipelines/__init__/","text":"Default (empty) module that is used as a base path for pipelines contained in this package.","title":"pipelines"}]}