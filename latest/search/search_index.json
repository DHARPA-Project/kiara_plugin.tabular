{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"kiara plugin: tabular \u00b6 This package contains a set of commonly used/useful modules, pipelines, types and metadata schemas for Kiara . Description \u00b6 kiara data-types and modules for working with tables and databases. Package content \u00b6 data_types \u00b6 array : An array, in most cases used as a column within a table. database : A database, containing one or several tables. table : Tabular data (table, spreadsheet, data_frame, what have you). module_types \u00b6 table.filters : -- n/a -- render.database : -- n/a -- render.table : -- n/a -- export.table : Export table data items. load.array : Deserialize array data. load.database : -- n/a -- load.table : -- n/a -- parse.date_array : Create an array of date objects from an array of strings. create.database : -- n/a -- create.table : -- n/a -- query.database : Execute a sql query against a (sqlite) database. table.cut_column : Cut off one column from a table, returning an array. table.merge : Create a table from other tables and/or arrays. query.table : Execute a sql query against an (Arrow) table. kiara_model_types \u00b6 database_metadata : Database and table properties. kiara_table_metadata : File stats. table_metadata : Describes properties for the 'table' data type. kiara_array : A class to manage array-like data. kiara_database : A wrapper class to manage a sqlite database. kiara_table : A wrapper class to manage tabular data in a memory efficient way. operations \u00b6 create.database.from.csv_file : Create a database from a csv_file value. create.database.from.csv_file_bundle : Create a database from a csv_file_bundle value. create.database.from.table : Create a database value from a table. create.table.from.csv_file : Create a table from a csv_file value. create.table.from.text_file_bundle : Create a table value from a text file_bundle. deserialize.array.as.python_object : -- n/a -- deserialize.database.as.python_object : -- n/a -- deserialize.table.as.python_object : -- n/a -- export.table.as.csv_file : Export a table as csv file. extract.date_array.from.table : Extract a date array from a table column. import.database.from.csv_file : Import a database from a csv file. import.table.from.csv_file : Import a table from a csv file. import.table.from.text_file_bundle : Load a table from a bundle of text files. parse.date_array : Create an array of date objects from an array of strings. query.database : Execute a sql query against a (sqlite) database. query.table : Execute a sql query against an (Arrow) table. render.database.as.string : -- n/a -- render.database.as.terminal_renderable : -- n/a -- render.table.as.string : -- n/a -- render.table.as.terminal_renderable : -- n/a -- table.cut_column : Cut off one column from a table, returning an array. table_filter.drop_columns : -- n/a -- table_filter.select_columns : -- n/a -- table_filter.select_rows : -- n/a -- Links \u00b6 Documentation: https://DHARPA-Project.github.io/kiara_plugin.tabular Code: https://github.com/DHARPA-Project/kiara_plugin.tabular","title":"Home"},{"location":"#kiara-plugin-tabular","text":"This package contains a set of commonly used/useful modules, pipelines, types and metadata schemas for Kiara .","title":"kiara plugin: tabular"},{"location":"#description","text":"kiara data-types and modules for working with tables and databases.","title":"Description"},{"location":"#package-content","text":"","title":"Package content"},{"location":"#data_types","text":"array : An array, in most cases used as a column within a table. database : A database, containing one or several tables. table : Tabular data (table, spreadsheet, data_frame, what have you).","title":"data_types"},{"location":"#module_types","text":"table.filters : -- n/a -- render.database : -- n/a -- render.table : -- n/a -- export.table : Export table data items. load.array : Deserialize array data. load.database : -- n/a -- load.table : -- n/a -- parse.date_array : Create an array of date objects from an array of strings. create.database : -- n/a -- create.table : -- n/a -- query.database : Execute a sql query against a (sqlite) database. table.cut_column : Cut off one column from a table, returning an array. table.merge : Create a table from other tables and/or arrays. query.table : Execute a sql query against an (Arrow) table.","title":"module_types"},{"location":"#kiara_model_types","text":"database_metadata : Database and table properties. kiara_table_metadata : File stats. table_metadata : Describes properties for the 'table' data type. kiara_array : A class to manage array-like data. kiara_database : A wrapper class to manage a sqlite database. kiara_table : A wrapper class to manage tabular data in a memory efficient way.","title":"kiara_model_types"},{"location":"#operations","text":"create.database.from.csv_file : Create a database from a csv_file value. create.database.from.csv_file_bundle : Create a database from a csv_file_bundle value. create.database.from.table : Create a database value from a table. create.table.from.csv_file : Create a table from a csv_file value. create.table.from.text_file_bundle : Create a table value from a text file_bundle. deserialize.array.as.python_object : -- n/a -- deserialize.database.as.python_object : -- n/a -- deserialize.table.as.python_object : -- n/a -- export.table.as.csv_file : Export a table as csv file. extract.date_array.from.table : Extract a date array from a table column. import.database.from.csv_file : Import a database from a csv file. import.table.from.csv_file : Import a table from a csv file. import.table.from.text_file_bundle : Load a table from a bundle of text files. parse.date_array : Create an array of date objects from an array of strings. query.database : Execute a sql query against a (sqlite) database. query.table : Execute a sql query against an (Arrow) table. render.database.as.string : -- n/a -- render.database.as.terminal_renderable : -- n/a -- render.table.as.string : -- n/a -- render.table.as.terminal_renderable : -- n/a -- table.cut_column : Cut off one column from a table, returning an array. table_filter.drop_columns : -- n/a -- table_filter.select_columns : -- n/a -- table_filter.select_rows : -- n/a --","title":"operations"},{"location":"#links","text":"Documentation: https://DHARPA-Project.github.io/kiara_plugin.tabular Code: https://github.com/DHARPA-Project/kiara_plugin.tabular","title":"Links"},{"location":"SUMMARY/","text":"Home Usage Development Package contents API reference","title":"SUMMARY"},{"location":"usage/","text":"Usage \u00b6 TO BE DONE","title":"Usage"},{"location":"usage/#usage","text":"TO BE DONE","title":"Usage"},{"location":"info/SUMMARY/","text":"data_types module_types kiara_model_types operations","title":"SUMMARY"},{"location":"info/data_types/","text":"array \u00b6 lineage array any qualifier profile(s) -- n/a -- Documentation An array, in most cases used as a column within a table. Internally, this type uses the [KiaraArray][kiara_plugin.tabular.models.array.Kiara\u2026 wrapper class to manage array data. This wrapper class, in turn, uses an Apache Arrow Array to store the data in memory (and on disk). Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name ArrayType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraArray python_module_name kiara_plugin.tabular.models.arr\u2026 full_name kiara_plugin.tabular.models.arr\u2026 database \u00b6 lineage database any qualifier profile(s) -- n/a -- Documentation A database, containing one or several tables. This is backed by the [KiaraDatabase][kiara_plugin.tabular.models.db.Kiara\u2026 class to manage the stored data. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name DatabaseType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraDatabase python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.\u2026 table \u00b6 lineage table any qualifier profile(s) -- n/a -- Documentation Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the [ KiaraTable ][kiara_plugin.tabular.models.table.Kiara\u2026 class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: \u2022 KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name TableType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraTable python_module_name kiara_plugin.tabular.models.tab\u2026 full_name kiara_plugin.tabular.models.tab\u2026","title":"data_types"},{"location":"info/data_types/#kiara_info.data_types.array","text":"lineage array any qualifier profile(s) -- n/a -- Documentation An array, in most cases used as a column within a table. Internally, this type uses the [KiaraArray][kiara_plugin.tabular.models.array.Kiara\u2026 wrapper class to manage array data. This wrapper class, in turn, uses an Apache Arrow Array to store the data in memory (and on disk). Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name ArrayType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraArray python_module_name kiara_plugin.tabular.models.arr\u2026 full_name kiara_plugin.tabular.models.arr\u2026","title":"array"},{"location":"info/data_types/#kiara_info.data_types.database","text":"lineage database any qualifier profile(s) -- n/a -- Documentation A database, containing one or several tables. This is backed by the [KiaraDatabase][kiara_plugin.tabular.models.db.Kiara\u2026 class to manage the stored data. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name DatabaseType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraDatabase python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.\u2026","title":"database"},{"location":"info/data_types/#kiara_info.data_types.table","text":"lineage table any qualifier profile(s) -- n/a -- Documentation Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the [ KiaraTable ][kiara_plugin.tabular.models.table.Kiara\u2026 class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: \u2022 KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name TableType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraTable python_module_name kiara_plugin.tabular.models.tab\u2026 full_name kiara_plugin.tabular.models.tab\u2026","title":"table"},{"location":"info/kiara_model_types/","text":"database_metadata \u00b6 Documentation Database and table properties. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name DatabaseMetadata python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.Databas\u2026 kiara_table_metadata \u00b6 Documentation File stats. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name KiaraTableMetadata python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.Kiar\u2026 table_metadata \u00b6 Documentation Describes properties for the 'table' data type. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name TableMetadata python_module_name kiara_plugin.tabular.models full_name kiara_plugin.tabular.models.TableMetad\u2026 kiara_array \u00b6 Documentation A class to manage array-like data. Internally, this uses an Apache Arrow Array to handle the data in memory and on disk. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name KiaraArray python_module_name kiara_plugin.tabular.models.array full_name kiara_plugin.tabular.models.array.Kiar\u2026 kiara_database \u00b6 Documentation A wrapper class to manage a sqlite database. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name KiaraDatabase python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.KiaraDa\u2026 kiara_table \u00b6 Documentation A wrapper class to manage tabular data in a memory efficient way. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name KiaraTable python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.Kiar\u2026","title":"kiara_model_types"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.database_metadata","text":"Documentation Database and table properties. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name DatabaseMetadata python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.Databas\u2026","title":"database_metadata"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.kiara_table_metadata","text":"Documentation File stats. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name KiaraTableMetadata python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.Kiar\u2026","title":"kiara_table_metadata"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.table_metadata","text":"Documentation Describes properties for the 'table' data type. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name TableMetadata python_module_name kiara_plugin.tabular.models full_name kiara_plugin.tabular.models.TableMetad\u2026","title":"table_metadata"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.kiara_array","text":"Documentation A class to manage array-like data. Internally, this uses an Apache Arrow Array to handle the data in memory and on disk. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name KiaraArray python_module_name kiara_plugin.tabular.models.array full_name kiara_plugin.tabular.models.array.Kiar\u2026","title":"kiara_array"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.kiara_database","text":"Documentation A wrapper class to manage a sqlite database. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name KiaraDatabase python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.KiaraDa\u2026","title":"kiara_database"},{"location":"info/kiara_model_types/#kiara_info.kiara_model_types.kiara_table","text":"Documentation A wrapper class to manage tabular data in a memory efficient way. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plugin\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugin.\u2026 Python class python_class_name KiaraTable python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.Kiar\u2026","title":"kiara_table"},{"location":"info/module_types/","text":"table.filters \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. filter_n\u2026 string The name yes of the filter. Python class python_class_name TableFiltersModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 filter_name: str = self . get_config_value( \"filt\u2026 data_type_data = self . __class__ . get_supported_\u2026 data_type = data_type_data[ \"type\" ] # data_type_config = data_type_data[\"type_conf\u2026 # TODO: ensure value is of the right type? source_obj = inputs . get_value_obj( \"value\" ) func_name = f\"filter__{ filter_name }\" if not hasattr(self, func_name): raise Exception ( f\"Can't apply filter '{ filter_name }': \u2026 ) func = getattr(self, func_name) # TODO: check signature? filter_inputs = {} for k, v in inputs . items(): if k == data_type: continue filter_inputs[k] = v . data result = func(value = source_obj, filter_inputs = \u2026 if result is None : outputs . set_value( \"value\" , source_obj) else : outputs . set_value( \"value\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 render.database \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. source_t\u2026 string The yes (kiara) data type to be rendered. target_t\u2026 string The yes (kiara) data type of210 the rendered result. Python class python_class_name RenderDatabaseModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 value: Value = inputs . get_value_obj( \"value\" ) render_scene: DictModel = inputs . get_value_dat\u2026 if render_scene: rc = render_scene . dict_data else : rc = {} func_name = f\"render__{ source_type }__as__{ targ\u2026 func = getattr(self, func_name) result = func(value = value, render_config = rc) if isinstance(result, RenderValueResult): render_scene_result: RenderValueResult = r\u2026 else : render_scene_result = RenderValueResult( value_id = value . value_id, render_config = rc, render_manifest = self . manifest . manifest\u2026 rendered = result, related_scenes = {}, ) render_scene_result . manifest_lookup[self . manif\u2026 outputs . set_value( \"render_value_result\" , rende\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 render.table \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. source_t\u2026 string The yes (kiara) data type to be rendered. target_t\u2026 string The yes (kiara) data type of210 the rendered result. Python class python_class_name RenderTableModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 value: Value = inputs . get_value_obj( \"value\" ) render_scene: DictModel = inputs . get_value_dat\u2026 if render_scene: rc = render_scene . dict_data else : rc = {} func_name = f\"render__{ source_type }__as__{ targ\u2026 func = getattr(self, func_name) result = func(value = value, render_config = rc) if isinstance(result, RenderValueResult): render_scene_result: RenderValueResult = r\u2026 else : render_scene_result = RenderValueResult( value_id = value . value_id, render_config = rc, render_manifest = self . manifest . manifest\u2026 rendered = result, related_scenes = {}, ) render_scene_result . manifest_lookup[self . manif\u2026 outputs . set_value( \"render_value_result\" , rende\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 export.table \u00b6 Documentation Export table data items. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. source_t\u2026 string The type yes of the source data that is going to be exported. target_p\u2026 string The name yes of the target profile. Used to distingu\u2026 different target formats for the same data type. Python class python_class_name ExportTableModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 target_profile: str = self . get_config_value( \"t\u2026 source_type: str = self . get_config_value( \"sour\u2026 export_metadata = inputs . get_value_data( \"expor\u2026 source_obj = inputs . get_value_obj(source_type) source = source_obj . data func_name = f\"export__{ source_type }__as__{ targ\u2026 if not hasattr(self, func_name): raise Exception ( f\"Can't export '{ source_type }' value: \u2026 ) base_path = inputs . get_value_data( \"base_path\" ) if base_path is None : base_path = os . getcwd() name = inputs . get_value_data( \"name\" ) if not name: name = str(source_obj . value_id) func = getattr(self, func_name) # TODO: check signature? base_path = os . path . abspath(base_path) os . makedirs(base_path, exist_ok = True ) result = func(value = source, base_path = base_pat\u2026 if isinstance(result, Mapping): result = DataExportResult( ** result) elif isinstance(result, str): result = DataExportResult(files = [result]) if not isinstance(result, DataExportResult): raise KiaraProcessingException( f\"Can't export value: invalid result t\u2026 ) if export_metadata: metadata_file = Path(os . path . join(base_pat\u2026 value_info = source_obj . create_info() value_json = value_info . json() metadata_file . write_text(value_json) result . files . append(metadata_file . as_posix\u2026 # schema = ValueSchema(type=self.get_target_va\u2026 # value_lineage = ValueLineage.from_module_and\u2026 # module=self, output_name=output_key, inp\u2026 # ) # value: Value = self._kiara.data_registry.reg\u2026 # value_data=result, value_schema=schema, \u2026 # ) outputs . set_value( \"export_details\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load.array \u00b6 Documentation Deserialize array data. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. serializ\u2026 string The name yes of the serializ\u2026 profile used to serialize the source value. target_p\u2026 string The yes profile name of the de-seria\u2026 result data. value_ty\u2026 string The value yes type of the actual (unseria\u2026 value. Python class python_class_name DeserializeArrayModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 value_type = self . get_config_value( \"value_type\u2026 serialized_value = inputs . get_value_obj(value_\u2026 config = inputs . get_value_obj( \"deserialization\u2026 target_profile = self . get_config_value( \"target\u2026 func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . seria\u2026 outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load.database \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. serializ\u2026 string The name yes of the serializ\u2026 profile used to serialize the source value. target_p\u2026 string The yes profile name of the de-seria\u2026 result data. value_ty\u2026 string The value yes type of the actual (unseria\u2026 value. Python class python_class_name LoadDatabaseFromDiskModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 value_type = self . get_config_value( \"value_type\u2026 serialized_value = inputs . get_value_obj(value_\u2026 config = inputs . get_value_obj( \"deserialization\u2026 target_profile = self . get_config_value( \"target\u2026 func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . seria\u2026 outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load.table \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. serializ\u2026 string The name yes of the serializ\u2026 profile used to serialize the source value. target_p\u2026 string The yes profile name of the de-seria\u2026 result data. value_ty\u2026 string The value yes type of the actual (unseria\u2026 value. Python class python_class_name DeserializeTableModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 value_type = self . get_config_value( \"value_type\u2026 serialized_value = inputs . get_value_obj(value_\u2026 config = inputs . get_value_obj( \"deserialization\u2026 target_profile = self . get_config_value( \"target\u2026 func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . seria\u2026 outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 parse.date_array \u00b6 Documentation Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the dateutil package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: \u2022 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) \u2022 remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 add_inp\u2026 boolean If set to no true 'True', parse options will be available as inputs. constan\u2026 object Value no constants for this module. defaults object Value no defaults for this module. force_n\u2026 boolean If set to no true 'True', raise an error if any of the strings in the array can't be parsed. input_f\u2026 array If not no empty, only add the fields specified in here to the module inputs schema. max_ind\u2026 integer The no maximum index until whic to parse the string(s\u2026 min_ind\u2026 integer The no minimum index from where to start parsing the string(s\u2026 remove_\u2026 array A list of no tokens/c\u2026 to replace with a single white-sp\u2026 before parsing the input. Python class python_class_name ExtractDateModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import polars as pl import pyarrow as pa from dateutil import parser force_non_null: bool = self . get_data_for_field( field_name = \"force_non_null\" , inputs = inputs ) min_pos: Union[ None , int] = self . get_data_for_\u2026 field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos: Union[ None , int] = self . get_data_for_\u2026 field_name = \"max_index\" , inputs = inputs ) remove_tokens: Iterable[str] = self . get_data_f\u2026 field_name = \"remove_tokens\" , inputs = inputs ) def parse_date (_text: str): text = _text if min_pos: try : text = text[min_pos:] # type: ign\u2026 except Exception : return None if max_pos: try : text = text[ 0 : max_pos - min_pos]\u2026 except Exception : pass if remove_tokens: for t in remove_tokens: text = text . replace(t, \" \" ) try : d_obj = parser . parse(text, fuzzy = True ) except Exception as e: if force_non_null: raise KiaraProcessingException(e) return None if d_obj is None : if force_non_null: raise KiaraProcessingException( f\"Can't parse date from string\u2026 ) return None return d_obj value = inputs . get_value_obj( \"array\" ) array: KiaraArray = value . data series = pl . Series(name = \"tokens\" , values = array \u2026 job_log . add_log( f\"start parsing date for { len(\u2026 result = series . apply(parse_date) job_log . add_log( f\"finished parsing date for { l\u2026 result_array = result . to_arrow() # TODO: remove this cast once the array data t\u2026 chunked = pa . chunked_array(result_array) outputs . set_values(date_array = chunked) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 create.database \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descrip\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constan\u2026 for this module. defaults object Value no defaults for this module. ignore_e\u2026 boolean Whether no false to ignore convert errors and omit the failed items. include_\u2026 boolean When no false includi\u2026 source metadat\u2026 whether to also include the original raw (string) content. include_\u2026 boolean Whether no to include a table with metadata about the source files. merge_in\u2026 boolean Whether no false to merge all csv files into a single table. source_t\u2026 string The yes value type of the source value. target_t\u2026 string The yes value type of the target. Python class python_class_name CreateDatabaseModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 func_name = f\"create__{ target_type }__from__{ so\u2026 func = getattr(self, func_name) source_value = inputs . get_value_obj(source_typ\u2026 signature = inspect . signature(func) if \"optional\" in signature . parameters: optional: Dict[str, Value] = {} op_schemas = {} for field, schema in self . inputs_schema . it\u2026 if field == source_type: continue optional[field] = inputs . get_value_obj\u2026 op_schemas[field] = schema result = func( source_value = source_value, optional = ValueMapReadOnly( value_items = optional, values_schem\u2026 ), ) else : result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 create.table \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descrip\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constan\u2026 for this module. defaults object Value no defaults for this module. ignore_e\u2026 boolean Whether no false to ignore convert errors and omit the failed items. source_t\u2026 string The yes value type of the source value. target_t\u2026 string The yes value type of the target. Python class python_class_name CreateTableModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 func_name = f\"create__{ target_type }__from__{ so\u2026 func = getattr(self, func_name) source_value = inputs . get_value_obj(source_typ\u2026 signature = inspect . signature(func) if \"optional\" in signature . parameters: optional: Dict[str, Value] = {} op_schemas = {} for field, schema in self . inputs_schema . it\u2026 if field == source_type: continue optional[field] = inputs . get_value_obj\u2026 op_schemas[field] = schema result = func( source_value = source_value, optional = ValueMapReadOnly( value_items = optional, values_schem\u2026 ), ) else : result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query.database \u00b6 Documentation Execute a sql query against a (sqlite) database. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. query string The no query. Python class python_class_name QueryDatabaseModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import pyarrow as pa database: KiaraDatabase = inputs . get_value_dat\u2026 query = self . get_config_value( \"query\" ) if query is None : query = inputs . get_value_data( \"query\" ) # TODO: make this memory efficent result_columns: Dict[str, List[Any]] = {} with database . get_sqlalchemy_engine() . connect(\u2026 result = con . execute(text(query)) for r in result: for k, v in dict(r) . items(): result_columns . setdefault(k, []) . a\u2026 table = pa . Table . from_pydict(result_columns) outputs . set_value( \"query_result\" , table) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table.cut_column \u00b6 Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. Python class python_class_name CutColumnModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import pyarrow as pa column_name: str = inputs . get_value_data( \"colu\u2026 table_value: Value = inputs . get_value_obj( \"tab\u2026 table_metadata: KiaraTableMetadata = table_val\u2026 \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available: raise KiaraProcessingException( f\"Invalid column name '{ column_name }'.\u2026 ) table: pa . Table = table_value . data . arrow_table column = table . column(column_name) outputs . set_value( \"array\" , column) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table.merge \u00b6 Documentation Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: \u2022 '[inputs_schema key]' for inputs of type 'array' \u2022 '[inputs_schema_key].orig_column_name' for inputs of type 'table' Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 column_m\u2026 object A map no describi\u2026 constants object Value no constants for this module. defaults object Value no defaults for this module. inputs_s\u2026 object A dict yes describi\u2026 the inputs for this merge process. Python class python_class_name MergeTableModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import pyarrow as pa inputs_schema: Dict[str, Any] = self . get_confi\u2026 column_map: Dict[str, str] = self . get_config_v\u2026 sources = {} for field_name in inputs_schema . keys(): sources[field_name] = inputs . get_value_dat\u2026 len_dict = {} arrays = {} column_map_final = dict(column_map) for source_key, table_or_array in sources . item\u2026 if isinstance(table_or_array, KiaraTable): rows = table_or_array . num_rows for name in table_or_array . column_name\u2026 array_name = f\"{ source_key }.{ name }\" if column_map and array_name not i\u2026 job_log . add_log( f\"Ignoring column '{ name }'\u2026 ) continue column = table_or_array . arrow_tabl\u2026 arrays[array_name] = column if not column_map: if name in column_map_final: raise Exception ( f\"Can't merge table, d\u2026 ) column_map_final[name] = array\u2026 elif isinstance(table_or_array, KiaraArray\u2026 if column_map and source_key not in co\u2026 job_log . add_log( f\"Ignoring array '{ source_key }\u2026 ) continue rows = len(table_or_array) arrays[source_key] = table_or_array . ar\u2026 if not column_map: if source_key in column_map_final . \u2026 raise Exception ( f\"Can't merge table, dupli\u2026 ) column_map_final[source_key] = sou\u2026 else : raise KiaraProcessingException( f\"Can't merge table: invalid type \u2026 ) len_dict[source_key] = rows all_rows = None for source_key, rows in len_dict . items(): if all_rows is None : all_rows = rows else : if all_rows != rows: all_rows = None break if all_rows is None : len_str = \"\" for name, rows in len_dict . items(): len_str = f\" { name } ({ rows })\" raise KiaraProcessingException( f\"Can't merge table, sources have diff\u2026 ) column_names = [] columns = [] for column_name, ref in column_map_final . items\u2026 column_names . append(column_name) column = arrays[ref] columns . append(column) table = pa . Table . from_arrays(arrays = columns, n\u2026 outputs . set_value( \"table\" , table) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query.table \u00b6 Documentation Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. query string The query no to execute. If not specifie\u2026 the user will be able to provide their own. relation\u2026 string The name no \"data\" the table is referred to in the sql query. If not specifie\u2026 the user will be able to provide their own. Python class python_class_name QueryTableSQL python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import duckdb if self . get_config_value( \"query\" ) is None : _query: str = inputs . get_value_data( \"query\u2026 _relation_name: str = inputs . get_value_dat\u2026 else : _query = self . get_config_value( \"query\" ) _relation_name = self . get_config_value( \"re\u2026 if _relation_name . upper() in RESERVED_SQL_KEYW\u2026 raise KiaraProcessingException( f\"Invalid relation name '{ _relation_na\u2026 ) _table: KiaraTable = inputs . get_value_data( \"ta\u2026 rel_from_arrow = duckdb . arrow(_table . arrow_tab\u2026 result: duckdb . DuckDBPyRelation = rel_from_arr\u2026 outputs . set_value( \"query_result\" , result . arrow\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"module_types"},{"location":"info/module_types/#kiara_info.module_types.table.filters","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. filter_n\u2026 string The name yes of the filter. Python class python_class_name TableFiltersModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 filter_name: str = self . get_config_value( \"filt\u2026 data_type_data = self . __class__ . get_supported_\u2026 data_type = data_type_data[ \"type\" ] # data_type_config = data_type_data[\"type_conf\u2026 # TODO: ensure value is of the right type? source_obj = inputs . get_value_obj( \"value\" ) func_name = f\"filter__{ filter_name }\" if not hasattr(self, func_name): raise Exception ( f\"Can't apply filter '{ filter_name }': \u2026 ) func = getattr(self, func_name) # TODO: check signature? filter_inputs = {} for k, v in inputs . items(): if k == data_type: continue filter_inputs[k] = v . data result = func(value = source_obj, filter_inputs = \u2026 if result is None : outputs . set_value( \"value\" , source_obj) else : outputs . set_value( \"value\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"table.filters"},{"location":"info/module_types/#kiara_info.module_types.render.database","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. source_t\u2026 string The yes (kiara) data type to be rendered. target_t\u2026 string The yes (kiara) data type of210 the rendered result. Python class python_class_name RenderDatabaseModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 value: Value = inputs . get_value_obj( \"value\" ) render_scene: DictModel = inputs . get_value_dat\u2026 if render_scene: rc = render_scene . dict_data else : rc = {} func_name = f\"render__{ source_type }__as__{ targ\u2026 func = getattr(self, func_name) result = func(value = value, render_config = rc) if isinstance(result, RenderValueResult): render_scene_result: RenderValueResult = r\u2026 else : render_scene_result = RenderValueResult( value_id = value . value_id, render_config = rc, render_manifest = self . manifest . manifest\u2026 rendered = result, related_scenes = {}, ) render_scene_result . manifest_lookup[self . manif\u2026 outputs . set_value( \"render_value_result\" , rende\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"render.database"},{"location":"info/module_types/#kiara_info.module_types.render.table","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. source_t\u2026 string The yes (kiara) data type to be rendered. target_t\u2026 string The yes (kiara) data type of210 the rendered result. Python class python_class_name RenderTableModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 value: Value = inputs . get_value_obj( \"value\" ) render_scene: DictModel = inputs . get_value_dat\u2026 if render_scene: rc = render_scene . dict_data else : rc = {} func_name = f\"render__{ source_type }__as__{ targ\u2026 func = getattr(self, func_name) result = func(value = value, render_config = rc) if isinstance(result, RenderValueResult): render_scene_result: RenderValueResult = r\u2026 else : render_scene_result = RenderValueResult( value_id = value . value_id, render_config = rc, render_manifest = self . manifest . manifest\u2026 rendered = result, related_scenes = {}, ) render_scene_result . manifest_lookup[self . manif\u2026 outputs . set_value( \"render_value_result\" , rende\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"render.table"},{"location":"info/module_types/#kiara_info.module_types.export.table","text":"Documentation Export table data items. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. source_t\u2026 string The type yes of the source data that is going to be exported. target_p\u2026 string The name yes of the target profile. Used to distingu\u2026 different target formats for the same data type. Python class python_class_name ExportTableModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 target_profile: str = self . get_config_value( \"t\u2026 source_type: str = self . get_config_value( \"sour\u2026 export_metadata = inputs . get_value_data( \"expor\u2026 source_obj = inputs . get_value_obj(source_type) source = source_obj . data func_name = f\"export__{ source_type }__as__{ targ\u2026 if not hasattr(self, func_name): raise Exception ( f\"Can't export '{ source_type }' value: \u2026 ) base_path = inputs . get_value_data( \"base_path\" ) if base_path is None : base_path = os . getcwd() name = inputs . get_value_data( \"name\" ) if not name: name = str(source_obj . value_id) func = getattr(self, func_name) # TODO: check signature? base_path = os . path . abspath(base_path) os . makedirs(base_path, exist_ok = True ) result = func(value = source, base_path = base_pat\u2026 if isinstance(result, Mapping): result = DataExportResult( ** result) elif isinstance(result, str): result = DataExportResult(files = [result]) if not isinstance(result, DataExportResult): raise KiaraProcessingException( f\"Can't export value: invalid result t\u2026 ) if export_metadata: metadata_file = Path(os . path . join(base_pat\u2026 value_info = source_obj . create_info() value_json = value_info . json() metadata_file . write_text(value_json) result . files . append(metadata_file . as_posix\u2026 # schema = ValueSchema(type=self.get_target_va\u2026 # value_lineage = ValueLineage.from_module_and\u2026 # module=self, output_name=output_key, inp\u2026 # ) # value: Value = self._kiara.data_registry.reg\u2026 # value_data=result, value_schema=schema, \u2026 # ) outputs . set_value( \"export_details\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"export.table"},{"location":"info/module_types/#kiara_info.module_types.load.array","text":"Documentation Deserialize array data. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. serializ\u2026 string The name yes of the serializ\u2026 profile used to serialize the source value. target_p\u2026 string The yes profile name of the de-seria\u2026 result data. value_ty\u2026 string The value yes type of the actual (unseria\u2026 value. Python class python_class_name DeserializeArrayModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 value_type = self . get_config_value( \"value_type\u2026 serialized_value = inputs . get_value_obj(value_\u2026 config = inputs . get_value_obj( \"deserialization\u2026 target_profile = self . get_config_value( \"target\u2026 func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . seria\u2026 outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"load.array"},{"location":"info/module_types/#kiara_info.module_types.load.database","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. serializ\u2026 string The name yes of the serializ\u2026 profile used to serialize the source value. target_p\u2026 string The yes profile name of the de-seria\u2026 result data. value_ty\u2026 string The value yes type of the actual (unseria\u2026 value. Python class python_class_name LoadDatabaseFromDiskModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 value_type = self . get_config_value( \"value_type\u2026 serialized_value = inputs . get_value_obj(value_\u2026 config = inputs . get_value_obj( \"deserialization\u2026 target_profile = self . get_config_value( \"target\u2026 func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . seria\u2026 outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"load.database"},{"location":"info/module_types/#kiara_info.module_types.load.table","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. serializ\u2026 string The name yes of the serializ\u2026 profile used to serialize the source value. target_p\u2026 string The yes profile name of the de-seria\u2026 result data. value_ty\u2026 string The value yes type of the actual (unseria\u2026 value. Python class python_class_name DeserializeTableModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 value_type = self . get_config_value( \"value_type\u2026 serialized_value = inputs . get_value_obj(value_\u2026 config = inputs . get_value_obj( \"deserialization\u2026 target_profile = self . get_config_value( \"target\u2026 func_name = f\"to__{ target_profile }\" func = getattr(self, func_name) if config . is_set: _config = config . data else : _config = {} result: Any = func(data = serialized_value . seria\u2026 outputs . set_value( \"python_object\" , result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"load.table"},{"location":"info/module_types/#kiara_info.module_types.parse.date_array","text":"Documentation Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the dateutil package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: \u2022 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) \u2022 remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 add_inp\u2026 boolean If set to no true 'True', parse options will be available as inputs. constan\u2026 object Value no constants for this module. defaults object Value no defaults for this module. force_n\u2026 boolean If set to no true 'True', raise an error if any of the strings in the array can't be parsed. input_f\u2026 array If not no empty, only add the fields specified in here to the module inputs schema. max_ind\u2026 integer The no maximum index until whic to parse the string(s\u2026 min_ind\u2026 integer The no minimum index from where to start parsing the string(s\u2026 remove_\u2026 array A list of no tokens/c\u2026 to replace with a single white-sp\u2026 before parsing the input. Python class python_class_name ExtractDateModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import polars as pl import pyarrow as pa from dateutil import parser force_non_null: bool = self . get_data_for_field( field_name = \"force_non_null\" , inputs = inputs ) min_pos: Union[ None , int] = self . get_data_for_\u2026 field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos: Union[ None , int] = self . get_data_for_\u2026 field_name = \"max_index\" , inputs = inputs ) remove_tokens: Iterable[str] = self . get_data_f\u2026 field_name = \"remove_tokens\" , inputs = inputs ) def parse_date (_text: str): text = _text if min_pos: try : text = text[min_pos:] # type: ign\u2026 except Exception : return None if max_pos: try : text = text[ 0 : max_pos - min_pos]\u2026 except Exception : pass if remove_tokens: for t in remove_tokens: text = text . replace(t, \" \" ) try : d_obj = parser . parse(text, fuzzy = True ) except Exception as e: if force_non_null: raise KiaraProcessingException(e) return None if d_obj is None : if force_non_null: raise KiaraProcessingException( f\"Can't parse date from string\u2026 ) return None return d_obj value = inputs . get_value_obj( \"array\" ) array: KiaraArray = value . data series = pl . Series(name = \"tokens\" , values = array \u2026 job_log . add_log( f\"start parsing date for { len(\u2026 result = series . apply(parse_date) job_log . add_log( f\"finished parsing date for { l\u2026 result_array = result . to_arrow() # TODO: remove this cast once the array data t\u2026 chunked = pa . chunked_array(result_array) outputs . set_values(date_array = chunked) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"parse.date_array"},{"location":"info/module_types/#kiara_info.module_types.create.database","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descrip\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constan\u2026 for this module. defaults object Value no defaults for this module. ignore_e\u2026 boolean Whether no false to ignore convert errors and omit the failed items. include_\u2026 boolean When no false includi\u2026 source metadat\u2026 whether to also include the original raw (string) content. include_\u2026 boolean Whether no to include a table with metadata about the source files. merge_in\u2026 boolean Whether no false to merge all csv files into a single table. source_t\u2026 string The yes value type of the source value. target_t\u2026 string The yes value type of the target. Python class python_class_name CreateDatabaseModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 func_name = f\"create__{ target_type }__from__{ so\u2026 func = getattr(self, func_name) source_value = inputs . get_value_obj(source_typ\u2026 signature = inspect . signature(func) if \"optional\" in signature . parameters: optional: Dict[str, Value] = {} op_schemas = {} for field, schema in self . inputs_schema . it\u2026 if field == source_type: continue optional[field] = inputs . get_value_obj\u2026 op_schemas[field] = schema result = func( source_value = source_value, optional = ValueMapReadOnly( value_items = optional, values_schem\u2026 ), ) else : result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"create.database"},{"location":"info/module_types/#kiara_info.module_types.create.table","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descrip\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constan\u2026 for this module. defaults object Value no defaults for this module. ignore_e\u2026 boolean Whether no false to ignore convert errors and omit the failed items. source_t\u2026 string The yes value type of the source value. target_t\u2026 string The yes value type of the target. Python class python_class_name CreateTableModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 func_name = f\"create__{ target_type }__from__{ so\u2026 func = getattr(self, func_name) source_value = inputs . get_value_obj(source_typ\u2026 signature = inspect . signature(func) if \"optional\" in signature . parameters: optional: Dict[str, Value] = {} op_schemas = {} for field, schema in self . inputs_schema . it\u2026 if field == source_type: continue optional[field] = inputs . get_value_obj\u2026 op_schemas[field] = schema result = func( source_value = source_value, optional = ValueMapReadOnly( value_items = optional, values_schem\u2026 ), ) else : result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"create.table"},{"location":"info/module_types/#kiara_info.module_types.query.database","text":"Documentation Execute a sql query against a (sqlite) database. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. query string The no query. Python class python_class_name QueryDatabaseModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import pyarrow as pa database: KiaraDatabase = inputs . get_value_dat\u2026 query = self . get_config_value( \"query\" ) if query is None : query = inputs . get_value_data( \"query\" ) # TODO: make this memory efficent result_columns: Dict[str, List[Any]] = {} with database . get_sqlalchemy_engine() . connect(\u2026 result = con . execute(text(query)) for r in result: for k, v in dict(r) . items(): result_columns . setdefault(k, []) . a\u2026 table = pa . Table . from_pydict(result_columns) outputs . set_value( \"query_result\" , table) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"query.database"},{"location":"info/module_types/#kiara_info.module_types.table.cut_column","text":"Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. Python class python_class_name CutColumnModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import pyarrow as pa column_name: str = inputs . get_value_data( \"colu\u2026 table_value: Value = inputs . get_value_obj( \"tab\u2026 table_metadata: KiaraTableMetadata = table_val\u2026 \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available: raise KiaraProcessingException( f\"Invalid column name '{ column_name }'.\u2026 ) table: pa . Table = table_value . data . arrow_table column = table . column(column_name) outputs . set_value( \"array\" , column) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"table.cut_column"},{"location":"info/module_types/#kiara_info.module_types.table.merge","text":"Documentation Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: \u2022 '[inputs_schema key]' for inputs of type 'array' \u2022 '[inputs_schema_key].orig_column_name' for inputs of type 'table' Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 column_m\u2026 object A map no describi\u2026 constants object Value no constants for this module. defaults object Value no defaults for this module. inputs_s\u2026 object A dict yes describi\u2026 the inputs for this merge process. Python class python_class_name MergeTableModule python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import pyarrow as pa inputs_schema: Dict[str, Any] = self . get_confi\u2026 column_map: Dict[str, str] = self . get_config_v\u2026 sources = {} for field_name in inputs_schema . keys(): sources[field_name] = inputs . get_value_dat\u2026 len_dict = {} arrays = {} column_map_final = dict(column_map) for source_key, table_or_array in sources . item\u2026 if isinstance(table_or_array, KiaraTable): rows = table_or_array . num_rows for name in table_or_array . column_name\u2026 array_name = f\"{ source_key }.{ name }\" if column_map and array_name not i\u2026 job_log . add_log( f\"Ignoring column '{ name }'\u2026 ) continue column = table_or_array . arrow_tabl\u2026 arrays[array_name] = column if not column_map: if name in column_map_final: raise Exception ( f\"Can't merge table, d\u2026 ) column_map_final[name] = array\u2026 elif isinstance(table_or_array, KiaraArray\u2026 if column_map and source_key not in co\u2026 job_log . add_log( f\"Ignoring array '{ source_key }\u2026 ) continue rows = len(table_or_array) arrays[source_key] = table_or_array . ar\u2026 if not column_map: if source_key in column_map_final . \u2026 raise Exception ( f\"Can't merge table, dupli\u2026 ) column_map_final[source_key] = sou\u2026 else : raise KiaraProcessingException( f\"Can't merge table: invalid type \u2026 ) len_dict[source_key] = rows all_rows = None for source_key, rows in len_dict . items(): if all_rows is None : all_rows = rows else : if all_rows != rows: all_rows = None break if all_rows is None : len_str = \"\" for name, rows in len_dict . items(): len_str = f\" { name } ({ rows })\" raise KiaraProcessingException( f\"Can't merge table, sources have diff\u2026 ) column_names = [] columns = [] for column_name, ref in column_map_final . items\u2026 column_names . append(column_name) column = arrays[ref] columns . append(column) table = pa . Table . from_arrays(arrays = columns, n\u2026 outputs . set_value( \"table\" , table) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"table.merge"},{"location":"info/module_types/#kiara_info.module_types.query.table","text":"Documentation Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. query string The query no to execute. If not specifie\u2026 the user will be able to provide their own. relation\u2026 string The name no \"data\" the table is referred to in the sql query. If not specifie\u2026 the user will be able to provide their own. Python class python_class_name QueryTableSQL python_module_name kiara_plugin.tabular.modules.\u2026 full_name kiara_plugin.tabular.modules.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import duckdb if self . get_config_value( \"query\" ) is None : _query: str = inputs . get_value_data( \"query\u2026 _relation_name: str = inputs . get_value_dat\u2026 else : _query = self . get_config_value( \"query\" ) _relation_name = self . get_config_value( \"re\u2026 if _relation_name . upper() in RESERVED_SQL_KEYW\u2026 raise KiaraProcessingException( f\"Invalid relation name '{ _relation_na\u2026 ) _table: KiaraTable = inputs . get_value_data( \"ta\u2026 rel_from_arrow = duckdb . arrow(_table . arrow_tab\u2026 result: duckdb . DuckDBPyRelation = rel_from_arr\u2026 outputs . set_value( \"query_result\" , result . arrow\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"query.table"},{"location":"info/operations/","text":"create.database.from.csv_file \u00b6 Documentation Create a database from a csv_file value. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create a database from a csv_file value. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_f csv_\u2026 The yes -- no ile type defa\u2026 of -- the sour\u2026 valu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value. create.database.from.csv_file_bundle \u00b6 Documentation Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_f csv_\u2026 The yes -- no ile_b type defa\u2026 undle of -- the sour\u2026 valu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value. create.database.from.table \u00b6 Documentation Create a database value from a table. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create a database value from a table. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The yes -- no type defa\u2026 of -- the sour\u2026 valu\u2026 table stri\u2026 The no impo\u2026 _name name of the table in the new data\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value. create.table.from.csv_file \u00b6 Documentation Create a table from a csv_file value. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create a table from a csv_file value. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_f csv_\u2026 The yes -- no ile type defa\u2026 of -- the sour\u2026 valu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value. create.table.from.text_file_bundle \u00b6 Documentation Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: \u2022 id: an auto-assigned index \u2022 rel_path: the relative path of the file (from the provided base path) \u2022 content: the text file content Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 text_ text\u2026 The yes -- no file_ type defa\u2026 bundl of -- e the sour\u2026 valu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value. deserialize.array.as.python_object \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The yes -- value no obje\u2026 def\u2026 -- deser any Seri\u2026 no -- ializ spec\u2026 no ation conf\u2026 def\u2026 _conf -- ig Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_obj python_ob\u2026 The ect deseriali\u2026 python object instance. deserialize.database.as.python_object \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 datab data\u2026 The yes -- no ase value defa\u2026 obje\u2026 -- deser any Seri\u2026 no -- no ializ spec\u2026 defa\u2026 ation conf\u2026 -- _conf ig Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_obj python_ob\u2026 The ect deseriali\u2026 python object instance. deserialize.table.as.python_object \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The yes -- value no obje\u2026 def\u2026 -- deser any Seri\u2026 no -- ializ spec\u2026 no ation conf\u2026 def\u2026 _conf -- ig Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_obj python_ob\u2026 The ect deseriali\u2026 python object instance. export.table.as.csv_file \u00b6 Documentation Export a table as csv file. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Export a table as csv file. Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table A yes -- value no of def\u2026 type -- 'tab\u2026 base_ stri\u2026 The no -- path dire\u2026 no to def\u2026 expo\u2026 -- the file\u2026 to. name stri\u2026 The no -- (bas\u2026 no name def\u2026 of -- the expo\u2026 file\u2026 expor bool\u2026 Whet\u2026 no Fal\u2026 t_met to adata also expo\u2026 the value meta\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 export_detail dict Details about s the exported files/folder\u2026 extract.date_array.from.table \u00b6 Documentation Extract a date array from a table column. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Extract a date array from a table column. Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table A yes -- tabl\u2026 no def\u2026 -- colum stri\u2026 The yes -- n_nam name no e of def\u2026 the -- colu\u2026 to extr\u2026 parse bool\u2026 If no True _date set _arra to y__fo 'Tru\u2026 rce_n raise on_nu an ll error if any of the stri\u2026 in the array can't be pars\u2026 parse inte\u2026 The no -- _date mini\u2026 no _arra index def\u2026 y__mi from -- n_ind where ex to start pars\u2026 the stri\u2026 parse inte\u2026 The no -- _date maxi\u2026 no _arra index def\u2026 y__ma until -- x_ind whic ex to parse the stri\u2026 parse list A no [] _date list _arra of y__re toke\u2026 move_ to token repl\u2026 s with a sing\u2026 whit\u2026 befo\u2026 pars\u2026 the inpu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 date_array array The resulting array with items of a date data type. import.database.from.csv_file \u00b6 Documentation Import a database from a csv file. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Import a database from a csv file. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 path stri\u2026 The yes -- no local defa\u2026 path -- to the file. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 imported_fi file The loaded le files. database database The result value. import.table.from.csv_file \u00b6 Documentation Import a table from a csv file. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Import a table from a csv file. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 path stri\u2026 The yes -- no local defa\u2026 path -- to the file. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 imported_file file The loaded files. table table The result value. import.table.from.text_file_bundle \u00b6 Documentation Load a table from a bundle of text files. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Load a table from a bundle of text files. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 path stri\u2026 The yes -- no local defa\u2026 path -- of the fold\u2026 to impo\u2026 Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 imported_f file_bund\u2026 The ile_bundle imported file bundle. table table The result value. parse.date_array \u00b6 Documentation Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the dateutil package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: \u2022 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) \u2022 remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the (https://github.com/dateutil/dateutil) package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: - 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) - remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The yes -- input no arra\u2026 def\u2026 -- force bool\u2026 If no True _non_ set null to 'Tru\u2026 raise an error if any of the stri\u2026 in the array can't be pars\u2026 min_i inte\u2026 The no -- ndex mini\u2026 no index def\u2026 from -- where to start pars\u2026 the stri\u2026 max_i inte\u2026 The no -- ndex maxi\u2026 no index def\u2026 until -- whic to parse the stri\u2026 remov list A no [] e_tok list ens of toke\u2026 to repl\u2026 with a sing\u2026 whit\u2026 befo\u2026 pars\u2026 the inpu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 date_array array The resulting array with items of a date data type. query.database \u00b6 Documentation Execute a sql query against a (sqlite) database. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Execute a sql query against a (sqlite) database. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 datab data\u2026 The yes -- no ase data\u2026 defa\u2026 to -- quer\u2026 query stri\u2026 The yes -- no query defa\u2026 to -- exec\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query_result table The query result. query.table \u00b6 Documentation Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The yes -- no table defa\u2026 to -- query query stri\u2026 The yes -- no quer\u2026 defa\u2026 -- relat stri\u2026 The no data ion_n name ame the table is refe\u2026 to in the sql quer\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query_result table The query result. render.database.as.string \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value data\u2026 A yes -- no value defa\u2026 of -- type 'dat\u2026 rende dict Inst\u2026 no {} r_con on fig how (or what) to rend\u2026 the prov\u2026 valu\u2026 Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 render_val render_va\u2026 The ue_result rendered value, incl. some metadata. render.database.as.terminal_renderable \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value data\u2026 A yes -- no value defa\u2026 of -- type 'dat\u2026 rende dict Inst\u2026 no {} r_con on fig how (or what) to rend\u2026 the prov\u2026 valu\u2026 Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 render_val render_va\u2026 The ue_result rendered value, incl. some metadata. render.table.as.string \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table A yes -- value no of def\u2026 type -- 'tab\u2026 rende dict Inst\u2026 no {} r_con on fig how (or what) to rend\u2026 the prov\u2026 valu\u2026 Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 render_val render_va\u2026 The ue_result rendered value, incl. some metadata. render.table.as.terminal_renderable \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table A yes -- value no of def\u2026 type -- 'tab\u2026 rende dict Inst\u2026 no {} r_con on fig how (or what) to rend\u2026 the prov\u2026 valu\u2026 Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 render_val render_va\u2026 The ue_result rendered value, incl. some metadata. table.cut_column \u00b6 Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Cut off one column from a table, returning an array. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table A yes -- no tabl\u2026 defa\u2026 -- colum stri\u2026 The yes -- no n_nam name defa\u2026 e of -- the colu\u2026 to extr\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The column. table_filter.drop_columns \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table A yes -- value no of def\u2026 type -- 'tab\u2026 colum list The no -- ns name no of def\u2026 the -- colu\u2026 to incl\u2026 ignor bool\u2026 Whet\u2026 no True e_inv to alid_ igno\u2026 colum inva\u2026 n_nam colu\u2026 es name\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table The filtered value. table_filter.select_columns \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table A yes -- value no of def\u2026 type -- 'tab\u2026 colum list The no -- ns name no of def\u2026 the -- colu\u2026 to incl\u2026 ignor bool\u2026 Whet\u2026 no True e_inv to alid_ igno\u2026 colum inva\u2026 n_nam colu\u2026 es name\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table The filtered value. table_filter.select_rows \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table A yes -- value no of def\u2026 type -- 'tab\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table The filtered value.","title":"operations"},{"location":"info/operations/#kiara_info.operations.create.database.from.csv_file","text":"Documentation Create a database from a csv_file value. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create a database from a csv_file value. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_f csv_\u2026 The yes -- no ile type defa\u2026 of -- the sour\u2026 valu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value.","title":"create.database.from.csv_file"},{"location":"info/operations/#kiara_info.operations.create.database.from.csv_file_bundle","text":"Documentation Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_f csv_\u2026 The yes -- no ile_b type defa\u2026 undle of -- the sour\u2026 valu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value.","title":"create.database.from.csv_file_bundle"},{"location":"info/operations/#kiara_info.operations.create.database.from.table","text":"Documentation Create a database value from a table. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create a database value from a table. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The yes -- no type defa\u2026 of -- the sour\u2026 valu\u2026 table stri\u2026 The no impo\u2026 _name name of the table in the new data\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value.","title":"create.database.from.table"},{"location":"info/operations/#kiara_info.operations.create.table.from.csv_file","text":"Documentation Create a table from a csv_file value. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create a table from a csv_file value. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv_f csv_\u2026 The yes -- no ile type defa\u2026 of -- the sour\u2026 valu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value.","title":"create.table.from.csv_file"},{"location":"info/operations/#kiara_info.operations.create.table.from.text_file_bundle","text":"Documentation Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: \u2022 id: an auto-assigned index \u2022 rel_path: the relative path of the file (from the provided base path) \u2022 content: the text file content Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 text_ text\u2026 The yes -- no file_ type defa\u2026 bundl of -- e the sour\u2026 valu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value.","title":"create.table.from.text_file_bundle"},{"location":"info/operations/#kiara_info.operations.deserialize.array.as.python_object","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The yes -- value no obje\u2026 def\u2026 -- deser any Seri\u2026 no -- ializ spec\u2026 no ation conf\u2026 def\u2026 _conf -- ig Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_obj python_ob\u2026 The ect deseriali\u2026 python object instance.","title":"deserialize.array.as.python_object"},{"location":"info/operations/#kiara_info.operations.deserialize.database.as.python_object","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 datab data\u2026 The yes -- no ase value defa\u2026 obje\u2026 -- deser any Seri\u2026 no -- no ializ spec\u2026 defa\u2026 ation conf\u2026 -- _conf ig Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_obj python_ob\u2026 The ect deseriali\u2026 python object instance.","title":"deserialize.database.as.python_object"},{"location":"info/operations/#kiara_info.operations.deserialize.table.as.python_object","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The yes -- value no obje\u2026 def\u2026 -- deser any Seri\u2026 no -- ializ spec\u2026 no ation conf\u2026 def\u2026 _conf -- ig Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 python_obj python_ob\u2026 The ect deseriali\u2026 python object instance.","title":"deserialize.table.as.python_object"},{"location":"info/operations/#kiara_info.operations.export.table.as.csv_file","text":"Documentation Export a table as csv file. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Export a table as csv file. Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table A yes -- value no of def\u2026 type -- 'tab\u2026 base_ stri\u2026 The no -- path dire\u2026 no to def\u2026 expo\u2026 -- the file\u2026 to. name stri\u2026 The no -- (bas\u2026 no name def\u2026 of -- the expo\u2026 file\u2026 expor bool\u2026 Whet\u2026 no Fal\u2026 t_met to adata also expo\u2026 the value meta\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 export_detail dict Details about s the exported files/folder\u2026","title":"export.table.as.csv_file"},{"location":"info/operations/#kiara_info.operations.extract.date_array.from.table","text":"Documentation Extract a date array from a table column. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Extract a date array from a table column. Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table A yes -- tabl\u2026 no def\u2026 -- colum stri\u2026 The yes -- n_nam name no e of def\u2026 the -- colu\u2026 to extr\u2026 parse bool\u2026 If no True _date set _arra to y__fo 'Tru\u2026 rce_n raise on_nu an ll error if any of the stri\u2026 in the array can't be pars\u2026 parse inte\u2026 The no -- _date mini\u2026 no _arra index def\u2026 y__mi from -- n_ind where ex to start pars\u2026 the stri\u2026 parse inte\u2026 The no -- _date maxi\u2026 no _arra index def\u2026 y__ma until -- x_ind whic ex to parse the stri\u2026 parse list A no [] _date list _arra of y__re toke\u2026 move_ to token repl\u2026 s with a sing\u2026 whit\u2026 befo\u2026 pars\u2026 the inpu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 date_array array The resulting array with items of a date data type.","title":"extract.date_array.from.table"},{"location":"info/operations/#kiara_info.operations.import.database.from.csv_file","text":"Documentation Import a database from a csv file. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Import a database from a csv file. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 path stri\u2026 The yes -- no local defa\u2026 path -- to the file. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 imported_fi file The loaded le files. database database The result value.","title":"import.database.from.csv_file"},{"location":"info/operations/#kiara_info.operations.import.table.from.csv_file","text":"Documentation Import a table from a csv file. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Import a table from a csv file. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 path stri\u2026 The yes -- no local defa\u2026 path -- to the file. Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 imported_file file The loaded files. table table The result value.","title":"import.table.from.csv_file"},{"location":"info/operations/#kiara_info.operations.import.table.from.text_file_bundle","text":"Documentation Load a table from a bundle of text files. Author(s) Markus Binsteiner markus@frkl.io Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ Operation details Documentation Load a table from a bundle of text files. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 path stri\u2026 The yes -- no local defa\u2026 path -- of the fold\u2026 to impo\u2026 Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 imported_f file_bund\u2026 The ile_bundle imported file bundle. table table The result value.","title":"import.table.from.text_file_bundle"},{"location":"info/operations/#kiara_info.operations.parse.date_array","text":"Documentation Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the dateutil package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: \u2022 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) \u2022 remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the (https://github.com/dateutil/dateutil) package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: - 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) - remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The yes -- input no arra\u2026 def\u2026 -- force bool\u2026 If no True _non_ set null to 'Tru\u2026 raise an error if any of the stri\u2026 in the array can't be pars\u2026 min_i inte\u2026 The no -- ndex mini\u2026 no index def\u2026 from -- where to start pars\u2026 the stri\u2026 max_i inte\u2026 The no -- ndex maxi\u2026 no index def\u2026 until -- whic to parse the stri\u2026 remov list A no [] e_tok list ens of toke\u2026 to repl\u2026 with a sing\u2026 whit\u2026 befo\u2026 pars\u2026 the inpu\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 date_array array The resulting array with items of a date data type.","title":"parse.date_array"},{"location":"info/operations/#kiara_info.operations.query.database","text":"Documentation Execute a sql query against a (sqlite) database. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Execute a sql query against a (sqlite) database. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 datab data\u2026 The yes -- no ase data\u2026 defa\u2026 to -- quer\u2026 query stri\u2026 The yes -- no query defa\u2026 to -- exec\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query_result table The query result.","title":"query.database"},{"location":"info/operations/#kiara_info.operations.query.table","text":"Documentation Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The yes -- no table defa\u2026 to -- query query stri\u2026 The yes -- no quer\u2026 defa\u2026 -- relat stri\u2026 The no data ion_n name ame the table is refe\u2026 to in the sql quer\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 query_result table The query result.","title":"query.table"},{"location":"info/operations/#kiara_info.operations.render.database.as.string","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value data\u2026 A yes -- no value defa\u2026 of -- type 'dat\u2026 rende dict Inst\u2026 no {} r_con on fig how (or what) to rend\u2026 the prov\u2026 valu\u2026 Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 render_val render_va\u2026 The ue_result rendered value, incl. some metadata.","title":"render.database.as.string"},{"location":"info/operations/#kiara_info.operations.render.database.as.terminal_renderable","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value data\u2026 A yes -- no value defa\u2026 of -- type 'dat\u2026 rende dict Inst\u2026 no {} r_con on fig how (or what) to rend\u2026 the prov\u2026 valu\u2026 Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 render_val render_va\u2026 The ue_result rendered value, incl. some metadata.","title":"render.database.as.terminal_renderable"},{"location":"info/operations/#kiara_info.operations.render.table.as.string","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table A yes -- value no of def\u2026 type -- 'tab\u2026 rende dict Inst\u2026 no {} r_con on fig how (or what) to rend\u2026 the prov\u2026 valu\u2026 Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 render_val render_va\u2026 The ue_result rendered value, incl. some metadata.","title":"render.table.as.string"},{"location":"info/operations/#kiara_info.operations.render.table.as.terminal_renderable","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table A yes -- value no of def\u2026 type -- 'tab\u2026 rende dict Inst\u2026 no {} r_con on fig how (or what) to rend\u2026 the prov\u2026 valu\u2026 Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 render_val render_va\u2026 The ue_result rendered value, incl. some metadata.","title":"render.table.as.terminal_renderable"},{"location":"info/operations/#kiara_info.operations.table.cut_column","text":"Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Cut off one column from a table, returning an array. Inputs field name type desc\u2026 Req\u2026 Defa\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table A yes -- no tabl\u2026 defa\u2026 -- colum stri\u2026 The yes -- no n_nam name defa\u2026 e of -- the colu\u2026 to extr\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The column.","title":"table.cut_column"},{"location":"info/operations/#kiara_info.operations.table_filter.drop_columns","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table A yes -- value no of def\u2026 type -- 'tab\u2026 colum list The no -- ns name no of def\u2026 the -- colu\u2026 to incl\u2026 ignor bool\u2026 Whet\u2026 no True e_inv to alid_ igno\u2026 colum inva\u2026 n_nam colu\u2026 es name\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table The filtered value.","title":"table_filter.drop_columns"},{"location":"info/operations/#kiara_info.operations.table_filter.select_columns","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table A yes -- value no of def\u2026 type -- 'tab\u2026 colum list The no -- ns name no of def\u2026 the -- colu\u2026 to incl\u2026 ignor bool\u2026 Whet\u2026 no True e_inv to alid_ igno\u2026 colum inva\u2026 n_nam colu\u2026 es name\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table The filtered value.","title":"table_filter.select_columns"},{"location":"info/operations/#kiara_info.operations.table_filter.select_rows","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs field name type desc\u2026 Requ\u2026 Def\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table A yes -- value no of def\u2026 type -- 'tab\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value table The filtered value.","title":"table_filter.select_rows"},{"location":"reference/SUMMARY/","text":"kiara_plugin tabular data_types array db table defaults models array db table modules array db table filters pipelines utils","title":"SUMMARY"},{"location":"reference/kiara_plugin/tabular/__init__/","text":"Top-level package for kiara_plugin.tabular. KIARA_METADATA \u00b6 find_data_types : Union [ Type , Tuple , Callable ] \u00b6 find_model_classes : Union [ Type , Tuple , Callable ] \u00b6 find_modules : Union [ Type , Tuple , Callable ] \u00b6 find_pipelines : Union [ Type , Tuple , Callable ] \u00b6 get_version () \u00b6 Source code in tabular/__init__.py def get_version (): from pkg_resources import DistributionNotFound , get_distribution try : # Change here if project is renamed and does not equal the package name dist_name = __name__ __version__ = get_distribution ( dist_name ) . version except DistributionNotFound : try : version_file = os . path . join ( os . path . dirname ( __file__ ), \"version.txt\" ) if os . path . exists ( version_file ): with open ( version_file , encoding = \"utf-8\" ) as vf : __version__ = vf . read () else : __version__ = \"unknown\" except ( Exception ): pass if __version__ is None : __version__ = \"unknown\" return __version__ Modules \u00b6 data_types special \u00b6 This module contains the value type classes that are used in the kiara_plugin.tabular package. Modules \u00b6 array \u00b6 Classes \u00b6 ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the KiaraArray wrapper class to manage array data. This wrapper class, in turn, uses an Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [KiaraArray][kiara_plugin.tabular.models.array.KiaraArray] wrapper class to manage array data. This wrapper class, in turn, uses an [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray serialize ( self , data ) \u00b6 Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized Functions \u00b6 store_array ( array_obj , file_name , column_name = 'array' ) \u00b6 Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa from pyarrow import ChunkedArray schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : if isinstance ( array_obj , ChunkedArray ): for chunk in array_obj . chunks : batch = pa . record_batch ([ chunk ], schema = schema ) writer . write ( batch ) else : raise NotImplementedError () db \u00b6 Classes \u00b6 DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by the KiaraDatabase class to manage the stored data. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by the [KiaraDatabase][kiara_plugin.tabular.models.db.KiaraDatabase] class to manage the stored data. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase serialize ( self , data ) \u00b6 Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized SqliteTabularWrap ( TabularWrap ) \u00b6 Source code in tabular/data_types/db.py class SqliteTabularWrap ( TabularWrap ): def __init__ ( self , engine : \"Engine\" , table_name : str ): self . _engine : Engine = engine self . _table_name : str = table_name super () . __init__ () def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict retrieve_column_names ( self ) \u00b6 Source code in tabular/data_types/db.py def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result retrieve_number_of_rows ( self ) \u00b6 Source code in tabular/data_types/db.py def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows slice ( self , offset = 0 , length = None ) \u00b6 Source code in tabular/data_types/db.py def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) to_pydict ( self ) \u00b6 Source code in tabular/data_types/db.py def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict table \u00b6 Classes \u00b6 TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) # def calculate_hash(self, data: KiaraTable) -> CID: # hashes = [] # for column_name in data.arrow_table.column_names: # hashes.append(column_name) # column = data.arrow_table.column(column_name) # for chunk in column.chunks: # for buf in chunk.buffers(): # if not buf: # continue # h = hash_from_buffer(memoryview(buf)) # hashes.append(h) # return compute_cid(hashes) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) # def calculate_size(self, data: KiaraTable) -> int: # return len(data.arrow_table) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable serialize ( self , data ) \u00b6 Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized defaults \u00b6 Attributes \u00b6 DEFAULT_TABULAR_DATA_CHUNK_SIZE \u00b6 KIARA_PLUGIN_TABULAR_BASE_FOLDER \u00b6 Marker to indicate the base folder for the kiara network module package. KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER \u00b6 Default resources folder for this package. RESERVED_SQL_KEYWORDS \u00b6 SQLALCHEMY_SQLITE_TYPE_MAP : Dict [ Type , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] \u00b6 SQLITE_DATA_TYPE : Tuple [ Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ], ... ] \u00b6 SQLITE_SQLALCHEMY_TYPE_MAP : Dict [ Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ], Type ] \u00b6 SqliteDataType \u00b6 TEMPLATES_FOLDER \u00b6 models special \u00b6 This module contains the metadata (and other) models that are used in the kiara_plugin.tabular package. Those models are convenience wrappers that make it easier for kiara to find, create, manage and version metadata -- but also other type of models -- that is attached to data, as well as kiara modules. Metadata models must be a sub-class of kiara.metadata.MetadataModel . Other models usually sub-class a pydantic BaseModel or implement custom base classes. Classes \u00b6 ColumnSchema ( BaseModel ) pydantic-model \u00b6 Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict ) Attributes \u00b6 metadata : Dict [ str , Any ] pydantic-field \u00b6 Other metadata for the column. type_name : str pydantic-field required \u00b6 The type name of the column (backend-specific). TableMetadata ( KiaraModel ) pydantic-model \u00b6 Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , } Attributes \u00b6 column_names : List [ str ] pydantic-field required \u00b6 The name of the columns of the table. column_schema : Dict [ str , kiara_plugin . tabular . models . ColumnSchema ] pydantic-field required \u00b6 The schema description of the table. rows : int pydantic-field required \u00b6 The number of rows the table contains. size : int pydantic-field \u00b6 The tables size in bytes. Modules \u00b6 array \u00b6 Classes \u00b6 KiaraArray ( KiaraModel ) pydantic-model \u00b6 A class to manage array-like data. Internally, this uses an Apache Arrow Array to handle the data in memory and on disk. Source code in tabular/models/array.py class KiaraArray ( KiaraModel ): \"\"\"A class to manage array-like data. Internally, this uses an [Apache Arrow Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to handle the data in memory and on disk. \"\"\" # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/array.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/array.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/array.py def to_pylist ( self ): return self . arrow_array . to_pylist () db \u00b6 Classes \u00b6 DatabaseMetadata ( ValueMetadata ) pydantic-model \u00b6 Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" ) Attributes \u00b6 tables : Dict [ str , kiara_plugin . tabular . models . TableMetadata ] pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage a sqlite database. Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): \"\"\"A wrapper class to manage a sqlite database.\"\"\" @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_cid : CID property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector get_sqlalchemy_metadata ( self ) \u00b6 Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj get_sqlalchemy_table ( self , table_name ) \u00b6 Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table SqliteTableSchema ( BaseModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table Attributes \u00b6 columns : Dict [ str , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] pydantic-field required \u00b6 The table columns and their attributes. index_columns : List [ str ] pydantic-field \u00b6 The columns to index nullable_columns : List [ str ] pydantic-field \u00b6 The columns that are nullable. primary_key : str pydantic-field \u00b6 The primary key for this table. unique_columns : List [ str ] pydantic-field \u00b6 The columns that should be marked 'UNIQUE'. Methods \u00b6 create_table ( self , table_name , engine ) \u00b6 Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table create_table_metadata ( self , table_name ) \u00b6 Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table table \u00b6 Classes \u00b6 KiaraTable ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage tabular data in a memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Union [ None , str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 Return the data as an Apache Arrow Table instance. column_names : Iterable [ str ] property readonly \u00b6 Retrieve the names of all the columns of this table. data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 Return the number of rows in this table. Methods \u00b6 create_table ( data ) classmethod \u00b6 Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] modules special \u00b6 Modules \u00b6 array special \u00b6 FORCE_NON_NULL_DOC \u00b6 MAX_INDEX_DOC \u00b6 MIN_INDEX_DOC \u00b6 REMOVE_TOKENS_DOC \u00b6 Classes \u00b6 DeserializeArrayModule ( DeserializeValueModule ) \u00b6 Deserialize array data. Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): \"\"\"Deserialize array data.\"\"\" _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array ExtractDateConfig ( KiaraInputsConfig ) pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. ExtractDateModule ( AutoInputsKiaraModule ) \u00b6 Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the dateutil package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): \"\"\"Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the [dateutil](https://github.com/dateutil/dateutil) package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: - 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) - remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. \"\"\" _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) Classes \u00b6 _config_cls ( KiaraInputsConfig ) private pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) db special \u00b6 Classes \u00b6 CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. Methods \u00b6 create__database__from__csv_file ( self , source_value ) \u00b6 Create a database from a csv_file value. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__table ( self , source_value , optional ) \u00b6 Create a database value from a table. Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db create_optional_inputs ( self , source_type , target_type ) \u00b6 Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db QueryDatabaseConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. QueryDatabaseModule ( KiaraModule ) \u00b6 Execute a sql query against a (sqlite) database. Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): \"\"\"Execute a sql query against a (sqlite) database.\"\"\" _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) RenderDatabaseModule ( RenderDatabaseModuleBase ) \u00b6 Source code in tabular/modules/db/__init__.py class RenderDatabaseModule ( RenderDatabaseModuleBase ): _module_type_name = \"render.database\" def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , ) render__database__as__string ( self , value , render_config ) \u00b6 Source code in tabular/modules/db/__init__.py def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) render__database__as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/modules/db/__init__.py def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , ) RenderDatabaseModuleBase ( RenderValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class RenderDatabaseModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables preprocess_database ( self , value , table_name , input_number_of_rows , input_row_offset ) \u00b6 Source code in tabular/modules/db/__init__.py def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables table special \u00b6 EMPTY_COLUMN_NAME_MARKER \u00b6 Classes \u00b6 CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. Methods \u00b6 create__table__from__csv_file ( self , source_value ) \u00b6 Create a table from a csv_file value. Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) create__table__from__text_file_bundle ( self , source_value ) \u00b6 Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) DeserializeTableModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table ExportTableModule ( DataExportModule ) \u00b6 Export table data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export table data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path} Methods \u00b6 export__table__as__csv_file ( self , value , base_path , name ) \u00b6 Export a table as csv file. Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } MergeTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. MergeTableModule ( KiaraModule ) \u00b6 Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' \"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) QueryTableSQL ( KiaraModule ) \u00b6 Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. \"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ()) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ()) QueryTableSQLModuleConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. RenderTableModule ( RenderTableModuleBase ) \u00b6 Source code in tabular/modules/table/__init__.py class RenderTableModule ( RenderTableModuleBase ): _module_type_name = \"render.table\" def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) render__table__as__string ( self , value , render_config ) \u00b6 Source code in tabular/modules/table/__init__.py def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) render__table__as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/modules/table/__init__.py def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) RenderTableModuleBase ( RenderValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class RenderTableModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes preprocess_table ( self , value , input_number_of_rows , input_row_offset ) \u00b6 Source code in tabular/modules/table/__init__.py def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes filters \u00b6 TableFiltersModule ( FilterModule ) \u00b6 Source code in tabular/modules/table/filters.py class TableFiltersModule ( FilterModule ): _module_type_name = \"table.filters\" @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\" def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass create_filter_inputs ( self , filter_name ) \u00b6 Source code in tabular/modules/table/filters.py def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None filter__drop_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_rows ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass retrieve_supported_type () classmethod \u00b6 Source code in tabular/modules/table/filters.py @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\" pipelines special \u00b6 Default (empty) module that is used as a base path for pipelines contained in this package. utils \u00b6 Functions \u00b6 convert_arrow_column_types_to_sqlite ( table ) \u00b6 Source code in tabular/utils.py def convert_arrow_column_types_to_sqlite ( table : \"pa.Table\" , ) -> Dict [ str , SqliteDataType ]: result : Dict [ str , SqliteDataType ] = {} for column_name in table . column_names : field = table . field ( column_name ) sqlite_type = convert_arrow_type_to_sqlite ( str ( field . type )) result [ column_name ] = sqlite_type return result convert_arrow_type_to_sqlite ( data_type ) \u00b6 Source code in tabular/utils.py def convert_arrow_type_to_sqlite ( data_type : str ) -> SqliteDataType : if data_type . startswith ( \"int\" ) or data_type . startswith ( \"uint\" ): return \"INTEGER\" if ( data_type . startswith ( \"float\" ) or data_type . startswith ( \"decimal\" ) or data_type . startswith ( \"double\" ) ): return \"REAL\" if data_type . startswith ( \"time\" ) or data_type . startswith ( \"date\" ): return \"TEXT\" if data_type == \"bool\" : return \"INTEGER\" if data_type in [ \"string\" , \"utf8\" , \"large_string\" , \"large_utf8\" ]: return \"TEXT\" if data_type in [ \"binary\" , \"large_binary\" ]: return \"BLOB\" raise Exception ( f \"Can't convert to sqlite type: { data_type } \" ) create_sqlite_schema_data_from_arrow_table ( table , column_map = None , index_columns = None , nullable_columns = None , unique_columns = None , primary_key = None ) \u00b6 Create a sql schema statement from an Arrow table object. Parameters: Name Type Description Default table pa.Table the Arrow table object required column_map Optional[Mapping[str, str]] a map that contains column names that should be changed in the new table None index_columns Optional[Iterable[str]] a list of column names (after mapping) to create module_indexes for None extra_column_info a list of extra schema instructions per column name (after mapping) required Source code in tabular/utils.py def create_sqlite_schema_data_from_arrow_table ( table : \"pa.Table\" , column_map : Optional [ Mapping [ str , str ]] = None , index_columns : Optional [ Iterable [ str ]] = None , nullable_columns : Optional [ Iterable [ str ]] = None , unique_columns : Optional [ Iterable [ str ]] = None , primary_key : Optional [ str ] = None , ) -> SqliteTableSchema : \"\"\"Create a sql schema statement from an Arrow table object. Arguments: table: the Arrow table object column_map: a map that contains column names that should be changed in the new table index_columns: a list of column names (after mapping) to create module_indexes for extra_column_info: a list of extra schema instructions per column name (after mapping) \"\"\" columns = convert_arrow_column_types_to_sqlite ( table = table ) if column_map is None : column_map = {} temp : Dict [ str , SqliteDataType ] = {} if index_columns is None : index_columns = [] if nullable_columns is None : nullable_columns = [] if unique_columns is None : unique_columns = [] for cn , sqlite_data_type in columns . items (): if cn in column_map . keys (): new_key = column_map [ cn ] index_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in index_columns ] unique_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in unique_columns ] nullable_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in nullable_columns ] else : new_key = cn temp [ new_key ] = sqlite_data_type columns = temp if not columns : raise Exception ( \"Resulting table schema has no columns.\" ) else : for ic in index_columns : if ic not in columns . keys (): raise Exception ( f \"Can't create schema, requested index column name not available: { ic } \" ) schema = SqliteTableSchema ( columns = columns , index_columns = index_columns , nullable_columns = nullable_columns , unique_columns = unique_columns , primary_key = primary_key , ) return schema create_sqlite_table_from_tabular_file ( target_db_file , file_item , table_name = None , is_csv = True , is_tsv = False , is_nl = False , primary_key_column_names = None , flatten_nested_json_objects = False , csv_delimiter = None , quotechar = None , sniff = True , no_headers = False , encoding = 'utf-8' , batch_size = 100 , detect_types = True ) \u00b6 Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : Union [ str , None ] = None , quotechar : Union [ str , None ] = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension f = open ( file_item . path , \"rb\" ) try : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , ) except Exception as e : log_exception ( e ) finally : f . close () insert_db_table_from_file_bundle ( database , file_bundle , table_name = 'file_items' , include_content = True ) \u00b6 Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import Column , Integer , MetaData , String , Table , Text , insert from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"tabular"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.KIARA_METADATA","text":"","title":"KIARA_METADATA"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_data_types","text":"","title":"find_data_types"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_model_classes","text":"","title":"find_model_classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_modules","text":"","title":"find_modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_pipelines","text":"","title":"find_pipelines"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.get_version","text":"Source code in tabular/__init__.py def get_version (): from pkg_resources import DistributionNotFound , get_distribution try : # Change here if project is renamed and does not equal the package name dist_name = __name__ __version__ = get_distribution ( dist_name ) . version except DistributionNotFound : try : version_file = os . path . join ( os . path . dirname ( __file__ ), \"version.txt\" ) if os . path . exists ( version_file ): with open ( version_file , encoding = \"utf-8\" ) as vf : __version__ = vf . read () else : __version__ = \"unknown\" except ( Exception ): pass if __version__ is None : __version__ = \"unknown\" return __version__","title":"get_version()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types","text":"This module contains the value type classes that are used in the kiara_plugin.tabular package.","title":"data_types"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.array","text":"","title":"array"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.array-classes","text":"ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the KiaraArray wrapper class to manage array data. This wrapper class, in turn, uses an Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [KiaraArray][kiara_plugin.tabular.models.array.KiaraArray] wrapper class to manage array data. This wrapper class, in turn, uses an [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray serialize ( self , data ) \u00b6 Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.array-functions","text":"store_array ( array_obj , file_name , column_name = 'array' ) \u00b6 Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa from pyarrow import ChunkedArray schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : if isinstance ( array_obj , ChunkedArray ): for chunk in array_obj . chunks : batch = pa . record_batch ([ chunk ], schema = schema ) writer . write ( batch ) else : raise NotImplementedError ()","title":"Functions"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.db-classes","text":"DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by the KiaraDatabase class to manage the stored data. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by the [KiaraDatabase][kiara_plugin.tabular.models.db.KiaraDatabase] class to manage the stored data. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase serialize ( self , data ) \u00b6 Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized SqliteTabularWrap ( TabularWrap ) \u00b6 Source code in tabular/data_types/db.py class SqliteTabularWrap ( TabularWrap ): def __init__ ( self , engine : \"Engine\" , table_name : str ): self . _engine : Engine = engine self . _table_name : str = table_name super () . __init__ () def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict retrieve_column_names ( self ) \u00b6 Source code in tabular/data_types/db.py def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result retrieve_number_of_rows ( self ) \u00b6 Source code in tabular/data_types/db.py def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows slice ( self , offset = 0 , length = None ) \u00b6 Source code in tabular/data_types/db.py def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) to_pydict ( self ) \u00b6 Source code in tabular/data_types/db.py def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.table-classes","text":"TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) # def calculate_hash(self, data: KiaraTable) -> CID: # hashes = [] # for column_name in data.arrow_table.column_names: # hashes.append(column_name) # column = data.arrow_table.column(column_name) # for chunk in column.chunks: # for buf in chunk.buffers(): # if not buf: # continue # h = hash_from_buffer(memoryview(buf)) # hashes.append(h) # return compute_cid(hashes) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) # def calculate_size(self, data: KiaraTable) -> int: # return len(data.arrow_table) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable serialize ( self , data ) \u00b6 Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults","text":"","title":"defaults"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.DEFAULT_TABULAR_DATA_CHUNK_SIZE","text":"","title":"DEFAULT_TABULAR_DATA_CHUNK_SIZE"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.KIARA_PLUGIN_TABULAR_BASE_FOLDER","text":"Marker to indicate the base folder for the kiara network module package.","title":"KIARA_PLUGIN_TABULAR_BASE_FOLDER"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER","text":"Default resources folder for this package.","title":"KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.RESERVED_SQL_KEYWORDS","text":"","title":"RESERVED_SQL_KEYWORDS"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.SQLALCHEMY_SQLITE_TYPE_MAP","text":"","title":"SQLALCHEMY_SQLITE_TYPE_MAP"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.SQLITE_DATA_TYPE","text":"","title":"SQLITE_DATA_TYPE"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.SQLITE_SQLALCHEMY_TYPE_MAP","text":"","title":"SQLITE_SQLALCHEMY_TYPE_MAP"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.SqliteDataType","text":"","title":"SqliteDataType"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.defaults.TEMPLATES_FOLDER","text":"","title":"TEMPLATES_FOLDER"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models","text":"This module contains the metadata (and other) models that are used in the kiara_plugin.tabular package. Those models are convenience wrappers that make it easier for kiara to find, create, manage and version metadata -- but also other type of models -- that is attached to data, as well as kiara modules. Metadata models must be a sub-class of kiara.metadata.MetadataModel . Other models usually sub-class a pydantic BaseModel or implement custom base classes.","title":"models"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.ColumnSchema","text":"Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict )","title":"ColumnSchema"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.ColumnSchema-attributes","text":"metadata : Dict [ str , Any ] pydantic-field \u00b6 Other metadata for the column. type_name : str pydantic-field required \u00b6 The type name of the column (backend-specific).","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.TableMetadata","text":"Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , }","title":"TableMetadata"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.TableMetadata-attributes","text":"column_names : List [ str ] pydantic-field required \u00b6 The name of the columns of the table. column_schema : Dict [ str , kiara_plugin . tabular . models . ColumnSchema ] pydantic-field required \u00b6 The schema description of the table. rows : int pydantic-field required \u00b6 The number of rows the table contains. size : int pydantic-field \u00b6 The tables size in bytes.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.array","text":"","title":"array"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.array-classes","text":"KiaraArray ( KiaraModel ) pydantic-model \u00b6 A class to manage array-like data. Internally, this uses an Apache Arrow Array to handle the data in memory and on disk. Source code in tabular/models/array.py class KiaraArray ( KiaraModel ): \"\"\"A class to manage array-like data. Internally, this uses an [Apache Arrow Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to handle the data in memory and on disk. \"\"\" # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/array.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/array.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/array.py def to_pylist ( self ): return self . arrow_array . to_pylist ()","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.db-classes","text":"DatabaseMetadata ( ValueMetadata ) pydantic-model \u00b6 Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" ) Attributes \u00b6 tables : Dict [ str , kiara_plugin . tabular . models . TableMetadata ] pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage a sqlite database. Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): \"\"\"A wrapper class to manage a sqlite database.\"\"\" @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_cid : CID property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector get_sqlalchemy_metadata ( self ) \u00b6 Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj get_sqlalchemy_table ( self , table_name ) \u00b6 Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table SqliteTableSchema ( BaseModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table Attributes \u00b6 columns : Dict [ str , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] pydantic-field required \u00b6 The table columns and their attributes. index_columns : List [ str ] pydantic-field \u00b6 The columns to index nullable_columns : List [ str ] pydantic-field \u00b6 The columns that are nullable. primary_key : str pydantic-field \u00b6 The primary key for this table. unique_columns : List [ str ] pydantic-field \u00b6 The columns that should be marked 'UNIQUE'. Methods \u00b6 create_table ( self , table_name , engine ) \u00b6 Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table create_table_metadata ( self , table_name ) \u00b6 Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.table-classes","text":"KiaraTable ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage tabular data in a memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Union [ None , str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 Return the data as an Apache Arrow Table instance. column_names : Iterable [ str ] property readonly \u00b6 Retrieve the names of all the columns of this table. data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 Return the number of rows in this table. Methods \u00b6 create_table ( data ) classmethod \u00b6 Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules","text":"","title":"modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.array","text":"FORCE_NON_NULL_DOC \u00b6 MAX_INDEX_DOC \u00b6 MIN_INDEX_DOC \u00b6 REMOVE_TOKENS_DOC \u00b6","title":"array"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.array-classes","text":"DeserializeArrayModule ( DeserializeValueModule ) \u00b6 Deserialize array data. Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): \"\"\"Deserialize array data.\"\"\" _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array ExtractDateConfig ( KiaraInputsConfig ) pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. ExtractDateModule ( AutoInputsKiaraModule ) \u00b6 Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the dateutil package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): \"\"\"Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the [dateutil](https://github.com/dateutil/dateutil) package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: - 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) - remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. \"\"\" _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) Classes \u00b6 _config_cls ( KiaraInputsConfig ) private pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.db-classes","text":"CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. Methods \u00b6 create__database__from__csv_file ( self , source_value ) \u00b6 Create a database from a csv_file value. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__table ( self , source_value , optional ) \u00b6 Create a database value from a table. Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db create_optional_inputs ( self , source_type , target_type ) \u00b6 Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db QueryDatabaseConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. QueryDatabaseModule ( KiaraModule ) \u00b6 Execute a sql query against a (sqlite) database. Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): \"\"\"Execute a sql query against a (sqlite) database.\"\"\" _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) RenderDatabaseModule ( RenderDatabaseModuleBase ) \u00b6 Source code in tabular/modules/db/__init__.py class RenderDatabaseModule ( RenderDatabaseModuleBase ): _module_type_name = \"render.database\" def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , ) render__database__as__string ( self , value , render_config ) \u00b6 Source code in tabular/modules/db/__init__.py def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) render__database__as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/modules/db/__init__.py def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , ) RenderDatabaseModuleBase ( RenderValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class RenderDatabaseModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables preprocess_database ( self , value , table_name , input_number_of_rows , input_row_offset ) \u00b6 Source code in tabular/modules/db/__init__.py def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.table","text":"EMPTY_COLUMN_NAME_MARKER \u00b6","title":"table"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.modules.table-classes","text":"CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. Methods \u00b6 create__table__from__csv_file ( self , source_value ) \u00b6 Create a table from a csv_file value. Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) create__table__from__text_file_bundle ( self , source_value ) \u00b6 Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) DeserializeTableModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table ExportTableModule ( DataExportModule ) \u00b6 Export table data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export table data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path} Methods \u00b6 export__table__as__csv_file ( self , value , base_path , name ) \u00b6 Export a table as csv file. Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } MergeTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. MergeTableModule ( KiaraModule ) \u00b6 Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' \"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) QueryTableSQL ( KiaraModule ) \u00b6 Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. \"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ()) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ()) QueryTableSQLModuleConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. RenderTableModule ( RenderTableModuleBase ) \u00b6 Source code in tabular/modules/table/__init__.py class RenderTableModule ( RenderTableModuleBase ): _module_type_name = \"render.table\" def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) render__table__as__string ( self , value , render_config ) \u00b6 Source code in tabular/modules/table/__init__.py def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) render__table__as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/modules/table/__init__.py def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) RenderTableModuleBase ( RenderValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class RenderTableModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes preprocess_table ( self , value , input_number_of_rows , input_row_offset ) \u00b6 Source code in tabular/modules/table/__init__.py def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes filters \u00b6 TableFiltersModule ( FilterModule ) \u00b6 Source code in tabular/modules/table/filters.py class TableFiltersModule ( FilterModule ): _module_type_name = \"table.filters\" @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\" def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass create_filter_inputs ( self , filter_name ) \u00b6 Source code in tabular/modules/table/filters.py def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None filter__drop_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_rows ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass retrieve_supported_type () classmethod \u00b6 Source code in tabular/modules/table/filters.py @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.pipelines","text":"Default (empty) module that is used as a base path for pipelines contained in this package.","title":"pipelines"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils","text":"","title":"utils"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils-functions","text":"","title":"Functions"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.convert_arrow_column_types_to_sqlite","text":"Source code in tabular/utils.py def convert_arrow_column_types_to_sqlite ( table : \"pa.Table\" , ) -> Dict [ str , SqliteDataType ]: result : Dict [ str , SqliteDataType ] = {} for column_name in table . column_names : field = table . field ( column_name ) sqlite_type = convert_arrow_type_to_sqlite ( str ( field . type )) result [ column_name ] = sqlite_type return result","title":"convert_arrow_column_types_to_sqlite()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.convert_arrow_type_to_sqlite","text":"Source code in tabular/utils.py def convert_arrow_type_to_sqlite ( data_type : str ) -> SqliteDataType : if data_type . startswith ( \"int\" ) or data_type . startswith ( \"uint\" ): return \"INTEGER\" if ( data_type . startswith ( \"float\" ) or data_type . startswith ( \"decimal\" ) or data_type . startswith ( \"double\" ) ): return \"REAL\" if data_type . startswith ( \"time\" ) or data_type . startswith ( \"date\" ): return \"TEXT\" if data_type == \"bool\" : return \"INTEGER\" if data_type in [ \"string\" , \"utf8\" , \"large_string\" , \"large_utf8\" ]: return \"TEXT\" if data_type in [ \"binary\" , \"large_binary\" ]: return \"BLOB\" raise Exception ( f \"Can't convert to sqlite type: { data_type } \" )","title":"convert_arrow_type_to_sqlite()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.create_sqlite_schema_data_from_arrow_table","text":"Create a sql schema statement from an Arrow table object. Parameters: Name Type Description Default table pa.Table the Arrow table object required column_map Optional[Mapping[str, str]] a map that contains column names that should be changed in the new table None index_columns Optional[Iterable[str]] a list of column names (after mapping) to create module_indexes for None extra_column_info a list of extra schema instructions per column name (after mapping) required Source code in tabular/utils.py def create_sqlite_schema_data_from_arrow_table ( table : \"pa.Table\" , column_map : Optional [ Mapping [ str , str ]] = None , index_columns : Optional [ Iterable [ str ]] = None , nullable_columns : Optional [ Iterable [ str ]] = None , unique_columns : Optional [ Iterable [ str ]] = None , primary_key : Optional [ str ] = None , ) -> SqliteTableSchema : \"\"\"Create a sql schema statement from an Arrow table object. Arguments: table: the Arrow table object column_map: a map that contains column names that should be changed in the new table index_columns: a list of column names (after mapping) to create module_indexes for extra_column_info: a list of extra schema instructions per column name (after mapping) \"\"\" columns = convert_arrow_column_types_to_sqlite ( table = table ) if column_map is None : column_map = {} temp : Dict [ str , SqliteDataType ] = {} if index_columns is None : index_columns = [] if nullable_columns is None : nullable_columns = [] if unique_columns is None : unique_columns = [] for cn , sqlite_data_type in columns . items (): if cn in column_map . keys (): new_key = column_map [ cn ] index_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in index_columns ] unique_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in unique_columns ] nullable_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in nullable_columns ] else : new_key = cn temp [ new_key ] = sqlite_data_type columns = temp if not columns : raise Exception ( \"Resulting table schema has no columns.\" ) else : for ic in index_columns : if ic not in columns . keys (): raise Exception ( f \"Can't create schema, requested index column name not available: { ic } \" ) schema = SqliteTableSchema ( columns = columns , index_columns = index_columns , nullable_columns = nullable_columns , unique_columns = unique_columns , primary_key = primary_key , ) return schema","title":"create_sqlite_schema_data_from_arrow_table()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.create_sqlite_table_from_tabular_file","text":"Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : Union [ str , None ] = None , quotechar : Union [ str , None ] = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension f = open ( file_item . path , \"rb\" ) try : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , ) except Exception as e : log_exception ( e ) finally : f . close ()","title":"create_sqlite_table_from_tabular_file()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.insert_db_table_from_file_bundle","text":"Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import Column , Integer , MetaData , String , Table , Text , insert from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"insert_db_table_from_file_bundle()"},{"location":"reference/kiara_plugin/tabular/defaults/","text":"Attributes \u00b6 DEFAULT_TABULAR_DATA_CHUNK_SIZE \u00b6 KIARA_PLUGIN_TABULAR_BASE_FOLDER \u00b6 Marker to indicate the base folder for the kiara network module package. KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER \u00b6 Default resources folder for this package. RESERVED_SQL_KEYWORDS \u00b6 SQLALCHEMY_SQLITE_TYPE_MAP : Dict [ Type , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] \u00b6 SQLITE_DATA_TYPE : Tuple [ Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ], ... ] \u00b6 SQLITE_SQLALCHEMY_TYPE_MAP : Dict [ Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ], Type ] \u00b6 SqliteDataType \u00b6 TEMPLATES_FOLDER \u00b6","title":"defaults"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.DEFAULT_TABULAR_DATA_CHUNK_SIZE","text":"","title":"DEFAULT_TABULAR_DATA_CHUNK_SIZE"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.KIARA_PLUGIN_TABULAR_BASE_FOLDER","text":"Marker to indicate the base folder for the kiara network module package.","title":"KIARA_PLUGIN_TABULAR_BASE_FOLDER"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER","text":"Default resources folder for this package.","title":"KIARA_PLUGIN_TABULAR_RESOURCES_FOLDER"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.RESERVED_SQL_KEYWORDS","text":"","title":"RESERVED_SQL_KEYWORDS"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.SQLALCHEMY_SQLITE_TYPE_MAP","text":"","title":"SQLALCHEMY_SQLITE_TYPE_MAP"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.SQLITE_DATA_TYPE","text":"","title":"SQLITE_DATA_TYPE"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.SQLITE_SQLALCHEMY_TYPE_MAP","text":"","title":"SQLITE_SQLALCHEMY_TYPE_MAP"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.SqliteDataType","text":"","title":"SqliteDataType"},{"location":"reference/kiara_plugin/tabular/defaults/#kiara_plugin.tabular.defaults.TEMPLATES_FOLDER","text":"","title":"TEMPLATES_FOLDER"},{"location":"reference/kiara_plugin/tabular/utils/","text":"Functions \u00b6 convert_arrow_column_types_to_sqlite ( table ) \u00b6 Source code in tabular/utils.py def convert_arrow_column_types_to_sqlite ( table : \"pa.Table\" , ) -> Dict [ str , SqliteDataType ]: result : Dict [ str , SqliteDataType ] = {} for column_name in table . column_names : field = table . field ( column_name ) sqlite_type = convert_arrow_type_to_sqlite ( str ( field . type )) result [ column_name ] = sqlite_type return result convert_arrow_type_to_sqlite ( data_type ) \u00b6 Source code in tabular/utils.py def convert_arrow_type_to_sqlite ( data_type : str ) -> SqliteDataType : if data_type . startswith ( \"int\" ) or data_type . startswith ( \"uint\" ): return \"INTEGER\" if ( data_type . startswith ( \"float\" ) or data_type . startswith ( \"decimal\" ) or data_type . startswith ( \"double\" ) ): return \"REAL\" if data_type . startswith ( \"time\" ) or data_type . startswith ( \"date\" ): return \"TEXT\" if data_type == \"bool\" : return \"INTEGER\" if data_type in [ \"string\" , \"utf8\" , \"large_string\" , \"large_utf8\" ]: return \"TEXT\" if data_type in [ \"binary\" , \"large_binary\" ]: return \"BLOB\" raise Exception ( f \"Can't convert to sqlite type: { data_type } \" ) create_sqlite_schema_data_from_arrow_table ( table , column_map = None , index_columns = None , nullable_columns = None , unique_columns = None , primary_key = None ) \u00b6 Create a sql schema statement from an Arrow table object. Parameters: Name Type Description Default table pa.Table the Arrow table object required column_map Optional[Mapping[str, str]] a map that contains column names that should be changed in the new table None index_columns Optional[Iterable[str]] a list of column names (after mapping) to create module_indexes for None extra_column_info a list of extra schema instructions per column name (after mapping) required Source code in tabular/utils.py def create_sqlite_schema_data_from_arrow_table ( table : \"pa.Table\" , column_map : Optional [ Mapping [ str , str ]] = None , index_columns : Optional [ Iterable [ str ]] = None , nullable_columns : Optional [ Iterable [ str ]] = None , unique_columns : Optional [ Iterable [ str ]] = None , primary_key : Optional [ str ] = None , ) -> SqliteTableSchema : \"\"\"Create a sql schema statement from an Arrow table object. Arguments: table: the Arrow table object column_map: a map that contains column names that should be changed in the new table index_columns: a list of column names (after mapping) to create module_indexes for extra_column_info: a list of extra schema instructions per column name (after mapping) \"\"\" columns = convert_arrow_column_types_to_sqlite ( table = table ) if column_map is None : column_map = {} temp : Dict [ str , SqliteDataType ] = {} if index_columns is None : index_columns = [] if nullable_columns is None : nullable_columns = [] if unique_columns is None : unique_columns = [] for cn , sqlite_data_type in columns . items (): if cn in column_map . keys (): new_key = column_map [ cn ] index_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in index_columns ] unique_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in unique_columns ] nullable_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in nullable_columns ] else : new_key = cn temp [ new_key ] = sqlite_data_type columns = temp if not columns : raise Exception ( \"Resulting table schema has no columns.\" ) else : for ic in index_columns : if ic not in columns . keys (): raise Exception ( f \"Can't create schema, requested index column name not available: { ic } \" ) schema = SqliteTableSchema ( columns = columns , index_columns = index_columns , nullable_columns = nullable_columns , unique_columns = unique_columns , primary_key = primary_key , ) return schema create_sqlite_table_from_tabular_file ( target_db_file , file_item , table_name = None , is_csv = True , is_tsv = False , is_nl = False , primary_key_column_names = None , flatten_nested_json_objects = False , csv_delimiter = None , quotechar = None , sniff = True , no_headers = False , encoding = 'utf-8' , batch_size = 100 , detect_types = True ) \u00b6 Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : Union [ str , None ] = None , quotechar : Union [ str , None ] = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension f = open ( file_item . path , \"rb\" ) try : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , ) except Exception as e : log_exception ( e ) finally : f . close () insert_db_table_from_file_bundle ( database , file_bundle , table_name = 'file_items' , include_content = True ) \u00b6 Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import Column , Integer , MetaData , String , Table , Text , insert from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"utils"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils-functions","text":"","title":"Functions"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.convert_arrow_column_types_to_sqlite","text":"Source code in tabular/utils.py def convert_arrow_column_types_to_sqlite ( table : \"pa.Table\" , ) -> Dict [ str , SqliteDataType ]: result : Dict [ str , SqliteDataType ] = {} for column_name in table . column_names : field = table . field ( column_name ) sqlite_type = convert_arrow_type_to_sqlite ( str ( field . type )) result [ column_name ] = sqlite_type return result","title":"convert_arrow_column_types_to_sqlite()"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.convert_arrow_type_to_sqlite","text":"Source code in tabular/utils.py def convert_arrow_type_to_sqlite ( data_type : str ) -> SqliteDataType : if data_type . startswith ( \"int\" ) or data_type . startswith ( \"uint\" ): return \"INTEGER\" if ( data_type . startswith ( \"float\" ) or data_type . startswith ( \"decimal\" ) or data_type . startswith ( \"double\" ) ): return \"REAL\" if data_type . startswith ( \"time\" ) or data_type . startswith ( \"date\" ): return \"TEXT\" if data_type == \"bool\" : return \"INTEGER\" if data_type in [ \"string\" , \"utf8\" , \"large_string\" , \"large_utf8\" ]: return \"TEXT\" if data_type in [ \"binary\" , \"large_binary\" ]: return \"BLOB\" raise Exception ( f \"Can't convert to sqlite type: { data_type } \" )","title":"convert_arrow_type_to_sqlite()"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.create_sqlite_schema_data_from_arrow_table","text":"Create a sql schema statement from an Arrow table object. Parameters: Name Type Description Default table pa.Table the Arrow table object required column_map Optional[Mapping[str, str]] a map that contains column names that should be changed in the new table None index_columns Optional[Iterable[str]] a list of column names (after mapping) to create module_indexes for None extra_column_info a list of extra schema instructions per column name (after mapping) required Source code in tabular/utils.py def create_sqlite_schema_data_from_arrow_table ( table : \"pa.Table\" , column_map : Optional [ Mapping [ str , str ]] = None , index_columns : Optional [ Iterable [ str ]] = None , nullable_columns : Optional [ Iterable [ str ]] = None , unique_columns : Optional [ Iterable [ str ]] = None , primary_key : Optional [ str ] = None , ) -> SqliteTableSchema : \"\"\"Create a sql schema statement from an Arrow table object. Arguments: table: the Arrow table object column_map: a map that contains column names that should be changed in the new table index_columns: a list of column names (after mapping) to create module_indexes for extra_column_info: a list of extra schema instructions per column name (after mapping) \"\"\" columns = convert_arrow_column_types_to_sqlite ( table = table ) if column_map is None : column_map = {} temp : Dict [ str , SqliteDataType ] = {} if index_columns is None : index_columns = [] if nullable_columns is None : nullable_columns = [] if unique_columns is None : unique_columns = [] for cn , sqlite_data_type in columns . items (): if cn in column_map . keys (): new_key = column_map [ cn ] index_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in index_columns ] unique_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in unique_columns ] nullable_columns = [ x if x not in column_map . keys () else column_map [ x ] for x in nullable_columns ] else : new_key = cn temp [ new_key ] = sqlite_data_type columns = temp if not columns : raise Exception ( \"Resulting table schema has no columns.\" ) else : for ic in index_columns : if ic not in columns . keys (): raise Exception ( f \"Can't create schema, requested index column name not available: { ic } \" ) schema = SqliteTableSchema ( columns = columns , index_columns = index_columns , nullable_columns = nullable_columns , unique_columns = unique_columns , primary_key = primary_key , ) return schema","title":"create_sqlite_schema_data_from_arrow_table()"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.create_sqlite_table_from_tabular_file","text":"Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : Union [ str , None ] = None , quotechar : Union [ str , None ] = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension f = open ( file_item . path , \"rb\" ) try : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , ) except Exception as e : log_exception ( e ) finally : f . close ()","title":"create_sqlite_table_from_tabular_file()"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.insert_db_table_from_file_bundle","text":"Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import Column , Integer , MetaData , String , Table , Text , insert from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"insert_db_table_from_file_bundle()"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/","text":"This module contains the value type classes that are used in the kiara_plugin.tabular package. Modules \u00b6 array \u00b6 Classes \u00b6 ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the KiaraArray wrapper class to manage array data. This wrapper class, in turn, uses an Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [KiaraArray][kiara_plugin.tabular.models.array.KiaraArray] wrapper class to manage array data. This wrapper class, in turn, uses an [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray serialize ( self , data ) \u00b6 Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized Functions \u00b6 store_array ( array_obj , file_name , column_name = 'array' ) \u00b6 Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa from pyarrow import ChunkedArray schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : if isinstance ( array_obj , ChunkedArray ): for chunk in array_obj . chunks : batch = pa . record_batch ([ chunk ], schema = schema ) writer . write ( batch ) else : raise NotImplementedError () db \u00b6 Classes \u00b6 DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by the KiaraDatabase class to manage the stored data. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by the [KiaraDatabase][kiara_plugin.tabular.models.db.KiaraDatabase] class to manage the stored data. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase serialize ( self , data ) \u00b6 Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized SqliteTabularWrap ( TabularWrap ) \u00b6 Source code in tabular/data_types/db.py class SqliteTabularWrap ( TabularWrap ): def __init__ ( self , engine : \"Engine\" , table_name : str ): self . _engine : Engine = engine self . _table_name : str = table_name super () . __init__ () def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict retrieve_column_names ( self ) \u00b6 Source code in tabular/data_types/db.py def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result retrieve_number_of_rows ( self ) \u00b6 Source code in tabular/data_types/db.py def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows slice ( self , offset = 0 , length = None ) \u00b6 Source code in tabular/data_types/db.py def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) to_pydict ( self ) \u00b6 Source code in tabular/data_types/db.py def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict table \u00b6 Classes \u00b6 TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) # def calculate_hash(self, data: KiaraTable) -> CID: # hashes = [] # for column_name in data.arrow_table.column_names: # hashes.append(column_name) # column = data.arrow_table.column(column_name) # for chunk in column.chunks: # for buf in chunk.buffers(): # if not buf: # continue # h = hash_from_buffer(memoryview(buf)) # hashes.append(h) # return compute_cid(hashes) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) # def calculate_size(self, data: KiaraTable) -> int: # return len(data.arrow_table) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable serialize ( self , data ) \u00b6 Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"data_types"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array","text":"","title":"array"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array.ArrayType","text":"An array, in most cases used as a column within a table. Internally, this type uses the KiaraArray wrapper class to manage array data. This wrapper class, in turn, uses an Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [KiaraArray][kiara_plugin.tabular.models.array.KiaraArray] wrapper class to manage array data. This wrapper class, in turn, uses an [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result","title":"ArrayType"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array.ArrayType-methods","text":"parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray serialize ( self , data ) \u00b6 Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array-functions","text":"","title":"Functions"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.array.store_array","text":"Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa from pyarrow import ChunkedArray schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : if isinstance ( array_obj , ChunkedArray ): for chunk in array_obj . chunks : batch = pa . record_batch ([ chunk ], schema = schema ) writer . write ( batch ) else : raise NotImplementedError ()","title":"store_array()"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db.DatabaseType","text":"A database, containing one or several tables. This is backed by the KiaraDatabase class to manage the stored data. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by the [KiaraDatabase][kiara_plugin.tabular.models.db.KiaraDatabase] class to manage the stored data. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"DatabaseType"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db.DatabaseType-methods","text":"parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase serialize ( self , data ) \u00b6 Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db.SqliteTabularWrap","text":"Source code in tabular/data_types/db.py class SqliteTabularWrap ( TabularWrap ): def __init__ ( self , engine : \"Engine\" , table_name : str ): self . _engine : Engine = engine self . _table_name : str = table_name super () . __init__ () def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict retrieve_column_names ( self ) \u00b6 Source code in tabular/data_types/db.py def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result retrieve_number_of_rows ( self ) \u00b6 Source code in tabular/data_types/db.py def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows slice ( self , offset = 0 , length = None ) \u00b6 Source code in tabular/data_types/db.py def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) to_pydict ( self ) \u00b6 Source code in tabular/data_types/db.py def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict","title":"SqliteTabularWrap"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table.TableType","text":"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) # def calculate_hash(self, data: KiaraTable) -> CID: # hashes = [] # for column_name in data.arrow_table.column_names: # hashes.append(column_name) # column = data.arrow_table.column(column_name) # for chunk in column.chunks: # for buf in chunk.buffers(): # if not buf: # continue # h = hash_from_buffer(memoryview(buf)) # hashes.append(h) # return compute_cid(hashes) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) # def calculate_size(self, data: KiaraTable) -> int: # return len(data.arrow_table) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"TableType"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table.TableType-methods","text":"parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable serialize ( self , data ) \u00b6 Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/array/","text":"Classes \u00b6 ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the KiaraArray wrapper class to manage array data. This wrapper class, in turn, uses an Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [KiaraArray][kiara_plugin.tabular.models.array.KiaraArray] wrapper class to manage array data. This wrapper class, in turn, uses an [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray serialize ( self , data ) \u00b6 Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized Functions \u00b6 store_array ( array_obj , file_name , column_name = 'array' ) \u00b6 Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa from pyarrow import ChunkedArray schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : if isinstance ( array_obj , ChunkedArray ): for chunk in array_obj . chunks : batch = pa . record_batch ([ chunk ], schema = schema ) writer . write ( batch ) else : raise NotImplementedError ()","title":"array"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType","text":"An array, in most cases used as a column within a table. Internally, this type uses the KiaraArray wrapper class to manage array data. This wrapper class, in turn, uses an Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/array.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [KiaraArray][kiara_plugin.tabular.models.array.KiaraArray] wrapper class to manage array data. This wrapper class, in turn, uses an [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result","title":"ArrayType"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType.parse_python_obj","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/array.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data )","title":"parse_python_obj()"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType.pretty_print_as__terminal_renderable","text":"Source code in tabular/data_types/array.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result","title":"pretty_print_as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType.python_class","text":"Source code in tabular/data_types/array.py @classmethod def python_class ( cls ) -> Type : return KiaraArray","title":"python_class()"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.ArrayType.serialize","text":"Source code in tabular/data_types/array.py def serialize ( self , data : KiaraArray ) -> SerializedData : import pyarrow as pa # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = data . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunks = { \"array.arrow\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : file_name }} serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.array\" , \"module_config\" : { \"value_type\" : \"array\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"serialize()"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array-functions","text":"","title":"Functions"},{"location":"reference/kiara_plugin/tabular/data_types/array/#kiara_plugin.tabular.data_types.array.store_array","text":"Utility methdo to stora an array to a file. Source code in tabular/data_types/array.py def store_array ( array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): \"\"\"Utility methdo to stora an array to a file.\"\"\" import pyarrow as pa from pyarrow import ChunkedArray schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : if isinstance ( array_obj , ChunkedArray ): for chunk in array_obj . chunks : batch = pa . record_batch ([ chunk ], schema = schema ) writer . write ( batch ) else : raise NotImplementedError ()","title":"store_array()"},{"location":"reference/kiara_plugin/tabular/data_types/db/","text":"Classes \u00b6 DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by the KiaraDatabase class to manage the stored data. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by the [KiaraDatabase][kiara_plugin.tabular.models.db.KiaraDatabase] class to manage the stored data. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase serialize ( self , data ) \u00b6 Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized SqliteTabularWrap ( TabularWrap ) \u00b6 Source code in tabular/data_types/db.py class SqliteTabularWrap ( TabularWrap ): def __init__ ( self , engine : \"Engine\" , table_name : str ): self . _engine : Engine = engine self . _table_name : str = table_name super () . __init__ () def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict retrieve_column_names ( self ) \u00b6 Source code in tabular/data_types/db.py def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result retrieve_number_of_rows ( self ) \u00b6 Source code in tabular/data_types/db.py def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows slice ( self , offset = 0 , length = None ) \u00b6 Source code in tabular/data_types/db.py def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) to_pydict ( self ) \u00b6 Source code in tabular/data_types/db.py def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict","title":"db"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType","text":"A database, containing one or several tables. This is backed by the KiaraDatabase class to manage the stored data. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by the [KiaraDatabase][kiara_plugin.tabular.models.db.KiaraDatabase] class to manage the stored data. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"DatabaseType"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.parse_python_obj","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data","title":"parse_python_obj()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.pretty_print_as__terminal_renderable","text":"Source code in tabular/data_types/db.py def pretty_print_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"pretty_print_as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.python_class","text":"Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase","title":"python_class()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.serialize","text":"Source code in tabular/data_types/db.py def serialize ( self , data : KiaraDatabase ) -> SerializedData : chunks = { \"db.sqlite\" : { \"type\" : \"file\" , \"codec\" : \"raw\" , \"file\" : data . db_file_path } } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunks , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.database\" , \"module_config\" : { \"value_type\" : self . data_type_name , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"copy\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"serialize()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.SqliteTabularWrap","text":"Source code in tabular/data_types/db.py class SqliteTabularWrap ( TabularWrap ): def __init__ ( self , engine : \"Engine\" , table_name : str ): self . _engine : Engine = engine self . _table_name : str = table_name super () . __init__ () def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict ) def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict","title":"SqliteTabularWrap"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.SqliteTabularWrap.retrieve_column_names","text":"Source code in tabular/data_types/db.py def retrieve_column_names ( self ) -> Iterable [ str ]: from sqlalchemy import inspect engine = self . _engine inspector = inspect ( engine ) columns = inspector . get_columns ( self . _table_name ) result = [ column [ \"name\" ] for column in columns ] return result","title":"retrieve_column_names()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.SqliteTabularWrap.retrieve_number_of_rows","text":"Source code in tabular/data_types/db.py def retrieve_number_of_rows ( self ) -> int : from sqlalchemy import text with self . _engine . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { self . _table_name } \" )) num_rows = result . fetchone ()[ 0 ] return num_rows","title":"retrieve_number_of_rows()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.SqliteTabularWrap.slice","text":"Source code in tabular/data_types/db.py def slice ( self , offset : int = 0 , length : Union [ int , None ] = None ) -> \"TabularWrap\" : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" if length : query = f \" { query } LIMIT { length } \" else : query = f \" { query } LIMIT { self . num_rows } \" if offset > 0 : query = f \" { query } OFFSET { offset } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return DictTabularWrap ( result_dict )","title":"slice()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.SqliteTabularWrap.to_pydict","text":"Source code in tabular/data_types/db.py def to_pydict ( self ) -> Mapping : from sqlalchemy import text query = f \"SELECT * FROM { self . _table_name } \" with self . _engine . connect () as con : result = con . execute ( text ( query )) result_dict : Dict [ str , List [ Any ]] = {} for cn in self . column_names : result_dict [ cn ] = [] for r in result : for i , cn in enumerate ( self . column_names ): result_dict [ cn ] . append ( r [ i ]) return result_dict","title":"to_pydict()"},{"location":"reference/kiara_plugin/tabular/data_types/table/","text":"Classes \u00b6 TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) # def calculate_hash(self, data: KiaraTable) -> CID: # hashes = [] # for column_name in data.arrow_table.column_names: # hashes.append(column_name) # column = data.arrow_table.column(column_name) # for chunk in column.chunks: # for buf in chunk.buffers(): # if not buf: # continue # h = hash_from_buffer(memoryview(buf)) # hashes.append(h) # return compute_cid(hashes) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) # def calculate_size(self, data: KiaraTable) -> int: # return len(data.arrow_table) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) pretty_print_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable serialize ( self , data ) \u00b6 Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"table"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType","text":"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. kiara uses an instance of the KiaraTable class to manage the table data, which let's developers access it in different formats ( Apache Arrow Table , Pandas dataframe , Python dict of lists, more to follow...). Please consult the API doc of the KiaraTable class for more information about how to access and query the data: KiaraTable API doc Internally, the data is stored in Apache Feather format -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). The table data is organized in sets of columns (arrays of data of the same type), with each column having a string identifier. *kiara* uses an instance of the [`KiaraTable`][kiara_plugin.tabular.models.table.KiaraTable] class to manage the table data, which let's developers access it in different formats ([Apache Arrow Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html), [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), Python dict of lists, more to follow...). Please consult the API doc of the `KiaraTable` class for more information about how to access and query the data: - [`KiaraTable` API doc](https://dharpa.org/kiara_plugin.tabular/latest/reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable) Internally, the data is stored in [Apache Feather format](https://arrow.apache.org/docs/python/feather.html) -- both in memory and on disk when saved, which enables some advanced usage to preserve memory and compute overhead. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return KiaraTable def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) # def calculate_hash(self, data: KiaraTable) -> CID: # hashes = [] # for column_name in data.arrow_table.column_names: # hashes.append(column_name) # column = data.arrow_table.column(column_name) # for chunk in column.chunks: # for buf in chunk.buffers(): # if not buf: # continue # h = hash_from_buffer(memoryview(buf)) # hashes.append(h) # return compute_cid(hashes) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) # def calculate_size(self, data: KiaraTable) -> int: # return len(data.arrow_table) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"TableType"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.parse_python_obj","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data )","title":"parse_python_obj()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.pretty_print_as__terminal_renderable","text":"Source code in tabular/data_types/table.py def pretty_print_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . as_terminal_renderable ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"pretty_print_as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.python_class","text":"Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraTable","title":"python_class()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.serialize","text":"Source code in tabular/data_types/table.py def serialize ( self , data : KiaraTable ) -> SerializedData : import pyarrow as pa chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in data . arrow_table . column_names : column : pa . Array = data . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = { \"type\" : \"file\" , \"file\" : file_name , \"codec\" : \"raw\" } serialized_data = { \"data_type\" : self . data_type_name , \"data_type_config\" : self . type_config . dict (), \"data\" : chunk_map , \"serialization_profile\" : \"feather\" , \"metadata\" : { \"environment\" : {}, \"deserialize\" : { \"python_object\" : { \"module_type\" : \"load.table\" , \"module_config\" : { \"value_type\" : \"table\" , \"target_profile\" : \"python_object\" , \"serialization_profile\" : \"feather\" , }, } }, }, } serialized = SerializationResult ( ** serialized_data ) return serialized","title":"serialize()"},{"location":"reference/kiara_plugin/tabular/models/__init__/","text":"This module contains the metadata (and other) models that are used in the kiara_plugin.tabular package. Those models are convenience wrappers that make it easier for kiara to find, create, manage and version metadata -- but also other type of models -- that is attached to data, as well as kiara modules. Metadata models must be a sub-class of kiara.metadata.MetadataModel . Other models usually sub-class a pydantic BaseModel or implement custom base classes. Classes \u00b6 ColumnSchema ( BaseModel ) pydantic-model \u00b6 Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict ) Attributes \u00b6 metadata : Dict [ str , Any ] pydantic-field \u00b6 Other metadata for the column. type_name : str pydantic-field required \u00b6 The type name of the column (backend-specific). TableMetadata ( KiaraModel ) pydantic-model \u00b6 Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , } Attributes \u00b6 column_names : List [ str ] pydantic-field required \u00b6 The name of the columns of the table. column_schema : Dict [ str , kiara_plugin . tabular . models . ColumnSchema ] pydantic-field required \u00b6 The schema description of the table. rows : int pydantic-field required \u00b6 The number of rows the table contains. size : int pydantic-field \u00b6 The tables size in bytes. Modules \u00b6 array \u00b6 Classes \u00b6 KiaraArray ( KiaraModel ) pydantic-model \u00b6 A class to manage array-like data. Internally, this uses an Apache Arrow Array to handle the data in memory and on disk. Source code in tabular/models/array.py class KiaraArray ( KiaraModel ): \"\"\"A class to manage array-like data. Internally, this uses an [Apache Arrow Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to handle the data in memory and on disk. \"\"\" # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/array.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/array.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/array.py def to_pylist ( self ): return self . arrow_array . to_pylist () db \u00b6 Classes \u00b6 DatabaseMetadata ( ValueMetadata ) pydantic-model \u00b6 Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" ) Attributes \u00b6 tables : Dict [ str , kiara_plugin . tabular . models . TableMetadata ] pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage a sqlite database. Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): \"\"\"A wrapper class to manage a sqlite database.\"\"\" @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_cid : CID property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector get_sqlalchemy_metadata ( self ) \u00b6 Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj get_sqlalchemy_table ( self , table_name ) \u00b6 Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table SqliteTableSchema ( BaseModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table Attributes \u00b6 columns : Dict [ str , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] pydantic-field required \u00b6 The table columns and their attributes. index_columns : List [ str ] pydantic-field \u00b6 The columns to index nullable_columns : List [ str ] pydantic-field \u00b6 The columns that are nullable. primary_key : str pydantic-field \u00b6 The primary key for this table. unique_columns : List [ str ] pydantic-field \u00b6 The columns that should be marked 'UNIQUE'. Methods \u00b6 create_table ( self , table_name , engine ) \u00b6 Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table create_table_metadata ( self , table_name ) \u00b6 Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table table \u00b6 Classes \u00b6 KiaraTable ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage tabular data in a memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Union [ None , str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 Return the data as an Apache Arrow Table instance. column_names : Iterable [ str ] property readonly \u00b6 Retrieve the names of all the columns of this table. data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 Return the number of rows in this table. Methods \u00b6 create_table ( data ) classmethod \u00b6 Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"models"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema","text":"Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict )","title":"ColumnSchema"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema.metadata","text":"Other metadata for the column.","title":"metadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema.type_name","text":"The type name of the column (backend-specific).","title":"type_name"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata","text":"Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , }","title":"TableMetadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.column_names","text":"The name of the columns of the table.","title":"column_names"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.column_schema","text":"The schema description of the table.","title":"column_schema"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.rows","text":"The number of rows the table contains.","title":"rows"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.size","text":"The tables size in bytes.","title":"size"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.array","text":"","title":"array"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.array-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.array.KiaraArray","text":"A class to manage array-like data. Internally, this uses an Apache Arrow Array to handle the data in memory and on disk. Source code in tabular/models/array.py class KiaraArray ( KiaraModel ): \"\"\"A class to manage array-like data. Internally, this uses an [Apache Arrow Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to handle the data in memory and on disk. \"\"\" # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas ()","title":"KiaraArray"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.array.KiaraArray-attributes","text":"arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/array.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/array.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/array.py def to_pylist ( self ): return self . arrow_array . to_pylist ()","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.DatabaseMetadata","text":"Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" )","title":"DatabaseMetadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.DatabaseMetadata-attributes","text":"tables : Dict [ str , kiara_plugin . tabular . models . TableMetadata ] pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ]","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.KiaraDatabase","text":"A wrapper class to manage a sqlite database. Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): \"\"\"A wrapper class to manage a sqlite database.\"\"\" @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table","title":"KiaraDatabase"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.KiaraDatabase-attributes","text":"db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_cid : CID property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.KiaraDatabase-methods","text":"copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector get_sqlalchemy_metadata ( self ) \u00b6 Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj get_sqlalchemy_table ( self , table_name ) \u00b6 Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.SqliteTableSchema","text":"Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table","title":"SqliteTableSchema"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.SqliteTableSchema-attributes","text":"columns : Dict [ str , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] pydantic-field required \u00b6 The table columns and their attributes. index_columns : List [ str ] pydantic-field \u00b6 The columns to index nullable_columns : List [ str ] pydantic-field \u00b6 The columns that are nullable. primary_key : str pydantic-field \u00b6 The primary key for this table. unique_columns : List [ str ] pydantic-field \u00b6 The columns that should be marked 'UNIQUE'.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.SqliteTableSchema-methods","text":"create_table ( self , table_name , engine ) \u00b6 Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table create_table_metadata ( self , table_name ) \u00b6 Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable","text":"A wrapper class to manage tabular data in a memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Union [ None , str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas ()","title":"KiaraTable"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable-attributes","text":"arrow_table : Table property readonly \u00b6 Return the data as an Apache Arrow Table instance. column_names : Iterable [ str ] property readonly \u00b6 Retrieve the names of all the columns of this table. data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 Return the number of rows in this table.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable-methods","text":"create_table ( data ) classmethod \u00b6 Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist ()","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTableMetadata","text":"File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" )","title":"KiaraTableMetadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTableMetadata-attributes","text":"table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/array/","text":"Classes \u00b6 KiaraArray ( KiaraModel ) pydantic-model \u00b6 A class to manage array-like data. Internally, this uses an Apache Arrow Array to handle the data in memory and on disk. Source code in tabular/models/array.py class KiaraArray ( KiaraModel ): \"\"\"A class to manage array-like data. Internally, this uses an [Apache Arrow Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to handle the data in memory and on disk. \"\"\" # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/array.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/array.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/array.py def to_pylist ( self ): return self . arrow_array . to_pylist ()","title":"array"},{"location":"reference/kiara_plugin/tabular/models/array/#kiara_plugin.tabular.models.array-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/array/#kiara_plugin.tabular.models.array.KiaraArray","text":"A class to manage array-like data. Internally, this uses an Apache Arrow Array to handle the data in memory and on disk. Source code in tabular/models/array.py class KiaraArray ( KiaraModel ): \"\"\"A class to manage array-like data. Internally, this uses an [Apache Arrow Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to handle the data in memory and on disk. \"\"\" # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () def __len__ ( self ): return len ( self . arrow_array ) @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas ()","title":"KiaraArray"},{"location":"reference/kiara_plugin/tabular/models/array/#kiara_plugin.tabular.models.array.KiaraArray-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/array/#kiara_plugin.tabular.models.array.KiaraArray.arrow_array","text":"","title":"arrow_array"},{"location":"reference/kiara_plugin/tabular/models/array/#kiara_plugin.tabular.models.array.KiaraArray.data_path","text":"The path to the (feather) file backing this array.","title":"data_path"},{"location":"reference/kiara_plugin/tabular/models/array/#kiara_plugin.tabular.models.array.KiaraArray.create_array","text":"Source code in tabular/models/array.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () if not isinstance ( array_obj , pa . lib . ChunkedArray ): array_obj = pa . chunked_array ( array_obj ) obj . _array_obj = array_obj return obj","title":"create_array()"},{"location":"reference/kiara_plugin/tabular/models/array/#kiara_plugin.tabular.models.array.KiaraArray.to_pandas","text":"Source code in tabular/models/array.py def to_pandas ( self ): return self . arrow_array . to_pandas ()","title":"to_pandas()"},{"location":"reference/kiara_plugin/tabular/models/array/#kiara_plugin.tabular.models.array.KiaraArray.to_pylist","text":"Source code in tabular/models/array.py def to_pylist ( self ): return self . arrow_array . to_pylist ()","title":"to_pylist()"},{"location":"reference/kiara_plugin/tabular/models/db/","text":"Classes \u00b6 DatabaseMetadata ( ValueMetadata ) pydantic-model \u00b6 Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" ) Attributes \u00b6 tables : Dict [ str , kiara_plugin . tabular . models . TableMetadata ] pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage a sqlite database. Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): \"\"\"A wrapper class to manage a sqlite database.\"\"\" @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_cid : CID property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector get_sqlalchemy_metadata ( self ) \u00b6 Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj get_sqlalchemy_table ( self , table_name ) \u00b6 Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table SqliteTableSchema ( BaseModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table Attributes \u00b6 columns : Dict [ str , Literal [ 'NULL' , 'INTEGER' , 'REAL' , 'TEXT' , 'BLOB' ]] pydantic-field required \u00b6 The table columns and their attributes. index_columns : List [ str ] pydantic-field \u00b6 The columns to index nullable_columns : List [ str ] pydantic-field \u00b6 The columns that are nullable. primary_key : str pydantic-field \u00b6 The primary key for this table. unique_columns : List [ str ] pydantic-field \u00b6 The columns that should be marked 'UNIQUE'. Methods \u00b6 create_table ( self , table_name , engine ) \u00b6 Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table create_table_metadata ( self , table_name ) \u00b6 Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table","title":"db"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.DatabaseMetadata","text":"Database and table properties. Source code in tabular/models/db.py class DatabaseMetadata ( ValueMetadata ): \"\"\"Database and table properties.\"\"\" _metadata_key = \"database\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ] @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds ) tables : Dict [ str , TableMetadata ] = Field ( description = \"The table schema.\" )","title":"DatabaseMetadata"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.DatabaseMetadata-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.DatabaseMetadata.tables","text":"The table schema.","title":"tables"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.DatabaseMetadata.create_value_metadata","text":"Source code in tabular/models/db.py @classmethod def create_value_metadata ( cls , value : Value ) -> \"DatabaseMetadata\" : database : KiaraDatabase = value . data insp = database . get_sqlalchemy_inspector () mds = {} for table_name in insp . get_table_names (): with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) num_rows = result . fetchone ()[ 0 ] try : result = con . execute ( text ( f 'SELECT SUM(\"pgsize\") FROM \"dbstat\" WHERE name=\" { table_name } \"' ) ) size : Optional [ int ] = result . fetchone ()[ 0 ] except Exception : size = None columns = {} for column in insp . get_columns ( table_name = table_name ): name = column [ \"name\" ] _type = column [ \"type\" ] type_name = SQLALCHEMY_SQLITE_TYPE_MAP [ type ( _type )] columns [ name ] = { \"type_name\" : type_name , \"metadata\" : { \"nullable\" : column [ \"nullable\" ], \"primary_key\" : True if column [ \"primary_key\" ] else False , }, } schema = { \"column_names\" : list ( columns . keys ()), \"column_schema\" : columns , \"rows\" : num_rows , \"size\" : size , } md = TableMetadata ( ** schema ) mds [ table_name ] = md return DatabaseMetadata . construct ( tables = mds )","title":"create_value_metadata()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.DatabaseMetadata.retrieve_supported_data_types","text":"Source code in tabular/models/db.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"database\" ]","title":"retrieve_supported_data_types()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase","text":"A wrapper class to manage a sqlite database. Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): \"\"\"A wrapper class to manage a sqlite database.\"\"\" @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _tables : Dict [ str , Table ] = PrivateAttr ( default_factory = dict ) _metadata_obj : Optional [ MetaData ] = PrivateAttr ( default = None ) # _table_schemas: Optional[Dict[str, SqliteTableSchema]] = PrivateAttr(default=None) # _file_hash: Optional[str] = PrivateAttr(default=None) _file_cid : Optional [ CID ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( self . file_cid ) def _retrieve_data_to_hash ( self ) -> Any : return self . file_cid @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_cid ( self ) -> CID : if self . _file_cid is not None : return self . _file_cid self . _file_cid = compute_cid_from_file ( file = self . db_file_path , codec = \"raw\" ) return self . _file_cid def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None # self._file_hash = None self . _metadata_obj = None self . _tables . clear () def _invalidate_other ( self ): pass def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table","title":"KiaraDatabase"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.db_file_path","text":"The path to the sqlite database file.","title":"db_file_path"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.db_url","text":"","title":"db_url"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.file_cid","text":"","title":"file_cid"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.table_names","text":"","title":"table_names"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.copy_database_file","text":"Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) # if self._file_hash: # new_db._file_hash = self._file_hash return new_db","title":"copy_database_file()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.create_if_not_exists","text":"Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url )","title":"create_if_not_exists()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.create_in_temp_dir","text":"Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db","title":"create_in_temp_dir()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.ensure_absolute_path","text":"Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path","title":"ensure_absolute_path()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.execute_sql","text":"Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) with self . get_sqlalchemy_engine () . connect () as con : con . execute ( statement ) if invalidate : self . _invalidate ()","title":"execute_sql()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_sqlalchemy_engine","text":"Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine","title":"get_sqlalchemy_engine()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_sqlalchemy_inspector","text":"Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> Inspector : if self . _cached_inspector is not None : return self . _cached_inspector self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector","title":"get_sqlalchemy_inspector()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_sqlalchemy_metadata","text":"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. Source code in tabular/models/db.py def get_sqlalchemy_metadata ( self ) -> MetaData : \"\"\"Return the sqlalchemy Metadtaa object for the underlying database. This is used internally, you typically don't need to access this attribute. \"\"\" if self . _metadata_obj is None : self . _metadata_obj = MetaData () return self . _metadata_obj","title":"get_sqlalchemy_metadata()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_sqlalchemy_table","text":"Return the sqlalchemy edges table instance for this network datab. Source code in tabular/models/db.py def get_sqlalchemy_table ( self , table_name : str ) -> Table : \"\"\"Return the sqlalchemy edges table instance for this network datab.\"\"\" if table_name in self . _tables . keys (): return self . _tables [ table_name ] table = Table ( table_name , self . get_sqlalchemy_metadata (), autoload_with = self . get_sqlalchemy_engine (), ) self . _tables [ table_name ] = table return table","title":"get_sqlalchemy_table()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema","text":"Source code in tabular/models/db.py class SqliteTableSchema ( BaseModel ): columns : Dict [ str , SqliteDataType ] = Field ( description = \"The table columns and their attributes.\" ) index_columns : List [ str ] = Field ( description = \"The columns to index\" , default_factory = list ) nullable_columns : List [ str ] = Field ( description = \"The columns that are nullable.\" , default_factory = list ) unique_columns : List [ str ] = Field ( description = \"The columns that should be marked 'UNIQUE'.\" , default_factory = list ) primary_key : Optional [ str ] = Field ( description = \"The primary key for this table.\" , default = None ) def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table","title":"SqliteTableSchema"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.columns","text":"The table columns and their attributes.","title":"columns"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.index_columns","text":"The columns to index","title":"index_columns"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.nullable_columns","text":"The columns that are nullable.","title":"nullable_columns"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.primary_key","text":"The primary key for this table.","title":"primary_key"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.unique_columns","text":"The columns that should be marked 'UNIQUE'.","title":"unique_columns"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.create_table","text":"Source code in tabular/models/db.py def create_table ( self , table_name : str , engine : Engine ) -> Table : meta , table = self . create_table_metadata ( table_name = table_name ) meta . create_all ( engine ) return table","title":"create_table()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.SqliteTableSchema.create_table_metadata","text":"Create an sql script to initialize a table. Parameters: Name Type Description Default column_attrs a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values required Source code in tabular/models/db.py def create_table_metadata ( self , table_name : str , ) -> Tuple [ MetaData , Table ]: \"\"\"Create an sql script to initialize a table. Arguments: column_attrs: a map with the column name as key, and column details ('type', 'extra_column_info', 'create_index') as values \"\"\" table_columns = [] for column_name , data_type in self . columns . items (): column_obj = Column ( column_name , SQLITE_SQLALCHEMY_TYPE_MAP [ data_type ], nullable = column_name in self . nullable_columns , primary_key = column_name == self . primary_key , index = column_name in self . index_columns , unique = column_name in self . unique_columns , ) table_columns . append ( column_obj ) meta = MetaData () table = Table ( table_name , meta , * table_columns ) return meta , table","title":"create_table_metadata()"},{"location":"reference/kiara_plugin/tabular/models/table/","text":"Classes \u00b6 KiaraTable ( KiaraModel ) pydantic-model \u00b6 A wrapper class to manage tabular data in a memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Union [ None , str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 Return the data as an Apache Arrow Table instance. column_names : Iterable [ str ] property readonly \u00b6 Retrieve the names of all the columns of this table. data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 Return the number of rows in this table. Methods \u00b6 create_table ( data ) classmethod \u00b6 Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"table"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable","text":"A wrapper class to manage tabular data in a memory efficient way. Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): \"\"\"A wrapper class to manage tabular data in a memory efficient way.\"\"\" @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Union [ None , str ] = Field ( description = \"The path to the (feather) file backing this array.\" , default = None ) \"\"\"The path where the table object is store (for internal or read-only use).\"\"\" _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : \"\"\"Return the data as an Apache Arrow Table instance.\"\"\" if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: \"\"\"Retrieve the names of all the columns of this table.\"\"\" return self . arrow_table . column_names @property def num_rows ( self ) -> int : \"\"\"Return the number of rows in this table.\"\"\" return self . arrow_table . num_rows def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict () def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist () def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas ()","title":"KiaraTable"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.arrow_table","text":"Return the data as an Apache Arrow Table instance.","title":"arrow_table"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.column_names","text":"Retrieve the names of all the columns of this table.","title":"column_names"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.data_path","text":"The path to the (feather) file backing this array.","title":"data_path"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.num_rows","text":"Return the number of rows in this table.","title":"num_rows"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.create_table","text":"Create a KiaraTable instance from an Apache Arrow Table, or dict of lists. Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : \"\"\"Create a `KiaraTable` instance from an Apache Arrow Table, or dict of lists.\"\"\" table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create table, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj","title":"create_table()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.to_pandas","text":"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pandas ( self ): \"\"\"Convert and return the table data to a Pandas dataframe. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pandas ()","title":"to_pandas()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.to_pydict","text":"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pydict ( self ): \"\"\"Convert and return the table data as a dictionary of lists. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pydict ()","title":"to_pydict()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.to_pylist","text":"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. Source code in tabular/models/table.py def to_pylist ( self ): \"\"\"Convert and return the table data as a list of rows/dictionaries. This will load all data into memory, so you might or might not want to do that. \"\"\" return self . arrow_table . to_pylist ()","title":"to_pylist()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata","text":"File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" )","title":"KiaraTableMetadata"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata.table","text":"The table schema.","title":"table"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata.create_value_metadata","text":"Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"KiaraTableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata . construct ( ** schema ) return KiaraTableMetadata . construct ( table = md )","title":"create_value_metadata()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata.retrieve_supported_data_types","text":"Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"retrieve_supported_data_types()"},{"location":"reference/kiara_plugin/tabular/modules/__init__/","text":"Modules \u00b6 array special \u00b6 FORCE_NON_NULL_DOC \u00b6 MAX_INDEX_DOC \u00b6 MIN_INDEX_DOC \u00b6 REMOVE_TOKENS_DOC \u00b6 Classes \u00b6 DeserializeArrayModule ( DeserializeValueModule ) \u00b6 Deserialize array data. Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): \"\"\"Deserialize array data.\"\"\" _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array ExtractDateConfig ( KiaraInputsConfig ) pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. ExtractDateModule ( AutoInputsKiaraModule ) \u00b6 Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the dateutil package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): \"\"\"Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the [dateutil](https://github.com/dateutil/dateutil) package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: - 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) - remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. \"\"\" _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) Classes \u00b6 _config_cls ( KiaraInputsConfig ) private pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) db special \u00b6 Classes \u00b6 CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. Methods \u00b6 create__database__from__csv_file ( self , source_value ) \u00b6 Create a database from a csv_file value. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__table ( self , source_value , optional ) \u00b6 Create a database value from a table. Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db create_optional_inputs ( self , source_type , target_type ) \u00b6 Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db QueryDatabaseConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. QueryDatabaseModule ( KiaraModule ) \u00b6 Execute a sql query against a (sqlite) database. Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): \"\"\"Execute a sql query against a (sqlite) database.\"\"\" _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) RenderDatabaseModule ( RenderDatabaseModuleBase ) \u00b6 Source code in tabular/modules/db/__init__.py class RenderDatabaseModule ( RenderDatabaseModuleBase ): _module_type_name = \"render.database\" def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , ) render__database__as__string ( self , value , render_config ) \u00b6 Source code in tabular/modules/db/__init__.py def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) render__database__as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/modules/db/__init__.py def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , ) RenderDatabaseModuleBase ( RenderValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class RenderDatabaseModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables preprocess_database ( self , value , table_name , input_number_of_rows , input_row_offset ) \u00b6 Source code in tabular/modules/db/__init__.py def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables table special \u00b6 EMPTY_COLUMN_NAME_MARKER \u00b6 Classes \u00b6 CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. Methods \u00b6 create__table__from__csv_file ( self , source_value ) \u00b6 Create a table from a csv_file value. Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) create__table__from__text_file_bundle ( self , source_value ) \u00b6 Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) DeserializeTableModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table ExportTableModule ( DataExportModule ) \u00b6 Export table data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export table data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path} Methods \u00b6 export__table__as__csv_file ( self , value , base_path , name ) \u00b6 Export a table as csv file. Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } MergeTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. MergeTableModule ( KiaraModule ) \u00b6 Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' \"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) QueryTableSQL ( KiaraModule ) \u00b6 Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. \"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ()) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ()) QueryTableSQLModuleConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. RenderTableModule ( RenderTableModuleBase ) \u00b6 Source code in tabular/modules/table/__init__.py class RenderTableModule ( RenderTableModuleBase ): _module_type_name = \"render.table\" def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) render__table__as__string ( self , value , render_config ) \u00b6 Source code in tabular/modules/table/__init__.py def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) render__table__as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/modules/table/__init__.py def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) RenderTableModuleBase ( RenderValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class RenderTableModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes preprocess_table ( self , value , input_number_of_rows , input_row_offset ) \u00b6 Source code in tabular/modules/table/__init__.py def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes filters \u00b6 TableFiltersModule ( FilterModule ) \u00b6 Source code in tabular/modules/table/filters.py class TableFiltersModule ( FilterModule ): _module_type_name = \"table.filters\" @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\" def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass create_filter_inputs ( self , filter_name ) \u00b6 Source code in tabular/modules/table/filters.py def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None filter__drop_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_rows ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass retrieve_supported_type () classmethod \u00b6 Source code in tabular/modules/table/filters.py @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\"","title":"modules"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array","text":"","title":"array"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.FORCE_NON_NULL_DOC","text":"","title":"FORCE_NON_NULL_DOC"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.MAX_INDEX_DOC","text":"","title":"MAX_INDEX_DOC"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.MIN_INDEX_DOC","text":"","title":"MIN_INDEX_DOC"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.REMOVE_TOKENS_DOC","text":"","title":"REMOVE_TOKENS_DOC"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule","text":"Deserialize array data. Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): \"\"\"Deserialize array data.\"\"\" _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array","title":"DeserializeArrayModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig","text":"Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list )","title":"ExtractDateConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig-attributes","text":"force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule","text":"Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the dateutil package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): \"\"\"Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the [dateutil](https://github.com/dateutil/dateutil) package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: - 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) - remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. \"\"\" _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"ExtractDateModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule-classes","text":"_config_cls ( KiaraInputsConfig ) private pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule","text":"Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db","title":"CreateDatabaseModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule-classes","text":"_config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule-methods","text":"create__database__from__csv_file ( self , source_value ) \u00b6 Create a database from a csv_file value. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__table ( self , source_value , optional ) \u00b6 Create a database value from a table. Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db create_optional_inputs ( self , source_type , target_type ) \u00b6 Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig","text":"Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , )","title":"CreateDatabaseModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule","text":"Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db","title":"LoadDatabaseFromDiskModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseConfig","text":"Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None )","title":"QueryDatabaseConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseConfig-attributes","text":"query : str pydantic-field \u00b6 The query.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule","text":"Execute a sql query against a (sqlite) database. Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): \"\"\"Execute a sql query against a (sqlite) database.\"\"\" _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table )","title":"QueryDatabaseModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule-classes","text":"_config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.RenderDatabaseModule","text":"Source code in tabular/modules/db/__init__.py class RenderDatabaseModule ( RenderDatabaseModuleBase ): _module_type_name = \"render.database\" def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , ) render__database__as__string ( self , value , render_config ) \u00b6 Source code in tabular/modules/db/__init__.py def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) render__database__as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/modules/db/__init__.py def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , )","title":"RenderDatabaseModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.db.RenderDatabaseModuleBase","text":"Source code in tabular/modules/db/__init__.py class RenderDatabaseModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables preprocess_database ( self , value , table_name , input_number_of_rows , input_row_offset ) \u00b6 Source code in tabular/modules/db/__init__.py def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables","title":"RenderDatabaseModuleBase"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.EMPTY_COLUMN_NAME_MARKER","text":"","title":"EMPTY_COLUMN_NAME_MARKER"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule","text":"Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"CreateTableModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule-classes","text":"_config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule-methods","text":"create__table__from__csv_file ( self , source_value ) \u00b6 Create a table from a csv_file value. Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) create__table__from__text_file_bundle ( self , source_value ) \u00b6 Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CreateTableModuleConfig","text":"Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , )","title":"CreateTableModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CreateTableModuleConfig-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule","text":"Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"CutColumnModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule","text":"Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table","title":"DeserializeTableModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.ExportTableModule","text":"Export table data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export table data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path}","title":"ExportTableModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.ExportTableModule-methods","text":"export__table__as__csv_file ( self , value , base_path , name ) \u00b6 Export a table as csv file. Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path }","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig","text":"Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict )","title":"MergeTableConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig-attributes","text":"column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule","text":"Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' \"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table )","title":"MergeTableModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule-classes","text":"_config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL","text":"Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. \"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ())","title":"QueryTableSQL"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL-classes","text":"_config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ())","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig","text":"Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , )","title":"QueryTableSQLModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig-attributes","text":"query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.RenderTableModule","text":"Source code in tabular/modules/table/__init__.py class RenderTableModule ( RenderTableModuleBase ): _module_type_name = \"render.table\" def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) render__table__as__string ( self , value , render_config ) \u00b6 Source code in tabular/modules/table/__init__.py def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) render__table__as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/modules/table/__init__.py def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , )","title":"RenderTableModule"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.RenderTableModuleBase","text":"Source code in tabular/modules/table/__init__.py class RenderTableModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes preprocess_table ( self , value , input_number_of_rows , input_row_offset ) \u00b6 Source code in tabular/modules/table/__init__.py def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes","title":"RenderTableModuleBase"},{"location":"reference/kiara_plugin/tabular/modules/__init__/#kiara_plugin.tabular.modules.table.filters","text":"TableFiltersModule ( FilterModule ) \u00b6 Source code in tabular/modules/table/filters.py class TableFiltersModule ( FilterModule ): _module_type_name = \"table.filters\" @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\" def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass create_filter_inputs ( self , filter_name ) \u00b6 Source code in tabular/modules/table/filters.py def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None filter__drop_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_rows ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass retrieve_supported_type () classmethod \u00b6 Source code in tabular/modules/table/filters.py @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\"","title":"filters"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/","text":"FORCE_NON_NULL_DOC \u00b6 MAX_INDEX_DOC \u00b6 MIN_INDEX_DOC \u00b6 REMOVE_TOKENS_DOC \u00b6 Classes \u00b6 DeserializeArrayModule ( DeserializeValueModule ) \u00b6 Deserialize array data. Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): \"\"\"Deserialize array data.\"\"\" _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array ExtractDateConfig ( KiaraInputsConfig ) pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. ExtractDateModule ( AutoInputsKiaraModule ) \u00b6 Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the dateutil package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): \"\"\"Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the [dateutil](https://github.com/dateutil/dateutil) package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: - 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) - remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. \"\"\" _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked ) Classes \u00b6 _config_cls ( KiaraInputsConfig ) private pydantic-model \u00b6 Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list ) Attributes \u00b6 force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"array"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.FORCE_NON_NULL_DOC","text":"","title":"FORCE_NON_NULL_DOC"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.MAX_INDEX_DOC","text":"","title":"MAX_INDEX_DOC"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.MIN_INDEX_DOC","text":"","title":"MIN_INDEX_DOC"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.REMOVE_TOKENS_DOC","text":"","title":"REMOVE_TOKENS_DOC"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule","text":"Deserialize array data. Source code in tabular/modules/array/__init__.py class DeserializeArrayModule ( DeserializeValueModule ): \"\"\"Deserialize array data.\"\"\" _module_type_name = \"load.array\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array","title":"DeserializeArrayModule"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule.retrieve_serialized_value_type","text":"Source code in tabular/modules/array/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"array\"","title":"retrieve_serialized_value_type()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule.retrieve_supported_serialization_profile","text":"Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\"","title":"retrieve_supported_serialization_profile()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule.retrieve_supported_target_profiles","text":"Source code in tabular/modules/array/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraArray }","title":"retrieve_supported_target_profiles()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.DeserializeArrayModule.to__python_object","text":"Source code in tabular/modules/array/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"array.arrow\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"array.arrow\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 array_file = files [ 0 ] array = KiaraArray ( data_path = array_file ) return array","title":"to__python_object()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig","text":"Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list )","title":"ExtractDateConfig"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig.force_non_null","text":"If set to 'True', raise an error if any of the strings in the array can't be parsed.","title":"force_non_null"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig.max_index","text":"The maximum index until whic to parse the string(s).","title":"max_index"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig.min_index","text":"The minimum index from where to start parsing the string(s).","title":"min_index"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateConfig.remove_tokens","text":"A list of tokens/characters to replace with a single white-space before parsing the input.","title":"remove_tokens"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule","text":"Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the dateutil package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. Source code in tabular/modules/array/__init__.py class ExtractDateModule ( AutoInputsKiaraModule ): \"\"\"Create an array of date objects from an array of strings. This module is very simplistic at the moment, more functionality and options will be added in the future. At its core, this module uses the standard parser from the [dateutil](https://github.com/dateutil/dateutil) package to parse strings into dates. As this parser can't handle complex strings, the input strings can be pre-processed in the following ways: - 'cut' non-relevant parts of the string (using 'min_index' & 'max_index' input/config options) - remove matching tokens from the string, and replace them with a single whitespace (using the 'remove_tokens' option) By default, if an input string can't be parsed this module will raise an exception. This can be prevented by setting this modules 'force_non_null' config option or input to 'False', in which case un-parsable strings will appear as 'NULL' value in the resulting array. \"\"\" _module_type_name = \"parse.date_array\" _config_cls = ExtractDateConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } } def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"ExtractDateModule"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule._config_cls","text":"Source code in tabular/modules/array/__init__.py class ExtractDateConfig ( KiaraInputsConfig ): force_non_null : bool = Field ( description = FORCE_NON_NULL_DOC , default = True ) min_index : Union [ None , int ] = Field ( description = MIN_INDEX_DOC , default = None , ) max_index : Union [ None , int ] = Field ( description = MAX_INDEX_DOC , default = None ) remove_tokens : List [ str ] = Field ( description = REMOVE_TOKENS_DOC , default_factory = list )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule._config_cls-attributes","text":"force_non_null : bool pydantic-field \u00b6 If set to 'True', raise an error if any of the strings in the array can't be parsed. max_index : int pydantic-field \u00b6 The maximum index until whic to parse the string(s). min_index : int pydantic-field \u00b6 The minimum index from where to start parsing the string(s). remove_tokens : List [ str ] pydantic-field \u00b6 A list of tokens/characters to replace with a single white-space before parsing the input.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/modules/array/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The input array.\" }} return inputs","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/modules/array/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"date_array\" : { \"type\" : \"array\" , \"doc\" : \"The resulting array with items of a date data type.\" , } }","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/array/__init__/#kiara_plugin.tabular.modules.array.ExtractDateModule.process","text":"Source code in tabular/modules/array/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ): import polars as pl import pyarrow as pa from dateutil import parser force_non_null : bool = self . get_data_for_field ( field_name = \"force_non_null\" , inputs = inputs ) min_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"min_index\" , inputs = inputs ) if min_pos is None : min_pos = 0 max_pos : Union [ None , int ] = self . get_data_for_field ( field_name = \"max_index\" , inputs = inputs ) remove_tokens : Iterable [ str ] = self . get_data_for_field ( field_name = \"remove_tokens\" , inputs = inputs ) def parse_date ( _text : str ): text = _text if min_pos : try : text = text [ min_pos :] # type: ignore except Exception : return None if max_pos : try : text = text [ 0 : max_pos - min_pos ] # type: ignore # noqa except Exception : pass if remove_tokens : for t in remove_tokens : text = text . replace ( t , \" \" ) try : d_obj = parser . parse ( text , fuzzy = True ) except Exception as e : if force_non_null : raise KiaraProcessingException ( e ) return None if d_obj is None : if force_non_null : raise KiaraProcessingException ( f \"Can't parse date from string: { text } \" ) return None return d_obj value = inputs . get_value_obj ( \"array\" ) array : KiaraArray = value . data series = pl . Series ( name = \"tokens\" , values = array . arrow_array ) job_log . add_log ( f \"start parsing date for { len ( array ) } items\" ) result = series . apply ( parse_date ) job_log . add_log ( f \"finished parsing date for { len ( array ) } items\" ) result_array = result . to_arrow () # TODO: remove this cast once the array data type can handle non-chunked arrays chunked = pa . chunked_array ( result_array ) outputs . set_values ( date_array = chunked )","title":"process()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/","text":"Classes \u00b6 CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. Methods \u00b6 create__database__from__csv_file ( self , source_value ) \u00b6 Create a database from a csv_file value. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path create__database__from__table ( self , source_value , optional ) \u00b6 Create a database value from a table. Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db create_optional_inputs ( self , source_type , target_type ) \u00b6 Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db QueryDatabaseConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. QueryDatabaseModule ( KiaraModule ) \u00b6 Execute a sql query against a (sqlite) database. Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): \"\"\"Execute a sql query against a (sqlite) database.\"\"\" _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None ) Attributes \u00b6 query : str pydantic-field \u00b6 The query. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table ) RenderDatabaseModule ( RenderDatabaseModuleBase ) \u00b6 Source code in tabular/modules/db/__init__.py class RenderDatabaseModule ( RenderDatabaseModuleBase ): _module_type_name = \"render.database\" def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , ) render__database__as__string ( self , value , render_config ) \u00b6 Source code in tabular/modules/db/__init__.py def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) render__database__as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/modules/db/__init__.py def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , ) RenderDatabaseModuleBase ( RenderValueModule ) \u00b6 Source code in tabular/modules/db/__init__.py class RenderDatabaseModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables preprocess_database ( self , value , table_name , input_number_of_rows , input_row_offset ) \u00b6 Source code in tabular/modules/db/__init__.py def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables","title":"db"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule","text":"Source code in tabular/modules/db/__init__.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"create.database\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db","title":"CreateDatabaseModule"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule._config_cls","text":"Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule._config_cls-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule.create__database__from__csv_file","text":"Create a database from a csv_file value. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file value.\"\"\" temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) file_item : FileModel = source_value . data table_name = file_item . file_name_without_extension table_name = table_name . replace ( \"-\" , \"_\" ) table_name = table_name . replace ( \".\" , \"_\" ) try : create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = file_item . path , reason = str ( e )) else : raise KiaraProcessingException ( e ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) if include_raw_content_in_file_info : db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () included_files = { file_item . file_name : file_item } file_bundle = FileBundle . create_from_file_models ( files = included_files , bundle_name = file_item . file_name ) insert_db_table_from_file_bundle ( database = db , file_bundle = file_bundle , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path","title":"create__database__from__csv_file()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule.create__database__from__csv_file_bundle","text":"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. Source code in tabular/modules/db/__init__.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a database from a csv_file_bundle value. Unless 'merge_into_single_table' is set to 'True', each csv file will create one table in the resulting database. If this option is set, only a single table with all the values of all csv files will be created. For this to work, all csv files should follow the same schema. \"\"\" merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : Optional [ bool ] = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info in [ None , True ]: include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path","title":"create__database__from__csv_file_bundle()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule.create__database__from__table","text":"Create a database value from a table. Source code in tabular/modules/db/__init__.py def create__database__from__table ( self , source_value : Value , optional : ValueMap ) -> Any : \"\"\"Create a database value from a table.\"\"\" table_name = optional . get_value_data ( \"table_name\" ) if not table_name : table_name = \"imported_table\" table : KiaraTable = source_value . data arrow_table = table . arrow_table column_map = None index_columns = None sqlite_schema = create_sqlite_schema_data_from_arrow_table ( table = arrow_table , index_columns = index_columns , column_map = column_map ) db = KiaraDatabase . create_in_temp_dir () db . _unlock_db () engine = db . get_sqlalchemy_engine () _table = sqlite_schema . create_table ( table_name = table_name , engine = engine ) with engine . connect () as conn : for batch in arrow_table . to_batches ( max_chunksize = DEFAULT_TABULAR_DATA_CHUNK_SIZE ): conn . execute ( insert ( _table ), batch . to_pylist ()) conn . commit () db . _lock_db () return db","title":"create__database__from__table()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModule.create_optional_inputs","text":"Source code in tabular/modules/db/__init__.py def create_optional_inputs ( self , source_type : str , target_type ) -> Optional [ Mapping [ str , Mapping [ str , Any ]]]: if target_type == \"database\" and source_type == \"table\" : return { \"table_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the table in the new database.\" , \"default\" : \"imported_table\" , } } else : return None","title":"create_optional_inputs()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig","text":"Source code in tabular/modules/db/__init__.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : Optional [ bool ] = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = None , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , )","title":"CreateDatabaseModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig.ignore_errors","text":"Whether to ignore convert errors and omit the failed items.","title":"ignore_errors"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig.include_source_file_content","text":"When including source metadata, whether to also include the original raw (string) content.","title":"include_source_file_content"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig.include_source_metadata","text":"Whether to include a table with metadata about the source files.","title":"include_source_metadata"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.CreateDatabaseModuleConfig.merge_into_single_table","text":"Whether to merge all csv files into a single table.","title":"merge_into_single_table"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule","text":"Source code in tabular/modules/db/__init__.py class LoadDatabaseFromDiskModule ( DeserializeValueModule ): _module_type_name = \"load.database\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\" def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db","title":"LoadDatabaseFromDiskModule"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule.retrieve_serialized_value_type","text":"Source code in tabular/modules/db/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"database\"","title":"retrieve_serialized_value_type()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule.retrieve_supported_serialization_profile","text":"Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"copy\"","title":"retrieve_supported_serialization_profile()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule.retrieve_supported_target_profiles","text":"Source code in tabular/modules/db/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraDatabase }","title":"retrieve_supported_target_profiles()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.LoadDatabaseFromDiskModule.to__python_object","text":"Source code in tabular/modules/db/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): assert \"db.sqlite\" in data . get_keys () and len ( list ( data . get_keys ())) == 1 chunks = data . get_serialized_data ( \"db.sqlite\" ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 db_file = files [ 0 ] db = KiaraDatabase ( db_file_path = db_file ) return db","title":"to__python_object()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseConfig","text":"Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None )","title":"QueryDatabaseConfig"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseConfig.query","text":"The query.","title":"query"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule","text":"Execute a sql query against a (sqlite) database. Source code in tabular/modules/db/__init__.py class QueryDatabaseModule ( KiaraModule ): \"\"\"Execute a sql query against a (sqlite) database.\"\"\" _config_cls = QueryDatabaseConfig _module_type_name = \"query.database\" def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table )","title":"QueryDatabaseModule"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule._config_cls","text":"Source code in tabular/modules/db/__init__.py class QueryDatabaseConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query.\" , default = None )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule._config_cls-attributes","text":"query : str pydantic-field \u00b6 The query.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/modules/db/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : result : Dict [ str , Dict [ str , Any ]] = { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database to query.\" } } if not self . get_config_value ( \"query\" ): result [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query to execute.\" } return result","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/modules/db/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }}","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.QueryDatabaseModule.process","text":"Source code in tabular/modules/db/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa database : KiaraDatabase = inputs . get_value_data ( \"database\" ) query = self . get_config_value ( \"query\" ) if query is None : query = inputs . get_value_data ( \"query\" ) # TODO: make this memory efficent result_columns : Dict [ str , List [ Any ]] = {} with database . get_sqlalchemy_engine () . connect () as con : result = con . execute ( text ( query )) for r in result : for k , v in dict ( r ) . items (): result_columns . setdefault ( k , []) . append ( v ) table = pa . Table . from_pydict ( result_columns ) outputs . set_value ( \"query_result\" , table )","title":"process()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.RenderDatabaseModule","text":"Source code in tabular/modules/db/__init__.py class RenderDatabaseModule ( RenderDatabaseModuleBase ): _module_type_name = \"render.database\" def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , ) def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , )","title":"RenderDatabaseModule"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.RenderDatabaseModule.render__database__as__string","text":"Source code in tabular/modules/db/__init__.py def render__database__as__string ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , rendered = pretty , related_scenes = data_related_scenes , render_config = render_config , render_manifest = self . manifest . manifest_hash , )","title":"render__database__as__string()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.RenderDatabaseModule.render__database__as__terminal_renderable","text":"Source code in tabular/modules/db/__init__.py def render__database__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) table_name = render_config . get ( \"table_name\" , None ) wrap , data_related_scenes = self . preprocess_database ( value = value , table_name = table_name , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , rendered = pretty , related_scenes = data_related_scenes , render_manifest = self . manifest . manifest_hash , )","title":"render__database__as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.RenderDatabaseModuleBase","text":"Source code in tabular/modules/db/__init__.py class RenderDatabaseModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables","title":"RenderDatabaseModuleBase"},{"location":"reference/kiara_plugin/tabular/modules/db/__init__/#kiara_plugin.tabular.modules.db.RenderDatabaseModuleBase.preprocess_database","text":"Source code in tabular/modules/db/__init__.py def preprocess_database ( self , value : Value , table_name : Union [ str , None ], input_number_of_rows : int , input_row_offset : int , ): database : KiaraDatabase = value . data table_names = database . table_names if not table_name : table_name = list ( table_names )[ 0 ] if table_name not in table_names : raise Exception ( f \"Invalid table name: { table_name } . Available: { ', ' . join ( table_names ) } \" ) related_scenes_tables : Dict [ str , Union [ RenderScene , None ]] = { t : RenderScene . construct ( title = t , description = f \"Display the ' { t } ' table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"table_name\" : t }, ) for t in database . table_names } query = f \"\"\"SELECT * FROM { table_name } LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" result : Dict [ str , List [ Any ]] = {} # TODO: this could be written much more efficient with database . get_sqlalchemy_engine () . connect () as con : num_rows_result = con . execute ( text ( f \"SELECT count(*) from { table_name } \" )) table_num_rows = num_rows_result . fetchone ()[ 0 ] rs = con . execute ( text ( query )) for r in rs : for k , v in dict ( r ) . items (): result . setdefault ( k , []) . append ( v ) wrap = DictTabularWrap ( data = result ) row_offset = table_num_rows - input_number_of_rows related_scenes : Dict [ str , Union [ RenderScene , None ]] = {} if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < table_num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows , \"table_name\" : table_name , } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( table_num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > table_num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore \"table_name\" : table_name , }, ) related_scenes_tables [ table_name ] . disabled = True # type: ignore related_scenes_tables [ table_name ] . related_scenes = related_scenes # type: ignore return wrap , related_scenes_tables","title":"preprocess_database()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/","text":"EMPTY_COLUMN_NAME_MARKER \u00b6 Classes \u00b6 CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. Methods \u00b6 create__table__from__csv_file ( self , source_value ) \u00b6 Create a table from a csv_file value. Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) create__table__from__text_file_bundle ( self , source_value ) \u00b6 Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) DeserializeTableModule ( DeserializeValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table retrieve_serialized_value_type () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" retrieve_supported_serialization_profile () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" retrieve_supported_target_profiles () classmethod \u00b6 Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } to__python_object ( self , data , ** config ) \u00b6 Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table ExportTableModule ( DataExportModule ) \u00b6 Export table data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export table data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path} Methods \u00b6 export__table__as__csv_file ( self , value , base_path , name ) \u00b6 Export a table as csv file. Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } MergeTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. MergeTableModule ( KiaraModule ) \u00b6 Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' \"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict ) Attributes \u00b6 column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs process ( self , inputs , outputs , job_log ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table ) QueryTableSQL ( KiaraModule ) \u00b6 Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. \"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ()) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ()) QueryTableSQLModuleConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , ) Attributes \u00b6 query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own. RenderTableModule ( RenderTableModuleBase ) \u00b6 Source code in tabular/modules/table/__init__.py class RenderTableModule ( RenderTableModuleBase ): _module_type_name = \"render.table\" def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) render__table__as__string ( self , value , render_config ) \u00b6 Source code in tabular/modules/table/__init__.py def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) render__table__as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/modules/table/__init__.py def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) RenderTableModuleBase ( RenderValueModule ) \u00b6 Source code in tabular/modules/table/__init__.py class RenderTableModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes preprocess_table ( self , value , input_number_of_rows , input_row_offset ) \u00b6 Source code in tabular/modules/table/__init__.py def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes filters \u00b6 TableFiltersModule ( FilterModule ) \u00b6 Source code in tabular/modules/table/filters.py class TableFiltersModule ( FilterModule ): _module_type_name = \"table.filters\" @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\" def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass create_filter_inputs ( self , filter_name ) \u00b6 Source code in tabular/modules/table/filters.py def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None filter__drop_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_rows ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass retrieve_supported_type () classmethod \u00b6 Source code in tabular/modules/table/filters.py @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\"","title":"table"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.EMPTY_COLUMN_NAME_MARKER","text":"","title":"EMPTY_COLUMN_NAME_MARKER"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule","text":"Source code in tabular/modules/table/__init__.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"create.table\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data ) def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"CreateTableModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule._config_cls","text":"Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule._config_cls-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule.create__table__from__csv_file","text":"Create a table from a csv_file value. Source code in tabular/modules/table/__init__.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : \"\"\"Create a table from a csv_file value.\"\"\" from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) # import pandas as pd # df = pd.read_csv(input_file.path) # imported_data = pa.Table.from_pandas(df) return KiaraTable . create_table ( imported_data )","title":"create__table__from__csv_file()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModule.create__table__from__text_file_bundle","text":"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content Source code in tabular/modules/table/__init__.py def create__table__from__text_file_bundle ( self , source_value : Value ) -> Any : \"\"\"Create a table value from a text file_bundle. The resulting table will have (at a minimum) the following collumns: - id: an auto-assigned index - rel_path: the relative path of the file (from the provided base path) - content: the text file content \"\"\" import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"create__table__from__text_file_bundle()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModuleConfig","text":"Source code in tabular/modules/table/__init__.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , )","title":"CreateTableModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModuleConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CreateTableModuleConfig.ignore_errors","text":"Whether to ignore convert errors and omit the failed items.","title":"ignore_errors"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule","text":"Cut off one column from a table, returning an array. Source code in tabular/modules/table/__init__.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"CutColumnModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.CutColumnModule.process","text":"Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"process()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule","text":"Source code in tabular/modules/table/__init__.py class DeserializeTableModule ( DeserializeValueModule ): _module_type_name = \"load.table\" @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable } @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\" @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\" def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table","title":"DeserializeTableModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule.retrieve_serialized_value_type","text":"Source code in tabular/modules/table/__init__.py @classmethod def retrieve_serialized_value_type ( cls ) -> str : return \"table\"","title":"retrieve_serialized_value_type()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule.retrieve_supported_serialization_profile","text":"Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_serialization_profile ( cls ) -> str : return \"feather\"","title":"retrieve_supported_serialization_profile()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule.retrieve_supported_target_profiles","text":"Source code in tabular/modules/table/__init__.py @classmethod def retrieve_supported_target_profiles ( cls ) -> Mapping [ str , Type ]: return { \"python_object\" : KiaraTable }","title":"retrieve_supported_target_profiles()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.DeserializeTableModule.to__python_object","text":"Source code in tabular/modules/table/__init__.py def to__python_object ( self , data : SerializedData , ** config : Any ): import pyarrow as pa columns = {} for column_name in data . get_keys (): chunks = data . get_serialized_data ( column_name ) # TODO: support multiple chunks assert chunks . get_number_of_chunks () == 1 files = list ( chunks . get_chunks ( as_files = True , symlink_ok = True )) assert len ( files ) == 1 file = files [ 0 ] with pa . memory_map ( file , \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) return table","title":"to__python_object()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.ExportTableModule","text":"Export table data items. Source code in tabular/modules/table/__init__.py class ExportTableModule ( DataExportModule ): \"\"\"Export table data items.\"\"\" _module_type_name = \"export.table\" def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path } # def export__table__as__sqlite_db( # self, value: KiaraTable, base_path: str, name: str # ): # # target_path = os.path.abspath(os.path.join(base_path, f\"{name}.sqlite\")) # # raise NotImplementedError() # # shutil.copy2(value.db_file_path, target_path) # # return {\"files\": target_path}","title":"ExportTableModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.ExportTableModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.ExportTableModule.export__table__as__csv_file","text":"Export a table as csv file. Source code in tabular/modules/table/__init__.py def export__table__as__csv_file ( self , value : KiaraTable , base_path : str , name : str ): \"\"\"Export a table as csv file.\"\"\" import pyarrow.csv as csv target_path = os . path . join ( base_path , f \" { name } .csv\" ) csv . write_csv ( value . arrow_table , target_path ) return { \"files\" : target_path }","title":"export__table__as__csv_file()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig","text":"Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict )","title":"MergeTableConfig"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig.column_map","text":"A map describing","title":"column_map"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableConfig.inputs_schema","text":"A dict describing the inputs for this merge process.","title":"inputs_schema"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule","text":"Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' Source code in tabular/modules/table/__init__.py class MergeTableModule ( KiaraModule ): \"\"\"Create a table from other tables and/or arrays. This module needs configuration to be set (for now). It's currently not possible to merge an arbitrary number of tables/arrays, all tables to be merged must be specified in the module configuration. Column names of the resulting table can be controlled by the 'column_map' configuration, which takes the desired column name as key, and a field-name in the following format as value: - '[inputs_schema key]' for inputs of type 'array' - '[inputs_schema_key].orig_column_name' for inputs of type 'table' \"\"\" _module_type_name = \"table.merge\" _config_cls = MergeTableConfig def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table )","title":"MergeTableModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule._config_cls","text":"Source code in tabular/modules/table/__init__.py class MergeTableConfig ( KiaraModuleConfig ): inputs_schema : Dict [ str , ValueSchema ] = Field ( description = \"A dict describing the inputs for this merge process.\" ) column_map : Dict [ str , str ] = Field ( description = \"A map describing\" , default_factory = dict )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule._config_cls-attributes","text":"column_map : Dict [ str , str ] pydantic-field \u00b6 A map describing inputs_schema : Dict [ str , kiara . models . values . value_schema . ValueSchema ] pydantic-field required \u00b6 A dict describing the inputs for this merge process.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : input_schema_dict = self . get_config_value ( \"inputs_schema\" ) return input_schema_dict","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : outputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The merged table, including all source tables and columns.\" , } } return outputs","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.MergeTableModule.process","text":"Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap , job_log : JobLog ) -> None : import pyarrow as pa inputs_schema : Dict [ str , Any ] = self . get_config_value ( \"inputs_schema\" ) column_map : Dict [ str , str ] = self . get_config_value ( \"column_map\" ) sources = {} for field_name in inputs_schema . keys (): sources [ field_name ] = inputs . get_value_data ( field_name ) len_dict = {} arrays = {} column_map_final = dict ( column_map ) for source_key , table_or_array in sources . items (): if isinstance ( table_or_array , KiaraTable ): rows = table_or_array . num_rows for name in table_or_array . column_names : array_name = f \" { source_key } . { name } \" if column_map and array_name not in column_map . values (): job_log . add_log ( f \"Ignoring column ' { name } ' of input table ' { source_key } ': not listed in column_map.\" ) continue column = table_or_array . arrow_table . column ( name ) arrays [ array_name ] = column if not column_map : if name in column_map_final : raise Exception ( f \"Can't merge table, duplicate column name: { name } .\" ) column_map_final [ name ] = array_name elif isinstance ( table_or_array , KiaraArray ): if column_map and source_key not in column_map . values (): job_log . add_log ( f \"Ignoring array ' { source_key } ': not listed in column_map.\" ) continue rows = len ( table_or_array ) arrays [ source_key ] = table_or_array . arrow_array if not column_map : if source_key in column_map_final . keys (): raise Exception ( f \"Can't merge table, duplicate column name: { source_key } .\" ) column_map_final [ source_key ] = source_key else : raise KiaraProcessingException ( f \"Can't merge table: invalid type ' { type ( table_or_array ) } ' for source ' { source_key } '.\" ) len_dict [ source_key ] = rows all_rows = None for source_key , rows in len_dict . items (): if all_rows is None : all_rows = rows else : if all_rows != rows : all_rows = None break if all_rows is None : len_str = \"\" for name , rows in len_dict . items (): len_str = f \" { name } ( { rows } )\" raise KiaraProcessingException ( f \"Can't merge table, sources have different lengths: { len_str } \" ) column_names = [] columns = [] for column_name , ref in column_map_final . items (): column_names . append ( column_name ) column = arrays [ ref ] columns . append ( column ) table = pa . Table . from_arrays ( arrays = columns , names = column_names ) outputs . set_value ( \"table\" , table )","title":"process()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL","text":"Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. Source code in tabular/modules/table/__init__.py class QueryTableSQL ( KiaraModule ): \"\"\"Execute a sql query against an (Arrow) table. The default relation name for the sql query is 'data', but can be modified by the 'relation_name' config option/input. If the 'query' module config option is not set, users can provide their own query, otherwise the pre-set one will be used. \"\"\" _module_type_name = \"query.table\" _config_cls = QueryTableSQLModuleConfig def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ())","title":"QueryTableSQL"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL._config_cls","text":"Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL._config_cls-attributes","text":"query : str pydantic-field \u00b6 The query to execute. If not specified, the user will be able to provide their own. relation_name : str pydantic-field \u00b6 The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/modules/table/__init__.py def create_inputs_schema ( self , ) -> ValueMapSchema : inputs = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table to query\" , } } if self . get_config_value ( \"query\" ) is None : inputs [ \"query\" ] = { \"type\" : \"string\" , \"doc\" : \"The query.\" } inputs [ \"relation_name\" ] = { \"type\" : \"string\" , \"doc\" : \"The name the table is referred to in the sql query.\" , \"default\" : \"data\" , } return inputs","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/modules/table/__init__.py def create_outputs_schema ( self , ) -> ValueMapSchema : return { \"query_result\" : { \"type\" : \"table\" , \"doc\" : \"The query result.\" }}","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQL.process","text":"Source code in tabular/modules/table/__init__.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import duckdb if self . get_config_value ( \"query\" ) is None : _query : str = inputs . get_value_data ( \"query\" ) _relation_name : str = inputs . get_value_data ( \"relation_name\" ) else : _query = self . get_config_value ( \"query\" ) _relation_name = self . get_config_value ( \"relation_name\" ) if _relation_name . upper () in RESERVED_SQL_KEYWORDS : raise KiaraProcessingException ( f \"Invalid relation name ' { _relation_name } ': this is a reserved sql keyword, please select a different name.\" ) _table : KiaraTable = inputs . get_value_data ( \"table\" ) rel_from_arrow = duckdb . arrow ( _table . arrow_table ) result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( _relation_name , _query ) outputs . set_value ( \"query_result\" , result . arrow ())","title":"process()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig","text":"Source code in tabular/modules/table/__init__.py class QueryTableSQLModuleConfig ( KiaraModuleConfig ): query : Optional [ str ] = Field ( description = \"The query to execute. If not specified, the user will be able to provide their own.\" , default = None , ) relation_name : Optional [ str ] = Field ( description = \"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.\" , default = \"data\" , )","title":"QueryTableSQLModuleConfig"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig.query","text":"The query to execute. If not specified, the user will be able to provide their own.","title":"query"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.QueryTableSQLModuleConfig.relation_name","text":"The name the table is referred to in the sql query. If not specified, the user will be able to provide their own.","title":"relation_name"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.RenderTableModule","text":"Source code in tabular/modules/table/__init__.py class RenderTableModule ( RenderTableModuleBase ): _module_type_name = \"render.table\" def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , ) def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , )","title":"RenderTableModule"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.RenderTableModule.render__table__as__string","text":"Source code in tabular/modules/table/__init__.py def render__table__as__string ( self , value : Value , render_config : Mapping [ str , Any ]): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_string ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , )","title":"render__table__as__string()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.RenderTableModule.render__table__as__terminal_renderable","text":"Source code in tabular/modules/table/__init__.py def render__table__as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ): input_number_of_rows = render_config . get ( \"number_of_rows\" , 20 ) input_row_offset = render_config . get ( \"row_offset\" , 0 ) wrap , data_related_scenes = self . preprocess_table ( value = value , input_number_of_rows = input_number_of_rows , input_row_offset = input_row_offset , ) pretty = wrap . as_terminal_renderable ( max_row_height = 1 ) return RenderValueResult ( value_id = value . value_id , render_config = render_config , render_manifest = self . manifest . manifest_hash , rendered = pretty , related_scenes = data_related_scenes , )","title":"render__table__as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.RenderTableModuleBase","text":"Source code in tabular/modules/table/__init__.py class RenderTableModuleBase ( RenderValueModule ): _module_type_name : str = None # type: ignore def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes","title":"RenderTableModuleBase"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.RenderTableModuleBase.preprocess_table","text":"Source code in tabular/modules/table/__init__.py def preprocess_table ( self , value : Value , input_number_of_rows : int , input_row_offset : int ): import duckdb import pyarrow as pa if value . data_type_name == \"array\" : array : KiaraArray = value . data arrow_table = pa . table ( data = [ array . arrow_array ], names = [ \"array\" ]) column_names : Iterable [ str ] = [ \"array\" ] else : table : KiaraTable = value . data arrow_table = table . arrow_table column_names = table . column_names columnns = [ f '\" { x } \"' if not x . startswith ( '\"' ) else x for x in column_names ] query = f \"\"\"SELECT { ', ' . join ( columnns ) } FROM data LIMIT { input_number_of_rows } OFFSET { input_row_offset } \"\"\" rel_from_arrow = duckdb . arrow ( arrow_table ) query_result : duckdb . DuckDBPyRelation = rel_from_arrow . query ( \"data\" , query ) result_table = query_result . arrow () wrap = ArrowTabularWrap ( table = result_table ) related_scenes : Dict [ str , Union [ None , RenderScene ]] = {} row_offset = arrow_table . num_rows - input_number_of_rows if row_offset > 0 : if input_row_offset > 0 : related_scenes [ \"first\" ] = RenderScene . construct ( title = \"first\" , description = f \"Display the first { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : 0 , \"number_of_rows\" : input_number_of_rows , }, ) p_offset = input_row_offset - input_number_of_rows if p_offset < 0 : p_offset = 0 previous = { \"row_offset\" : p_offset , \"number_of_rows\" : input_number_of_rows , } related_scenes [ \"previous\" ] = RenderScene . construct ( title = \"previous\" , description = f \"Display the previous { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = previous ) # type: ignore else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None n_offset = input_row_offset + input_number_of_rows if n_offset < arrow_table . num_rows : next = { \"row_offset\" : n_offset , \"number_of_rows\" : input_number_of_rows } related_scenes [ \"next\" ] = RenderScene . construct ( title = \"next\" , description = f \"Display the next { input_number_of_rows } rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = next ) # type: ignore else : related_scenes [ \"next\" ] = None last_page = int ( arrow_table . num_rows / input_number_of_rows ) current_start = last_page * input_number_of_rows if ( input_row_offset + input_number_of_rows ) > arrow_table . num_rows : related_scenes [ \"last\" ] = None else : related_scenes [ \"last\" ] = RenderScene . construct ( title = \"last\" , description = \"Display the final rows of this table.\" , manifest_hash = self . manifest . manifest_hash , render_config = { \"row_offset\" : current_start , # type: ignore \"number_of_rows\" : input_number_of_rows , # type: ignore }, ) else : related_scenes [ \"first\" ] = None related_scenes [ \"previous\" ] = None related_scenes [ \"next\" ] = None related_scenes [ \"last\" ] = None return wrap , related_scenes","title":"preprocess_table()"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.filters","text":"","title":"filters"},{"location":"reference/kiara_plugin/tabular/modules/table/__init__/#kiara_plugin.tabular.modules.table.filters.TableFiltersModule","text":"Source code in tabular/modules/table/filters.py class TableFiltersModule ( FilterModule ): _module_type_name = \"table.filters\" @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\" def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass create_filter_inputs ( self , filter_name ) \u00b6 Source code in tabular/modules/table/filters.py def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None filter__drop_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_rows ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass retrieve_supported_type () classmethod \u00b6 Source code in tabular/modules/table/filters.py @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\"","title":"TableFiltersModule"},{"location":"reference/kiara_plugin/tabular/modules/table/filters/","text":"TableFiltersModule ( FilterModule ) \u00b6 Source code in tabular/modules/table/filters.py class TableFiltersModule ( FilterModule ): _module_type_name = \"table.filters\" @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\" def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass create_filter_inputs ( self , filter_name ) \u00b6 Source code in tabular/modules/table/filters.py def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None filter__drop_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_columns ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) filter__select_rows ( self , value , filter_inputs ) \u00b6 Source code in tabular/modules/table/filters.py def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass retrieve_supported_type () classmethod \u00b6 Source code in tabular/modules/table/filters.py @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\"","title":"filters"},{"location":"reference/kiara_plugin/tabular/modules/table/filters/#kiara_plugin.tabular.modules.table.filters.TableFiltersModule","text":"Source code in tabular/modules/table/filters.py class TableFiltersModule ( FilterModule ): _module_type_name = \"table.filters\" @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\" def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names ) def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass","title":"TableFiltersModule"},{"location":"reference/kiara_plugin/tabular/modules/table/filters/#kiara_plugin.tabular.modules.table.filters.TableFiltersModule.create_filter_inputs","text":"Source code in tabular/modules/table/filters.py def create_filter_inputs ( self , filter_name : str ) -> Union [ None , ValueMapSchema ]: if filter_name in [ \"select_columns\" , \"drop_columns\" ]: return { \"columns\" : { \"type\" : \"list\" , \"doc\" : \"The name of the columns to include.\" , \"optional\" : True , }, \"ignore_invalid_column_names\" : { \"type\" : \"boolean\" , \"doc\" : \"Whether to ignore invalid column names.\" , \"default\" : True , }, } return None","title":"create_filter_inputs()"},{"location":"reference/kiara_plugin/tabular/modules/table/filters/#kiara_plugin.tabular.modules.table.filters.TableFiltersModule.filter__drop_columns","text":"Source code in tabular/modules/table/filters.py def filter__drop_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names_to_ignore = filter_inputs [ \"columns\" ] if not column_names_to_ignore : return value table : KiaraTable = value . data arrow_table = table . arrow_table for column_name in column_names_to_ignore : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) _column_names = [] _columns = [] for column_name in arrow_table . column_names : if column_name in column_names_to_ignore : continue column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names )","title":"filter__drop_columns()"},{"location":"reference/kiara_plugin/tabular/modules/table/filters/#kiara_plugin.tabular.modules.table.filters.TableFiltersModule.filter__select_columns","text":"Source code in tabular/modules/table/filters.py def filter__select_columns ( self , value : Value , filter_inputs : Mapping [ str , Any ]): import pyarrow as pa ignore_invalid = filter_inputs [ \"ignore_invalid_column_names\" ] column_names = filter_inputs [ \"columns\" ] if not column_names : return value table : KiaraTable = value . data arrow_table = table . arrow_table _column_names = [] _columns = [] for column_name in column_names : if column_name not in arrow_table . column_names : if ignore_invalid : continue else : raise KiaraProcessingException ( f \"Can't select column ' { column_name } ' from table: column name not available. Available columns: { ', ' . join ( arrow_table . column_names ) } .\" ) column = arrow_table . column ( column_name ) _column_names . append ( column_name ) _columns . append ( column ) return pa . table ( data = _columns , names = _column_names )","title":"filter__select_columns()"},{"location":"reference/kiara_plugin/tabular/modules/table/filters/#kiara_plugin.tabular.modules.table.filters.TableFiltersModule.filter__select_rows","text":"Source code in tabular/modules/table/filters.py def filter__select_rows ( self , value : Value , filter_inputs : Mapping [ str , Any ]): pass","title":"filter__select_rows()"},{"location":"reference/kiara_plugin/tabular/modules/table/filters/#kiara_plugin.tabular.modules.table.filters.TableFiltersModule.retrieve_supported_type","text":"Source code in tabular/modules/table/filters.py @classmethod def retrieve_supported_type ( cls ) -> Union [ Dict [ str , Any ], str ]: return \"table\"","title":"retrieve_supported_type()"},{"location":"reference/kiara_plugin/tabular/pipelines/__init__/","text":"Default (empty) module that is used as a base path for pipelines contained in this package.","title":"pipelines"}]}