{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"kiara plugin: tabular \u00b6 This package contains a set of commonly used/useful modules, pipelines, types and metadata schemas for Kiara . Description \u00b6 kiara data-types and modules for working with tables and databases. Package content \u00b6 data_types \u00b6 database : A database, containing one or several tables. array : An array, in most cases used as a column within a table. table : Tabular data (table, spreadsheet, data_frame, what have you). module_types \u00b6 database.save_to.disk : -- n/a -- table.save_to.disk.as.feather : -- n/a -- database.create : -- n/a -- table.create : -- n/a -- database.load_from.disk : -- n/a -- table.load_from.disk : -- n/a -- table.cut_column : Cut off one column from a table, returning an array. metadata_types \u00b6 table : File stats. operations \u00b6 create.array.from.table : Cut off one column from a table, returning an array. create.database.from.csv_file : -- n/a -- create.database.from.csv_file_bundle : -- n/a -- create.load_config.from.array : -- n/a -- create.load_config.from.database : -- n/a -- create.load_config.from.table : Store the table as Apache Arrow feather file create.table.from.csv_file : -- n/a -- create.table.from.csv_file_bundle : -- n/a -- database.load_from.disk : -- n/a -- save.array.to.disk.as.arrays : -- n/a -- save.database.to.disk.as.arrays : -- n/a -- save.table.to.disk.as.arrays : Store the table as Apache Arrow feather file table.cut_column : Cut off one column from a table, returning an array. table.load_from.disk : -- n/a -- Links \u00b6 Documentation: https://DHARPA-Project.github.io/kiara_plugin.tabular Code: https://github.com/DHARPA-Project/kiara_plugin.tabular","title":"Home"},{"location":"#kiara-plugin-tabular","text":"This package contains a set of commonly used/useful modules, pipelines, types and metadata schemas for Kiara .","title":"kiara plugin: tabular"},{"location":"#description","text":"kiara data-types and modules for working with tables and databases.","title":"Description"},{"location":"#package-content","text":"","title":"Package content"},{"location":"#data_types","text":"database : A database, containing one or several tables. array : An array, in most cases used as a column within a table. table : Tabular data (table, spreadsheet, data_frame, what have you).","title":"data_types"},{"location":"#module_types","text":"database.save_to.disk : -- n/a -- table.save_to.disk.as.feather : -- n/a -- database.create : -- n/a -- table.create : -- n/a -- database.load_from.disk : -- n/a -- table.load_from.disk : -- n/a -- table.cut_column : Cut off one column from a table, returning an array.","title":"module_types"},{"location":"#metadata_types","text":"table : File stats.","title":"metadata_types"},{"location":"#operations","text":"create.array.from.table : Cut off one column from a table, returning an array. create.database.from.csv_file : -- n/a -- create.database.from.csv_file_bundle : -- n/a -- create.load_config.from.array : -- n/a -- create.load_config.from.database : -- n/a -- create.load_config.from.table : Store the table as Apache Arrow feather file create.table.from.csv_file : -- n/a -- create.table.from.csv_file_bundle : -- n/a -- database.load_from.disk : -- n/a -- save.array.to.disk.as.arrays : -- n/a -- save.database.to.disk.as.arrays : -- n/a -- save.table.to.disk.as.arrays : Store the table as Apache Arrow feather file table.cut_column : Cut off one column from a table, returning an array. table.load_from.disk : -- n/a --","title":"operations"},{"location":"#links","text":"Documentation: https://DHARPA-Project.github.io/kiara_plugin.tabular Code: https://github.com/DHARPA-Project/kiara_plugin.tabular","title":"Links"},{"location":"SUMMARY/","text":"Home Usage Package contents API reference","title":"SUMMARY"},{"location":"usage/","text":"Usage \u00b6 TO BE DONE","title":"Usage"},{"location":"usage/#usage","text":"TO BE DONE","title":"Usage"},{"location":"info/SUMMARY/","text":"data_types module_types metadata_types operations","title":"SUMMARY"},{"location":"info/data_types/","text":"database \u00b6 lineage database any qualifier profile(s) -- n/a -- Documentation A database, containing one or several tables. This is backed by a sqlite database file. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name DatabaseType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraDatabase python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.\u2026 array \u00b6 lineage array any qualifier profile(s) -- n/a -- Documentation An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name ArrayType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraArray python_module_name kiara_plugin.tabular.models.tab\u2026 full_name kiara_plugin.tabular.models.tab\u2026 table \u00b6 lineage table any qualifier profile(s) -- n/a -- Documentation Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the Apache Arrow Table class. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name TableType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name Table python_module_name pyarrow.lib full_name pyarrow.lib.Table","title":"data_types"},{"location":"info/data_types/#kiara_info.data_types.database","text":"lineage database any qualifier profile(s) -- n/a -- Documentation A database, containing one or several tables. This is backed by a sqlite database file. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name DatabaseType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraDatabase python_module_name kiara_plugin.tabular.models.db full_name kiara_plugin.tabular.models.db.\u2026","title":"database"},{"location":"info/data_types/#kiara_info.data_types.array","text":"lineage array any qualifier profile(s) -- n/a -- Documentation An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name ArrayType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name KiaraArray python_module_name kiara_plugin.tabular.models.tab\u2026 full_name kiara_plugin.tabular.models.tab\u2026","title":"array"},{"location":"info/data_types/#kiara_info.data_types.table","text":"lineage table any qualifier profile(s) -- n/a -- Documentation Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the Apache Arrow Table class. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara\u2026 documentation : https://DHARPA-Project.github.io/kiara_\u2026 Python class python_class_name TableType python_module_name kiara_plugin.tabular.data_types\u2026 full_name kiara_plugin.tabular.data_types\u2026 Config class python_class_name DataTypeConfig python_module_name kiara.data_types full_name kiara.data_types.DataTypeConfig Value class python_class_name Table python_module_name pyarrow.lib full_name pyarrow.lib.Table","title":"table"},{"location":"info/metadata_types/","text":"table \u00b6 Documentation File stats. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plug\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugi\u2026 Python class python_class_name KiaraTableMetadata python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.Ki\u2026 metadata_schema { \"title\" : \"KiaraTableMetadata\" , \"description\" : \"File stats.\" , \"type\" : \"object\" , \"properties\" : { \"table\" : { \"title\" : \"Table\" , \"description\" : \"The table schema.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/TableMetadata\" } ] } }, \"required\" : [ \"table\" ], \"definitions\" : { \"ColumnSchema\" : { \"title\" : \"ColumnSchema\" , \"description\" : \"Describes properties of a single column\u2026 \"type\" : \"object\" , \"properties\" : { \"type_name\" : { \"title\" : \"Type Name\" , \"description\" : \"The type name of the column (backen\u2026 \"type\" : \"string\" }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Other metadata for the column.\" , \"type\" : \"object\" } }, \"required\" : [ \"type_name\" ] }, \"TableMetadata\" : { \"title\" : \"TableMetadata\" , \"description\" : \"Describes properties for the 'table' da\u2026 \"type\" : \"object\" , \"properties\" : { \"column_names\" : { \"title\" : \"Column Names\" , \"description\" : \"The name of the columns of the tabl\u2026 \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"column_schema\" : { \"title\" : \"Column Schema\" , \"description\" : \"The schema description of the table\u2026 \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/ColumnSchema\" } }, \"rows\" : { \"title\" : \"Rows\" , \"description\" : \"The number of rows the table contai\u2026 \"type\" : \"integer\" }, \"size\" : { \"title\" : \"Size\" , \"description\" : \"The tables size in bytes.\" , \"type\" : \"integer\" } }, \"required\" : [ \"column_names\" , \"column_schema\" , \"rows\" ] } } }","title":"metadata_types"},{"location":"info/metadata_types/#kiara_info.metadata_types.table","text":"Documentation File stats. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_plug\u2026 documentation : https://DHARPA-Project.github.io/kiara_plugi\u2026 Python class python_class_name KiaraTableMetadata python_module_name kiara_plugin.tabular.models.table full_name kiara_plugin.tabular.models.table.Ki\u2026 metadata_schema { \"title\" : \"KiaraTableMetadata\" , \"description\" : \"File stats.\" , \"type\" : \"object\" , \"properties\" : { \"table\" : { \"title\" : \"Table\" , \"description\" : \"The table schema.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/TableMetadata\" } ] } }, \"required\" : [ \"table\" ], \"definitions\" : { \"ColumnSchema\" : { \"title\" : \"ColumnSchema\" , \"description\" : \"Describes properties of a single column\u2026 \"type\" : \"object\" , \"properties\" : { \"type_name\" : { \"title\" : \"Type Name\" , \"description\" : \"The type name of the column (backen\u2026 \"type\" : \"string\" }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Other metadata for the column.\" , \"type\" : \"object\" } }, \"required\" : [ \"type_name\" ] }, \"TableMetadata\" : { \"title\" : \"TableMetadata\" , \"description\" : \"Describes properties for the 'table' da\u2026 \"type\" : \"object\" , \"properties\" : { \"column_names\" : { \"title\" : \"Column Names\" , \"description\" : \"The name of the columns of the tabl\u2026 \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"column_schema\" : { \"title\" : \"Column Schema\" , \"description\" : \"The schema description of the table\u2026 \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/ColumnSchema\" } }, \"rows\" : { \"title\" : \"Rows\" , \"description\" : \"The number of rows the table contai\u2026 \"type\" : \"integer\" }, \"size\" : { \"title\" : \"Size\" , \"description\" : \"The tables size in bytes.\" , \"type\" : \"integer\" } }, \"required\" : [ \"column_names\" , \"column_schema\" , \"rows\" ] } } }","title":"table"},{"location":"info/module_types/","text":"database.save_to.disk \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. source_t\u2026 string The value yes type of the source. source_t\u2026 object The value no type config (if applicab\u2026 Python class python_class_name SaveDatabaseModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 value = inputs . get_value_obj(source_type) func_name = f\"data_type__{ self . get_config_valu\u2026 func = getattr(self, func_name) result: LoadConfig bytes_structure: Optional[BytesStructure] result, bytes_structure = func(value = value, pe\u2026 outputs . set_values(load_config = result, bytes_s\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table.save_to.disk.as.feather \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. source_t\u2026 string The value yes type of the source. source_t\u2026 object The value no type config (if applicab\u2026 Python class python_class_name SaveTableToDiskModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 value = inputs . get_value_obj(source_type) func_name = f\"data_type__{ self . get_config_valu\u2026 func = getattr(self, func_name) result: LoadConfig bytes_structure: Optional[BytesStructure] result, bytes_structure = func(value = value, pe\u2026 outputs . set_values(load_config = result, bytes_s\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database.create \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descrip\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constan\u2026 for this module. defaults object Value no defaults for this module. ignore_e\u2026 boolean Whether no false to ignore convert errors and omit the failed items. include_\u2026 boolean When no false includi\u2026 source metadat\u2026 whether to also include the original raw (string) content. include_\u2026 boolean Whether no true to include a table with metadata about the source files. merge_in\u2026 boolean Whether no false to merge all csv files into a single table. source_t\u2026 string The yes value type of the source value. target_t\u2026 string The yes value type of the target. Python class python_class_name CreateDatabaseModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 func_name = f\"create__{ target_type }__from__{ so\u2026 func = getattr(self, func_name) source_value = inputs . get_value_obj(source_typ\u2026 result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table.create \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descrip\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constan\u2026 for this module. defaults object Value no defaults for this module. ignore_e\u2026 boolean Whether no false to ignore convert errors and omit the failed items. source_t\u2026 string The yes value type of the source value. target_t\u2026 string The yes value type of the target. Python class python_class_name CreateTableModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 func_name = f\"create__{ target_type }__from__{ so\u2026 func = getattr(self, func_name) source_value = inputs . get_value_obj(source_typ\u2026 result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database.load_from.disk \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. Python class python_class_name LoadDatabaseFromDiskModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 bytes_structure: BytesStructure = inputs . get_v\u2026 db_file = bytes_structure . chunk_map[ \"db.sqlite\u2026 assert len(db_file) == 1 db = KiaraDatabase(db_file_path = db_file[ 0 ]) db . _immutable = True outputs . set_value( \"database\" , db) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table.load_from.disk \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. only_col\u2026 string Whether no to only load a single column instead of the whole table. Python class python_class_name LoadTableFromDiskModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import pyarrow as pa bytes_structure: BytesStructure = inputs . get_v\u2026 if not self . get_config_value( \"only_column\" ): columns = {} for column_name, chunks in bytes_structure \u2026 assert len(chunks) == 1 with pa . memory_map(chunks[ 0 ], \"r\" ) as \u2026 loaded_arrays: pa . Table = pa . ipc . o\u2026 column = loaded_arrays . column(colu\u2026 if column_name == EMPTY_COLUMN_NAM\u2026 columns[ \"\" ] = column else : columns[column_name] = column arrow_table = pa . table(columns) table = KiaraTable . create_table(arrow_tabl\u2026 outputs . set_value( \"table\" , table) else : chunks = bytes_structure . chunk_map[ \"array.\u2026 assert len(chunks) == 1 array_file = chunks[ 0 ] # with pa.memory_map(array_file, \"r\") as c\u2026 # loaded_arrays = pa.ipc.open_file(col\u2026 # column = loaded_arrays.column(\"array\u2026 # # array = KiaraArray.create_array(column) array = KiaraArray(data_path = array_file) outputs . set_value( \"array\" , array) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table.cut_column \u00b6 Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. Python class python_class_name CutColumnModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import pyarrow as pa column_name: str = inputs . get_value_data( \"colu\u2026 table_value: Value = inputs . get_value_obj( \"tab\u2026 table_metadata: KiaraTableMetadata = table_val\u2026 \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available: raise KiaraProcessingException( f\"Invalid column name '{ column_name }'.\u2026 ) table: pa . Table = table_value . data . arrow_table column = table . column(column_name) outputs . set_value( \"array\" , column) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"module_types"},{"location":"info/module_types/#kiara_info.module_types.database.save_to.disk","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. source_t\u2026 string The value yes type of the source. source_t\u2026 object The value no type config (if applicab\u2026 Python class python_class_name SaveDatabaseModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 value = inputs . get_value_obj(source_type) func_name = f\"data_type__{ self . get_config_valu\u2026 func = getattr(self, func_name) result: LoadConfig bytes_structure: Optional[BytesStructure] result, bytes_structure = func(value = value, pe\u2026 outputs . set_values(load_config = result, bytes_s\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"database.save_to.disk"},{"location":"info/module_types/#kiara_info.module_types.table.save_to.disk.as.feather","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. source_t\u2026 string The value yes type of the source. source_t\u2026 object The value no type config (if applicab\u2026 Python class python_class_name SaveTableToDiskModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 value = inputs . get_value_obj(source_type) func_name = f\"data_type__{ self . get_config_valu\u2026 func = getattr(self, func_name) result: LoadConfig bytes_structure: Optional[BytesStructure] result, bytes_structure = func(value = value, pe\u2026 outputs . set_values(load_config = result, bytes_s\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"table.save_to.disk.as.feather"},{"location":"info/module_types/#kiara_info.module_types.database.create","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descrip\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constan\u2026 for this module. defaults object Value no defaults for this module. ignore_e\u2026 boolean Whether no false to ignore convert errors and omit the failed items. include_\u2026 boolean When no false includi\u2026 source metadat\u2026 whether to also include the original raw (string) content. include_\u2026 boolean Whether no true to include a table with metadata about the source files. merge_in\u2026 boolean Whether no false to merge all csv files into a single table. source_t\u2026 string The yes value type of the source value. target_t\u2026 string The yes value type of the target. Python class python_class_name CreateDatabaseModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 func_name = f\"create__{ target_type }__from__{ so\u2026 func = getattr(self, func_name) source_value = inputs . get_value_obj(source_typ\u2026 result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"database.create"},{"location":"info/module_types/#kiara_info.module_types.table.create","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descrip\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constan\u2026 for this module. defaults object Value no defaults for this module. ignore_e\u2026 boolean Whether no false to ignore convert errors and omit the failed items. source_t\u2026 string The yes value type of the source value. target_t\u2026 string The yes value type of the target. Python class python_class_name CreateTableModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 source_type = self . get_config_value( \"source_ty\u2026 target_type = self . get_config_value( \"target_ty\u2026 func_name = f\"create__{ target_type }__from__{ so\u2026 func = getattr(self, func_name) source_value = inputs . get_value_obj(source_typ\u2026 result = func(source_value = source_value) outputs . set_value(target_type, result) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"table.create"},{"location":"info/module_types/#kiara_info.module_types.database.load_from.disk","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. Python class python_class_name LoadDatabaseFromDiskModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 bytes_structure: BytesStructure = inputs . get_v\u2026 db_file = bytes_structure . chunk_map[ \"db.sqlite\u2026 assert len(db_file) == 1 db = KiaraDatabase(db_file_path = db_file[ 0 ]) db . _immutable = True outputs . set_value( \"database\" , db) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"database.load_from.disk"},{"location":"info/module_types/#kiara_info.module_types.table.load_from.disk","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. only_col\u2026 string Whether no to only load a single column instead of the whole table. Python class python_class_name LoadTableFromDiskModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import pyarrow as pa bytes_structure: BytesStructure = inputs . get_v\u2026 if not self . get_config_value( \"only_column\" ): columns = {} for column_name, chunks in bytes_structure \u2026 assert len(chunks) == 1 with pa . memory_map(chunks[ 0 ], \"r\" ) as \u2026 loaded_arrays: pa . Table = pa . ipc . o\u2026 column = loaded_arrays . column(colu\u2026 if column_name == EMPTY_COLUMN_NAM\u2026 columns[ \"\" ] = column else : columns[column_name] = column arrow_table = pa . table(columns) table = KiaraTable . create_table(arrow_tabl\u2026 outputs . set_value( \"table\" , table) else : chunks = bytes_structure . chunk_map[ \"array.\u2026 assert len(chunks) == 1 array_file = chunks[ 0 ] # with pa.memory_map(array_file, \"r\") as c\u2026 # loaded_arrays = pa.ipc.open_file(col\u2026 # column = loaded_arrays.column(\"array\u2026 # # array = KiaraArray.create_array(column) array = KiaraArray(data_path = array_file) outputs . set_value( \"array\" , array) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"table.load_from.disk"},{"location":"info/module_types/#kiara_info.module_types.table.cut_column","text":"Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://DHARPA-Project.github.io/kiar\u2026 Module config schema Field Type Descript\u2026 Required Default \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value no constants for this module. defaults object Value no defaults for this module. Python class python_class_name CutColumnModule python_module_name kiara_plugin.tabular.tabular.\u2026 full_name kiara_plugin.tabular.tabular.\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueMap, outputs: Value\u2026 import pyarrow as pa column_name: str = inputs . get_value_data( \"colu\u2026 table_value: Value = inputs . get_value_obj( \"tab\u2026 table_metadata: KiaraTableMetadata = table_val\u2026 \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available: raise KiaraProcessingException( f\"Invalid column name '{ column_name }'.\u2026 ) table: pa . Table = table_value . data . arrow_table column = table . column(column_name) outputs . set_value( \"array\" , column) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"table.cut_column"},{"location":"info/operations/","text":"create.array.from.table \u00b6 Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Cut off one column from a table, returning an array. Inputs fie\u2026 type des\u2026 Requ\u2026 Def\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tab\u2026 table The yes -- sou\u2026 no val\u2026 def\u2026 -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The result value. Module type table.cut_column Module config { \"constants\" : {}, \"defaults\" : {} } Module metadata Cut off one column from a table, returning an array. Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CutColu\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 create.database.from.csv_file \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv\u2026 csv\u2026 The yes -- no sour\u2026 defa\u2026 valu\u2026 -- Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value. Module type database.create Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"csv_file\" , \"target_type\" : \"database\" , \"ignore_errors\" : false , \"merge_into_single_table\" : false , \"include_source_metadata\" : true , \"include_source_file_content\" : false } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CreateD\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 create.database.from.csv_file_bundle \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv\u2026 csv\u2026 The yes -- no sour\u2026 defa\u2026 valu\u2026 -- Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value. Module type database.create Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"csv_file_bundle\" , \"target_type\" : \"database\" , \"ignore_errors\" : false , \"merge_into_single_table\" : false , \"include_source_metadata\" : true , \"include_source_file_content\" : false } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CreateD\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 create.load_config.from.array \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type des\u2026 Requ\u2026 Def\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 arr\u2026 array The yes -- sou\u2026 no val\u2026 def\u2026 -- Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The result value. Module type table.save_to.disk.as.feather Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"array\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveTab\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 create.load_config.from.database \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dat\u2026 dat\u2026 The yes -- no sour\u2026 defa\u2026 valu\u2026 -- Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The result value. Module type database.save_to.disk Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"database\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveDat\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 create.load_config.from.table \u00b6 Documentation Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Inputs fie\u2026 type des\u2026 Requ\u2026 Def\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tab\u2026 table The yes -- sou\u2026 no val\u2026 def\u2026 -- Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The result value. Module type table.save_to.disk.as.feather Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"table\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveTab\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 create.table.from.csv_file \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv\u2026 csv\u2026 The yes -- no sour\u2026 defa\u2026 valu\u2026 -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value. Module type table.create Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"csv_file\" , \"target_type\" : \"table\" , \"ignore_errors\" : false } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CreateT\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 create.table.from.csv_file_bundle \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv\u2026 csv\u2026 The yes -- no sour\u2026 defa\u2026 valu\u2026 -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value. Module type table.create Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"csv_file_bundle\" , \"target_type\" : \"table\" , \"ignore_errors\" : false } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CreateT\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 database.load_from.disk \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 byt\u2026 any The yes -- no byte\u2026 defa\u2026 -- Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The database. Module type database.load_from.disk Module config { \"constants\" : {}, \"defaults\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 LoadDat\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 save.array.to.disk.as.arrays \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 val\u2026 any The yes -- no value defa\u2026 to -- pers\u2026 per\u2026 any (Opt\u2026 yes -- no conf\u2026 defa\u2026 for -- the pers\u2026 proc\u2026 Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The saved value details. bytes_st\u2026 any A structure of serialized bytes. Module type table.save_to.disk.as.feather Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"array\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveTab\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 save.database.to.disk.as.arrays \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 val\u2026 any The yes -- no value defa\u2026 to -- pers\u2026 per\u2026 any (Opt\u2026 yes -- no conf\u2026 defa\u2026 for -- the pers\u2026 proc\u2026 Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The saved value details. bytes_st\u2026 any A structure of serialized bytes. Module type database.save_to.disk Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"database\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveDat\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 save.table.to.disk.as.arrays \u00b6 Documentation Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 val\u2026 any The yes -- no value defa\u2026 to -- pers\u2026 per\u2026 any (Opt\u2026 yes -- no conf\u2026 defa\u2026 for -- the pers\u2026 proc\u2026 Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The saved value details. bytes_st\u2026 any A structure of serialized bytes. Module type table.save_to.disk.as.feather Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"table\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveTab\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 table.cut_column \u00b6 Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Cut off one column from a table, returning an array. Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tab\u2026 tab\u2026 A yes -- no tabl\u2026 defa\u2026 -- col\u2026 str\u2026 The yes -- no name defa\u2026 of -- the colu\u2026 to extr\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The column. Module type table.cut_column Module config { \"constants\" : {}, \"defaults\" : {} } Module metadata Cut off one column from a table, returning an array. Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CutColu\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026 table.load_from.disk \u00b6 Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 byt\u2026 any The yes -- no byte\u2026 defa\u2026 -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The table. Module type table.load_from.disk Module config { \"constants\" : {}, \"defaults\" : {}, \"only_column\" : null } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 LoadTab\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"operations"},{"location":"info/operations/#kiara_info.operations.create.array.from.table","text":"Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Cut off one column from a table, returning an array. Inputs fie\u2026 type des\u2026 Requ\u2026 Def\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tab\u2026 table The yes -- sou\u2026 no val\u2026 def\u2026 -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The result value. Module type table.cut_column Module config { \"constants\" : {}, \"defaults\" : {} } Module metadata Cut off one column from a table, returning an array. Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CutColu\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"create.array.from.table"},{"location":"info/operations/#kiara_info.operations.create.database.from.csv_file","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv\u2026 csv\u2026 The yes -- no sour\u2026 defa\u2026 valu\u2026 -- Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value. Module type database.create Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"csv_file\" , \"target_type\" : \"database\" , \"ignore_errors\" : false , \"merge_into_single_table\" : false , \"include_source_metadata\" : true , \"include_source_file_content\" : false } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CreateD\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"create.database.from.csv_file"},{"location":"info/operations/#kiara_info.operations.create.database.from.csv_file_bundle","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv\u2026 csv\u2026 The yes -- no sour\u2026 defa\u2026 valu\u2026 -- Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The result value. Module type database.create Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"csv_file_bundle\" , \"target_type\" : \"database\" , \"ignore_errors\" : false , \"merge_into_single_table\" : false , \"include_source_metadata\" : true , \"include_source_file_content\" : false } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CreateD\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"create.database.from.csv_file_bundle"},{"location":"info/operations/#kiara_info.operations.create.load_config.from.array","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type des\u2026 Requ\u2026 Def\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 arr\u2026 array The yes -- sou\u2026 no val\u2026 def\u2026 -- Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The result value. Module type table.save_to.disk.as.feather Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"array\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveTab\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"create.load_config.from.array"},{"location":"info/operations/#kiara_info.operations.create.load_config.from.database","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 dat\u2026 dat\u2026 The yes -- no sour\u2026 defa\u2026 valu\u2026 -- Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The result value. Module type database.save_to.disk Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"database\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveDat\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"create.load_config.from.database"},{"location":"info/operations/#kiara_info.operations.create.load_config.from.table","text":"Documentation Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Inputs fie\u2026 type des\u2026 Requ\u2026 Def\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tab\u2026 table The yes -- sou\u2026 no val\u2026 def\u2026 -- Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The result value. Module type table.save_to.disk.as.feather Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"table\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveTab\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"create.load_config.from.table"},{"location":"info/operations/#kiara_info.operations.create.table.from.csv_file","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv\u2026 csv\u2026 The yes -- no sour\u2026 defa\u2026 valu\u2026 -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value. Module type table.create Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"csv_file\" , \"target_type\" : \"table\" , \"ignore_errors\" : false } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CreateT\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"create.table.from.csv_file"},{"location":"info/operations/#kiara_info.operations.create.table.from.csv_file_bundle","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 csv\u2026 csv\u2026 The yes -- no sour\u2026 defa\u2026 valu\u2026 -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The result value. Module type table.create Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"csv_file_bundle\" , \"target_type\" : \"table\" , \"ignore_errors\" : false } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CreateT\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"create.table.from.csv_file_bundle"},{"location":"info/operations/#kiara_info.operations.database.load_from.disk","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 byt\u2026 any The yes -- no byte\u2026 defa\u2026 -- Outputs field name type descripti\u2026 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 database database The database. Module type database.load_from.disk Module config { \"constants\" : {}, \"defaults\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 LoadDat\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"database.load_from.disk"},{"location":"info/operations/#kiara_info.operations.save.array.to.disk.as.arrays","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 val\u2026 any The yes -- no value defa\u2026 to -- pers\u2026 per\u2026 any (Opt\u2026 yes -- no conf\u2026 defa\u2026 for -- the pers\u2026 proc\u2026 Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The saved value details. bytes_st\u2026 any A structure of serialized bytes. Module type table.save_to.disk.as.feather Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"array\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveTab\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"save.array.to.disk.as.arrays"},{"location":"info/operations/#kiara_info.operations.save.database.to.disk.as.arrays","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 val\u2026 any The yes -- no value defa\u2026 to -- pers\u2026 per\u2026 any (Opt\u2026 yes -- no conf\u2026 defa\u2026 for -- the pers\u2026 proc\u2026 Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The saved value details. bytes_st\u2026 any A structure of serialized bytes. Module type database.save_to.disk Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"database\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveDat\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"save.database.to.disk.as.arrays"},{"location":"info/operations/#kiara_info.operations.save.table.to.disk.as.arrays","text":"Documentation Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 val\u2026 any The yes -- no value defa\u2026 to -- pers\u2026 per\u2026 any (Opt\u2026 yes -- no conf\u2026 defa\u2026 for -- the pers\u2026 proc\u2026 Outputs field type descripti\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 load_con\u2026 load_con\u2026 The saved value details. bytes_st\u2026 any A structure of serialized bytes. Module type table.save_to.disk.as.feather Module config { \"constants\" : {}, \"defaults\" : {}, \"source_type\" : \"table\" , \"source_type_config\" : {} } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 SaveTab\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"save.table.to.disk.as.arrays"},{"location":"info/operations/#kiara_info.operations.table.cut_column","text":"Documentation Cut off one column from a table, returning an array. Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation Cut off one column from a table, returning an array. Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tab\u2026 tab\u2026 A yes -- no tabl\u2026 defa\u2026 -- col\u2026 str\u2026 The yes -- no name defa\u2026 of -- the colu\u2026 to extr\u2026 Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 array array The column. Module type table.cut_column Module config { \"constants\" : {}, \"defaults\" : {} } Module metadata Cut off one column from a table, returning an array. Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 CutColu\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"table.cut_column"},{"location":"info/operations/#kiara_info.operations.table.load_from.disk","text":"Documentation -- n/a -- Author(s) Markus Binsteiner markus@frkl.io Context Tags tabular Labels package : kiara_plugin.tabular References source_repo : https://github.com/DHARPA-Project/kiara_pl\u2026 documentation : https://DHARPA-Project.github.io/kiara_plu\u2026 Operation details Documentation -- n/a -- Inputs fie\u2026 type desc\u2026 Req\u2026 Defa\u2026 name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 byt\u2026 any The yes -- no byte\u2026 defa\u2026 -- Outputs field name type description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 table table The table. Module type table.load_from.disk Module config { \"constants\" : {}, \"defaults\" : {}, \"only_column\" : null } Module metadata -- n/a -- Author(s) Markus markus@\u2026 Binstei\u2026 Context Tags tabular Labels package : kiara_p\u2026 Referen\u2026 source_\u2026 https:/\u2026 documen\u2026 https:/\u2026 Python class python_\u2026 LoadTab\u2026 python_\u2026 kiara_p\u2026 full_na\u2026 kiara_p\u2026","title":"table.load_from.disk"},{"location":"reference/SUMMARY/","text":"kiara_plugin tabular data_types db table models db table pipelines tabular db table utils","title":"SUMMARY"},{"location":"reference/kiara_plugin/tabular/__init__/","text":"Top-level package for kiara_plugin.tabular. KIARA_METADATA \u00b6 find_data_types : Union [ Type , Tuple , Callable ] \u00b6 find_modules : Union [ Type , Tuple , Callable ] \u00b6 find_pipelines : Union [ Type , Tuple , Callable ] \u00b6 find_value_metadata : Union [ Type , Tuple , Callable ] \u00b6 get_version () \u00b6 Source code in tabular/__init__.py def get_version (): from pkg_resources import DistributionNotFound , get_distribution try : # Change here if project is renamed and does not equal the package name dist_name = __name__ __version__ = get_distribution ( dist_name ) . version except DistributionNotFound : try : version_file = os . path . join ( os . path . dirname ( __file__ ), \"version.txt\" ) if os . path . exists ( version_file ): with open ( version_file , encoding = \"utf-8\" ) as vf : __version__ = vf . read () else : __version__ = \"unknown\" except ( Exception ): pass if __version__ is None : __version__ = \"unknown\" return __version__ Modules \u00b6 data_types special \u00b6 This module contains the value type classes that are used in the kiara_plugin.tabular package. Modules \u00b6 db \u00b6 Classes \u00b6 DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/db.py def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/db.py def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) table \u00b6 Classes \u00b6 ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/table.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraArray render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the Apache Arrow Table class. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the [Apache Arrow](https://arrow.apache.org) [``Table``](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html) class. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return pa . Table def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return pa . Table render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result models special \u00b6 This module contains the metadata (and other) models that are used in the kiara_plugin.tabular package. Those models are convenience wrappers that make it easier for kiara to find, create, manage and version metadata -- but also other type of models -- that is attached to data, as well as kiara modules. Metadata models must be a sub-class of kiara.metadata.MetadataModel . Other models usually sub-class a pydantic BaseModel or implement custom base classes. Classes \u00b6 ColumnSchema ( BaseModel ) pydantic-model \u00b6 Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict ) Attributes \u00b6 metadata : Dict [ str , Any ] pydantic-field \u00b6 Other metadata for the column. type_name : str pydantic-field required \u00b6 The type name of the column (backend-specific). TableMetadata ( KiaraModel ) pydantic-model \u00b6 Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_id ( self ) -> str : return str ( self . model_data_hash ) def _retrieve_category_id ( self ) -> str : return \"instance.metadata.table\" def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , } Attributes \u00b6 column_names : List [ str ] pydantic-field required \u00b6 The name of the columns of the table. column_schema : Dict [ str , kiara_plugin . tabular . models . ColumnSchema ] pydantic-field required \u00b6 The schema description of the table. rows : int pydantic-field required \u00b6 The number of rows the table contains. size : int pydantic-field \u00b6 The tables size in bytes. Modules \u00b6 db \u00b6 Classes \u00b6 KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _table_schemas = PrivateAttr ( default = None ) _file_hash : Optional [ str ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return self . file_hash def _retrieve_category_id ( self ) -> str : return \"instance.database\" def _retrieve_data_to_hash ( self ) -> Any : return { \"file_hash\" : self . file_hash , } @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_hash ( self ) -> str : if self . _file_hash is not None : return self . _file_hash sha256_hash = hashlib . sha3_256 () with open ( self . db_file_path , \"rb\" ) as f : # Read and update hash string value in blocks of 4K for byte_block in iter ( lambda : f . read ( 4096 ), b \"\" ): sha256_hash . update ( byte_block ) self . _file_hash = sha256_hash . hexdigest () return self . _file_hash def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None self . _table_schemas = None self . _file_hash = None def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_hash : str property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () get_schema_for_table ( self , table_name ) \u00b6 Source code in tabular/models/db.py def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector table \u00b6 Classes \u00b6 KiaraArray ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return ARRAY_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist () KiaraTable ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return TABLE_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: return self . arrow_table . column_names @property def num_rows ( self ) -> int : return self . arrow_table . num_rows def to_pydict ( self ): return self . arrow_table . to_pydict () def to_pylist ( self ): return self . arrow_table . to_pylist () def to_pandas ( self ): return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 column_names : Iterable [ str ] property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 create_table ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Source code in tabular/models/table.py def to_pydict ( self ): return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] pipelines special \u00b6 Default (empty) module that is used as a base path for pipelines contained in this package. tabular special \u00b6 Modules \u00b6 db \u00b6 Classes \u00b6 CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"database.create\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. create__database__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/tabular/db.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/tabular/db.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( KiaraModule ) \u00b6 Source code in tabular/tabular/db.py class LoadDatabaseFromDiskModule ( KiaraModule ): _module_type_name = \"database.load_from.disk\" def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/db.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/db.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/db.py def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db ) SaveDatabaseModule ( PersistValueModule ) \u00b6 Source code in tabular/tabular/db.py class SaveDatabaseModule ( PersistValueModule ): _module_type_name = \"database.save_to.disk\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure data_type__database ( self , value , persistence_config ) \u00b6 Source code in tabular/tabular/db.py def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure get_persistence_format_name ( self ) \u00b6 Source code in tabular/tabular/db.py def get_persistence_format_name ( self ) -> str : return \"arrays\" get_persistence_target_name ( self ) \u00b6 Source code in tabular/tabular/db.py def get_persistence_target_name ( self ) -> str : return \"disk\" table \u00b6 EMPTY_COLUMN_NAME_MARKER \u00b6 Classes \u00b6 CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/tabular/table.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"table.create\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. create__table__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/tabular/table.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__table__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/tabular/table.py def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/tabular/table.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) LoadTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , ) Attributes \u00b6 only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table. LoadTableFromDiskModule ( KiaraModule ) \u00b6 Source code in tabular/tabular/table.py class LoadTableFromDiskModule ( KiaraModule ): _module_type_name = \"table.load_from.disk\" _config_cls = LoadTableConfig def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , ) Attributes \u00b6 only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array ) SaveTableToDiskModule ( PersistValueModule ) \u00b6 Source code in tabular/tabular/table.py class SaveTableToDiskModule ( PersistValueModule ): _module_type_name = \"table.save_to.disk.as.feather\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def _store_array ( self , array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch ) Methods \u00b6 data_type__array ( self , value , persistence_config ) \u00b6 Source code in tabular/tabular/table.py def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure data_type__table ( self , value , persistence_config ) \u00b6 Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Source code in tabular/tabular/table.py def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure get_persistence_format_name ( self ) \u00b6 Source code in tabular/tabular/table.py def get_persistence_format_name ( self ) -> str : return \"arrays\" get_persistence_target_name ( self ) \u00b6 Source code in tabular/tabular/table.py def get_persistence_target_name ( self ) -> str : return \"disk\" utils \u00b6 create_sqlite_table_from_tabular_file ( target_db_file , file_item , table_name = None , is_csv = True , is_tsv = False , is_nl = False , primary_key_column_names = None , flatten_nested_json_objects = False , csv_delimiter = None , quotechar = None , sniff = True , no_headers = False , encoding = 'utf-8' , batch_size = 100 , detect_types = True ) \u00b6 Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : str = None , quotechar : str = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension with open ( file_item . path , \"rb\" ) as f : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , ) insert_db_table_from_file_bundle ( database , file_bundle , table_name = 'file_items' , include_content = True ) \u00b6 Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import ( Column , DateTime , Integer , MetaData , String , Table , Text , insert , ) from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"import_time\" , DateTime (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"import_time\" : f . import_time , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"tabular"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.KIARA_METADATA","text":"","title":"KIARA_METADATA"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_data_types","text":"","title":"find_data_types"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_modules","text":"","title":"find_modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_pipelines","text":"","title":"find_pipelines"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.find_value_metadata","text":"","title":"find_value_metadata"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.get_version","text":"Source code in tabular/__init__.py def get_version (): from pkg_resources import DistributionNotFound , get_distribution try : # Change here if project is renamed and does not equal the package name dist_name = __name__ __version__ = get_distribution ( dist_name ) . version except DistributionNotFound : try : version_file = os . path . join ( os . path . dirname ( __file__ ), \"version.txt\" ) if os . path . exists ( version_file ): with open ( version_file , encoding = \"utf-8\" ) as vf : __version__ = vf . read () else : __version__ = \"unknown\" except ( Exception ): pass if __version__ is None : __version__ = \"unknown\" return __version__","title":"get_version()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types","text":"This module contains the value type classes that are used in the kiara_plugin.tabular package.","title":"data_types"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.db-classes","text":"DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/db.py def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/db.py def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.data_types.table-classes","text":"ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/table.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraArray render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the Apache Arrow Table class. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the [Apache Arrow](https://arrow.apache.org) [``Table``](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html) class. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return pa . Table def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return pa . Table render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models","text":"This module contains the metadata (and other) models that are used in the kiara_plugin.tabular package. Those models are convenience wrappers that make it easier for kiara to find, create, manage and version metadata -- but also other type of models -- that is attached to data, as well as kiara modules. Metadata models must be a sub-class of kiara.metadata.MetadataModel . Other models usually sub-class a pydantic BaseModel or implement custom base classes.","title":"models"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.ColumnSchema","text":"Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict )","title":"ColumnSchema"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.ColumnSchema-attributes","text":"metadata : Dict [ str , Any ] pydantic-field \u00b6 Other metadata for the column. type_name : str pydantic-field required \u00b6 The type name of the column (backend-specific).","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.TableMetadata","text":"Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_id ( self ) -> str : return str ( self . model_data_hash ) def _retrieve_category_id ( self ) -> str : return \"instance.metadata.table\" def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , }","title":"TableMetadata"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.TableMetadata-attributes","text":"column_names : List [ str ] pydantic-field required \u00b6 The name of the columns of the table. column_schema : Dict [ str , kiara_plugin . tabular . models . ColumnSchema ] pydantic-field required \u00b6 The schema description of the table. rows : int pydantic-field required \u00b6 The number of rows the table contains. size : int pydantic-field \u00b6 The tables size in bytes.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.db-classes","text":"KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _table_schemas = PrivateAttr ( default = None ) _file_hash : Optional [ str ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return self . file_hash def _retrieve_category_id ( self ) -> str : return \"instance.database\" def _retrieve_data_to_hash ( self ) -> Any : return { \"file_hash\" : self . file_hash , } @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_hash ( self ) -> str : if self . _file_hash is not None : return self . _file_hash sha256_hash = hashlib . sha3_256 () with open ( self . db_file_path , \"rb\" ) as f : # Read and update hash string value in blocks of 4K for byte_block in iter ( lambda : f . read ( 4096 ), b \"\" ): sha256_hash . update ( byte_block ) self . _file_hash = sha256_hash . hexdigest () return self . _file_hash def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None self . _table_schemas = None self . _file_hash = None def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_hash : str property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () get_schema_for_table ( self , table_name ) \u00b6 Source code in tabular/models/db.py def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.models.table-classes","text":"KiaraArray ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return ARRAY_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist () KiaraTable ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return TABLE_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: return self . arrow_table . column_names @property def num_rows ( self ) -> int : return self . arrow_table . num_rows def to_pydict ( self ): return self . arrow_table . to_pydict () def to_pylist ( self ): return self . arrow_table . to_pylist () def to_pandas ( self ): return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 column_names : Iterable [ str ] property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 create_table ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Source code in tabular/models/table.py def to_pydict ( self ): return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.pipelines","text":"Default (empty) module that is used as a base path for pipelines contained in this package.","title":"pipelines"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.tabular","text":"","title":"tabular"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.tabular-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.tabular.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.tabular.db-classes","text":"CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"database.create\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. create__database__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/tabular/db.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/tabular/db.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( KiaraModule ) \u00b6 Source code in tabular/tabular/db.py class LoadDatabaseFromDiskModule ( KiaraModule ): _module_type_name = \"database.load_from.disk\" def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/db.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/db.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/db.py def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db ) SaveDatabaseModule ( PersistValueModule ) \u00b6 Source code in tabular/tabular/db.py class SaveDatabaseModule ( PersistValueModule ): _module_type_name = \"database.save_to.disk\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure data_type__database ( self , value , persistence_config ) \u00b6 Source code in tabular/tabular/db.py def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure get_persistence_format_name ( self ) \u00b6 Source code in tabular/tabular/db.py def get_persistence_format_name ( self ) -> str : return \"arrays\" get_persistence_target_name ( self ) \u00b6 Source code in tabular/tabular/db.py def get_persistence_target_name ( self ) -> str : return \"disk\"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.tabular.table","text":"EMPTY_COLUMN_NAME_MARKER \u00b6","title":"table"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.tabular.table-classes","text":"CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/tabular/table.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"table.create\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. create__table__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/tabular/table.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__table__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/tabular/table.py def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/tabular/table.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) LoadTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , ) Attributes \u00b6 only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table. LoadTableFromDiskModule ( KiaraModule ) \u00b6 Source code in tabular/tabular/table.py class LoadTableFromDiskModule ( KiaraModule ): _module_type_name = \"table.load_from.disk\" _config_cls = LoadTableConfig def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , ) Attributes \u00b6 only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array ) SaveTableToDiskModule ( PersistValueModule ) \u00b6 Source code in tabular/tabular/table.py class SaveTableToDiskModule ( PersistValueModule ): _module_type_name = \"table.save_to.disk.as.feather\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def _store_array ( self , array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch ) Methods \u00b6 data_type__array ( self , value , persistence_config ) \u00b6 Source code in tabular/tabular/table.py def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure data_type__table ( self , value , persistence_config ) \u00b6 Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Source code in tabular/tabular/table.py def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure get_persistence_format_name ( self ) \u00b6 Source code in tabular/tabular/table.py def get_persistence_format_name ( self ) -> str : return \"arrays\" get_persistence_target_name ( self ) \u00b6 Source code in tabular/tabular/table.py def get_persistence_target_name ( self ) -> str : return \"disk\"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils","text":"","title":"utils"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.create_sqlite_table_from_tabular_file","text":"Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : str = None , quotechar : str = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension with open ( file_item . path , \"rb\" ) as f : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , )","title":"create_sqlite_table_from_tabular_file()"},{"location":"reference/kiara_plugin/tabular/__init__/#kiara_plugin.tabular.utils.insert_db_table_from_file_bundle","text":"Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import ( Column , DateTime , Integer , MetaData , String , Table , Text , insert , ) from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"import_time\" , DateTime (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"import_time\" : f . import_time , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"insert_db_table_from_file_bundle()"},{"location":"reference/kiara_plugin/tabular/utils/","text":"create_sqlite_table_from_tabular_file ( target_db_file , file_item , table_name = None , is_csv = True , is_tsv = False , is_nl = False , primary_key_column_names = None , flatten_nested_json_objects = False , csv_delimiter = None , quotechar = None , sniff = True , no_headers = False , encoding = 'utf-8' , batch_size = 100 , detect_types = True ) \u00b6 Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : str = None , quotechar : str = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension with open ( file_item . path , \"rb\" ) as f : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , ) insert_db_table_from_file_bundle ( database , file_bundle , table_name = 'file_items' , include_content = True ) \u00b6 Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import ( Column , DateTime , Integer , MetaData , String , Table , Text , insert , ) from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"import_time\" , DateTime (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"import_time\" : f . import_time , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"utils"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.create_sqlite_table_from_tabular_file","text":"Source code in tabular/utils.py def create_sqlite_table_from_tabular_file ( target_db_file : str , file_item : FileModel , table_name : Optional [ str ] = None , is_csv : bool = True , is_tsv : bool = False , is_nl : bool = False , primary_key_column_names : Optional [ Iterable [ str ]] = None , flatten_nested_json_objects : bool = False , csv_delimiter : str = None , quotechar : str = None , sniff : bool = True , no_headers : bool = False , encoding : str = \"utf-8\" , batch_size : int = 100 , detect_types : bool = True , ): if not table_name : table_name = file_item . file_name_without_extension with open ( file_item . path , \"rb\" ) as f : insert_upsert_implementation ( path = target_db_file , table = table_name , file = f , pk = primary_key_column_names , flatten = flatten_nested_json_objects , nl = is_nl , csv = is_csv , tsv = is_tsv , lines = False , text = False , convert = None , imports = None , delimiter = csv_delimiter , quotechar = quotechar , sniff = sniff , no_headers = no_headers , encoding = encoding , batch_size = batch_size , alter = False , upsert = False , ignore = False , replace = False , truncate = False , not_null = None , default = None , detect_types = detect_types , analyze = False , load_extension = None , silent = True , bulk_sql = None , )","title":"create_sqlite_table_from_tabular_file()"},{"location":"reference/kiara_plugin/tabular/utils/#kiara_plugin.tabular.utils.insert_db_table_from_file_bundle","text":"Source code in tabular/utils.py def insert_db_table_from_file_bundle ( database : KiaraDatabase , file_bundle : FileBundle , table_name : str = \"file_items\" , include_content : bool = True , ): # TODO: check if table with that name exists from sqlalchemy import ( Column , DateTime , Integer , MetaData , String , Table , Text , insert , ) from sqlalchemy.engine import Engine # if db_file_path is None: # temp_f = tempfile.mkdtemp() # db_file_path = os.path.join(temp_f, \"db.sqlite\") # # def cleanup(): # shutil.rmtree(db_file_path, ignore_errors=True) # # atexit.register(cleanup) metadata_obj = MetaData () file_items = Table ( table_name , metadata_obj , Column ( \"id\" , Integer , primary_key = True ), Column ( \"size\" , Integer (), nullable = False ), Column ( \"import_time\" , DateTime (), nullable = False ), Column ( \"mime_type\" , String ( length = 64 ), nullable = False ), Column ( \"rel_path\" , String (), nullable = False ), Column ( \"file_name\" , String (), nullable = False ), Column ( \"content\" , Text (), nullable = not include_content ), ) engine : Engine = database . get_sqlalchemy_engine () metadata_obj . create_all ( engine ) with engine . connect () as con : # TODO: commit in batches for better performance for index , rel_path in enumerate ( sorted ( file_bundle . included_files . keys ())): f : FileModel = file_bundle . included_files [ rel_path ] if not include_content : content : Optional [ str ] = f . read_text () # type: ignore else : content = None _values = { \"id\" : index , \"size\" : f . size , \"import_time\" : f . import_time , \"mime_type\" : f . mime_type , \"rel_path\" : rel_path , \"file_name\" : f . file_name , \"content\" : content , } stmt = insert ( file_items ) . values ( ** _values ) con . execute ( stmt ) con . commit ()","title":"insert_db_table_from_file_bundle()"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/","text":"This module contains the value type classes that are used in the kiara_plugin.tabular package. Modules \u00b6 db \u00b6 Classes \u00b6 DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/db.py def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/db.py def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) table \u00b6 Classes \u00b6 ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/table.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraArray render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the Apache Arrow Table class. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the [Apache Arrow](https://arrow.apache.org) [``Table``](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html) class. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return pa . Table def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return pa . Table render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"data_types"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db.DatabaseType","text":"A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"DatabaseType"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.db.DatabaseType-methods","text":"calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/db.py def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/db.py def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table.ArrayType","text":"An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/table.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result","title":"ArrayType"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table.ArrayType-methods","text":"calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraArray render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table.TableType","text":"Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the Apache Arrow Table class. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the [Apache Arrow](https://arrow.apache.org) [``Table``](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html) class. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return pa . Table def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"TableType"},{"location":"reference/kiara_plugin/tabular/data_types/__init__/#kiara_plugin.tabular.data_types.table.TableType-methods","text":"calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return pa . Table render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/db/","text":"Classes \u00b6 DatabaseType ( AnyType ) \u00b6 A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result ) Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/db.py def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/db.py def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data python_class () classmethod \u00b6 Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/db.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"db"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType","text":"A database, containing one or several tables. This is backed by a sqlite database file. Source code in tabular/data_types/db.py class DatabaseType ( AnyType [ KiaraDatabase , DataTypeConfig ]): \"\"\"A database, containing one or several tables. This is backed by a sqlite database file. \"\"\" _data_type_name = \"database\" @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash ) def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraDatabase )): raise ValueError ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraDatabase' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"DatabaseType"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.calculate_hash","text":"Calculate the hash of the value. Source code in tabular/data_types/db.py def calculate_hash ( self , data : KiaraDatabase ) -> int : return KIARA_HASH_FUNCTION ( data . file_hash )","title":"calculate_hash()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.calculate_size","text":"Source code in tabular/data_types/db.py def calculate_size ( self , data : KiaraDatabase ) -> int : file_stats = os . stat ( data . db_file_path ) size = file_stats . st_size return size","title":"calculate_size()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.parse_python_obj","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraDatabase 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/db.py def parse_python_obj ( self , data : Any ) -> KiaraDatabase : if isinstance ( data , Path ): data = data . as_posix () if isinstance ( data , str ): if not os . path . exists ( data ): raise ValueError ( f \"Can't create database from path ' { data } ': path does not exist.\" ) return KiaraDatabase ( db_file_path = data ) return data","title":"parse_python_obj()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.python_class","text":"Source code in tabular/data_types/db.py @classmethod def python_class ( self ) -> Type [ KiaraDatabase ]: return KiaraDatabase","title":"python_class()"},{"location":"reference/kiara_plugin/tabular/data_types/db/#kiara_plugin.tabular.data_types.db.DatabaseType.render_as__terminal_renderable","text":"Source code in tabular/data_types/db.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) db : KiaraDatabase = value . data result : List [ Any ] = [ \"\" ] for table_name in db . table_names : atw = SqliteTabularWrap ( engine = db . get_sqlalchemy_engine (), table_name = table_name ) pretty = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) result . append ( f \"[b]Table[/b]: [i] { table_name } [/i]\" ) result . append ( pretty ) return Group ( * result )","title":"render_as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/data_types/table/","text":"Classes \u00b6 ArrayType ( AnyType ) \u00b6 An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/table.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraArray render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result TableType ( AnyType ) \u00b6 Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the Apache Arrow Table class. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the [Apache Arrow](https://arrow.apache.org) [``Table``](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html) class. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return pa . Table def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result Methods \u00b6 calculate_hash ( self , data ) \u00b6 Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) calculate_size ( self , data ) \u00b6 Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) parse_python_obj ( self , data ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) python_class () classmethod \u00b6 Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return pa . Table render_as__terminal_renderable ( self , value , render_config ) \u00b6 Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"table"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.ArrayType","text":"An array, in most cases used as a column within a table. Internally, this type uses the Apache Arrow Array to store the data in memory (and on disk). Source code in tabular/data_types/table.py class ArrayType ( AnyType [ KiaraArray , DataTypeConfig ]): \"\"\"An array, in most cases used as a column within a table. Internally, this type uses the [Apache Arrow](https://arrow.apache.org) [Array](https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array) to store the data in memory (and on disk). \"\"\" _data_type_name = \"array\" @classmethod def python_class ( cls ) -> Type : return KiaraArray def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array ) def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data ) def _validate ( cls , value : Any ) -> None : if not isinstance ( value , ( KiaraArray )): raise Exception ( f \"Invalid type ' { type ( value ) . __name__ } ', must be an instance of the 'KiaraArray' class.\" ) def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result","title":"ArrayType"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.ArrayType-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.ArrayType.calculate_hash","text":"Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraArray ) -> int : hashes = [] for chunk in data . arrow_array . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array))","title":"calculate_hash()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.ArrayType.calculate_size","text":"Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraArray ) -> int : return len ( data . arrow_array )","title":"calculate_size()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.ArrayType.parse_python_obj","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraArray 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraArray : return KiaraArray . create_array ( data )","title":"parse_python_obj()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.ArrayType.python_class","text":"Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return KiaraArray","title":"python_class()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.ArrayType.render_as__terminal_renderable","text":"Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : Value , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) import pyarrow as pa array : pa . Array = value . data . arrow_array temp_table = pa . Table . from_arrays ( arrays = [ array ], names = [ \"array\" ]) atw = ArrowTabularWrap ( temp_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , show_table_header = False , ) return result","title":"render_as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType","text":"Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the Apache Arrow Table class. Source code in tabular/data_types/table.py class TableType ( AnyType [ KiaraTable , DataTypeConfig ]): \"\"\"Tabular data (table, spreadsheet, data_frame, what have you). Internally, this is backed by the [Apache Arrow](https://arrow.apache.org) [``Table``](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html) class. \"\"\" _data_type_name = \"table\" @classmethod def python_class ( cls ) -> Type : return pa . Table def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data ) def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array)) def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table ) def _validate ( cls , value : Any ) -> None : pass if not isinstance ( value , KiaraTable ): raise Exception ( f \"invalid type ' { type ( value ) . __name__ } ', must be 'KiaraTable'.\" ) def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"TableType"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.calculate_hash","text":"Calculate the hash of the value. Source code in tabular/data_types/table.py def calculate_hash ( self , data : KiaraTable ) -> int : hashes = [] for column_name in data . arrow_table . column_names : hashes . append ( column_name ) column = data . arrow_table . column ( column_name ) for chunk in column . chunks : for buf in chunk . buffers (): if not buf : continue h = hash_from_buffer ( memoryview ( buf )) hashes . append ( h ) return compute_hash ( hashes ) # return KIARA_HASH_FUNCTION(memoryview(data.arrow_array))","title":"calculate_hash()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.calculate_size","text":"Source code in tabular/data_types/table.py def calculate_size ( self , data : KiaraTable ) -> int : return len ( data . arrow_table )","title":"calculate_size()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.parse_python_obj","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description KiaraTable 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in tabular/data_types/table.py def parse_python_obj ( self , data : Any ) -> KiaraTable : return KiaraTable . create_table ( data )","title":"parse_python_obj()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.python_class","text":"Source code in tabular/data_types/table.py @classmethod def python_class ( cls ) -> Type : return pa . Table","title":"python_class()"},{"location":"reference/kiara_plugin/tabular/data_types/table/#kiara_plugin.tabular.data_types.table.TableType.render_as__terminal_renderable","text":"Source code in tabular/data_types/table.py def render_as__terminal_renderable ( self , value : \"Value\" , render_config : Mapping [ str , Any ] ) -> Any : max_rows = render_config . get ( \"max_no_rows\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_no_rows\" ] ) max_row_height = render_config . get ( \"max_row_height\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_row_height\" ] ) max_cell_length = render_config . get ( \"max_cell_length\" , DEFAULT_PRETTY_PRINT_CONFIG [ \"max_cell_length\" ] ) half_lines : Optional [ int ] = None if max_rows : half_lines = int ( max_rows / 2 ) atw = ArrowTabularWrap ( value . data . arrow_table ) result = atw . pretty_print ( rows_head = half_lines , rows_tail = half_lines , max_row_height = max_row_height , max_cell_length = max_cell_length , ) return result","title":"render_as__terminal_renderable()"},{"location":"reference/kiara_plugin/tabular/models/__init__/","text":"This module contains the metadata (and other) models that are used in the kiara_plugin.tabular package. Those models are convenience wrappers that make it easier for kiara to find, create, manage and version metadata -- but also other type of models -- that is attached to data, as well as kiara modules. Metadata models must be a sub-class of kiara.metadata.MetadataModel . Other models usually sub-class a pydantic BaseModel or implement custom base classes. Classes \u00b6 ColumnSchema ( BaseModel ) pydantic-model \u00b6 Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict ) Attributes \u00b6 metadata : Dict [ str , Any ] pydantic-field \u00b6 Other metadata for the column. type_name : str pydantic-field required \u00b6 The type name of the column (backend-specific). TableMetadata ( KiaraModel ) pydantic-model \u00b6 Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_id ( self ) -> str : return str ( self . model_data_hash ) def _retrieve_category_id ( self ) -> str : return \"instance.metadata.table\" def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , } Attributes \u00b6 column_names : List [ str ] pydantic-field required \u00b6 The name of the columns of the table. column_schema : Dict [ str , kiara_plugin . tabular . models . ColumnSchema ] pydantic-field required \u00b6 The schema description of the table. rows : int pydantic-field required \u00b6 The number of rows the table contains. size : int pydantic-field \u00b6 The tables size in bytes. Modules \u00b6 db \u00b6 Classes \u00b6 KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _table_schemas = PrivateAttr ( default = None ) _file_hash : Optional [ str ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return self . file_hash def _retrieve_category_id ( self ) -> str : return \"instance.database\" def _retrieve_data_to_hash ( self ) -> Any : return { \"file_hash\" : self . file_hash , } @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_hash ( self ) -> str : if self . _file_hash is not None : return self . _file_hash sha256_hash = hashlib . sha3_256 () with open ( self . db_file_path , \"rb\" ) as f : # Read and update hash string value in blocks of 4K for byte_block in iter ( lambda : f . read ( 4096 ), b \"\" ): sha256_hash . update ( byte_block ) self . _file_hash = sha256_hash . hexdigest () return self . _file_hash def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None self . _table_schemas = None self . _file_hash = None def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_hash : str property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () get_schema_for_table ( self , table_name ) \u00b6 Source code in tabular/models/db.py def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector table \u00b6 Classes \u00b6 KiaraArray ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return ARRAY_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist () KiaraTable ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return TABLE_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: return self . arrow_table . column_names @property def num_rows ( self ) -> int : return self . arrow_table . num_rows def to_pydict ( self ): return self . arrow_table . to_pydict () def to_pylist ( self ): return self . arrow_table . to_pylist () def to_pandas ( self ): return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 column_names : Iterable [ str ] property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 create_table ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Source code in tabular/models/table.py def to_pydict ( self ): return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"models"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema","text":"Describes properties of a single column of the 'table' data type. Source code in tabular/models/__init__.py class ColumnSchema ( BaseModel ): \"\"\"Describes properties of a single column of the 'table' data type.\"\"\" type_name : str = Field ( description = \"The type name of the column (backend-specific).\" ) metadata : Dict [ str , Any ] = Field ( description = \"Other metadata for the column.\" , default_factory = dict )","title":"ColumnSchema"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema.metadata","text":"Other metadata for the column.","title":"metadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.ColumnSchema.type_name","text":"The type name of the column (backend-specific).","title":"type_name"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata","text":"Describes properties for the 'table' data type. Source code in tabular/models/__init__.py class TableMetadata ( KiaraModel ): \"\"\"Describes properties for the 'table' data type.\"\"\" column_names : List [ str ] = Field ( description = \"The name of the columns of the table.\" ) column_schema : Dict [ str , ColumnSchema ] = Field ( description = \"The schema description of the table.\" ) rows : int = Field ( description = \"The number of rows the table contains.\" ) size : Optional [ int ] = Field ( description = \"The tables size in bytes.\" , default = None ) def _retrieve_id ( self ) -> str : return str ( self . model_data_hash ) def _retrieve_category_id ( self ) -> str : return \"instance.metadata.table\" def _retrieve_data_to_hash ( self ) -> Any : return { \"column_schemas\" : { k : v . dict () for k , v in self . column_schema . items ()}, \"rows\" : self . rows , \"size\" : self . size , }","title":"TableMetadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.column_names","text":"The name of the columns of the table.","title":"column_names"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.column_schema","text":"The schema description of the table.","title":"column_schema"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.rows","text":"The number of rows the table contains.","title":"rows"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.TableMetadata.size","text":"The tables size in bytes.","title":"size"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.KiaraDatabase","text":"Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _table_schemas = PrivateAttr ( default = None ) _file_hash : Optional [ str ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return self . file_hash def _retrieve_category_id ( self ) -> str : return \"instance.database\" def _retrieve_data_to_hash ( self ) -> Any : return { \"file_hash\" : self . file_hash , } @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_hash ( self ) -> str : if self . _file_hash is not None : return self . _file_hash sha256_hash = hashlib . sha3_256 () with open ( self . db_file_path , \"rb\" ) as f : # Read and update hash string value in blocks of 4K for byte_block in iter ( lambda : f . read ( 4096 ), b \"\" ): sha256_hash . update ( byte_block ) self . _file_hash = sha256_hash . hexdigest () return self . _file_hash def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None self . _table_schemas = None self . _file_hash = None def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ]","title":"KiaraDatabase"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.KiaraDatabase-attributes","text":"db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_hash : str property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.db.KiaraDatabase-methods","text":"copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () get_schema_for_table ( self , table_name ) \u00b6 Source code in tabular/models/db.py def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraArray","text":"Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return ARRAY_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas ()","title":"KiaraArray"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraArray-attributes","text":"arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist ()","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable","text":"Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return TABLE_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: return self . arrow_table . column_names @property def num_rows ( self ) -> int : return self . arrow_table . num_rows def to_pydict ( self ): return self . arrow_table . to_pydict () def to_pylist ( self ): return self . arrow_table . to_pylist () def to_pandas ( self ): return self . arrow_table . to_pandas ()","title":"KiaraTable"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTable-attributes","text":"arrow_table : Table property readonly \u00b6 column_names : Iterable [ str ] property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 create_table ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Source code in tabular/models/table.py def to_pydict ( self ): return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_table . to_pylist ()","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTableMetadata","text":"File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" )","title":"KiaraTableMetadata"},{"location":"reference/kiara_plugin/tabular/models/__init__/#kiara_plugin.tabular.models.table.KiaraTableMetadata-attributes","text":"table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/db/","text":"Classes \u00b6 KiaraDatabase ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _table_schemas = PrivateAttr ( default = None ) _file_hash : Optional [ str ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return self . file_hash def _retrieve_category_id ( self ) -> str : return \"instance.database\" def _retrieve_data_to_hash ( self ) -> Any : return { \"file_hash\" : self . file_hash , } @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_hash ( self ) -> str : if self . _file_hash is not None : return self . _file_hash sha256_hash = hashlib . sha3_256 () with open ( self . db_file_path , \"rb\" ) as f : # Read and update hash string value in blocks of 4K for byte_block in iter ( lambda : f . read ( 4096 ), b \"\" ): sha256_hash . update ( byte_block ) self . _file_hash = sha256_hash . hexdigest () return self . _file_hash def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None self . _table_schemas = None self . _file_hash = None def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] Attributes \u00b6 db_file_path : str pydantic-field required \u00b6 The path to the sqlite database file. db_url : str property readonly \u00b6 file_hash : str property readonly \u00b6 table_names : Iterable [ str ] property readonly \u00b6 Methods \u00b6 copy_database_file ( self , target ) \u00b6 Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db create_if_not_exists ( self ) \u00b6 Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) create_in_temp_dir ( init_statement = None , init_data = None ) classmethod \u00b6 Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db ensure_absolute_path ( path ) classmethod \u00b6 Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path execute_sql ( self , statement , data = None , invalidate = False ) \u00b6 Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () get_schema_for_table ( self , table_name ) \u00b6 Source code in tabular/models/db.py def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] get_sqlalchemy_engine ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine get_sqlalchemy_inspector ( self ) \u00b6 Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector","title":"db"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase","text":"Source code in tabular/models/db.py class KiaraDatabase ( KiaraModel ): @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db db_file_path : str = Field ( description = \"The path to the sqlite database file.\" ) _cached_engine = PrivateAttr ( default = None ) _cached_inspector = PrivateAttr ( default = None ) _table_names = PrivateAttr ( default = None ) _table_schemas = PrivateAttr ( default = None ) _file_hash : Optional [ str ] = PrivateAttr ( default = None ) _lock : bool = PrivateAttr ( default = True ) _immutable : bool = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return self . file_hash def _retrieve_category_id ( self ) -> str : return \"instance.database\" def _retrieve_data_to_hash ( self ) -> Any : return { \"file_hash\" : self . file_hash , } @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path @property def db_url ( self ) -> str : return f \"sqlite:/// { self . db_file_path } \" @property def file_hash ( self ) -> str : if self . _file_hash is not None : return self . _file_hash sha256_hash = hashlib . sha3_256 () with open ( self . db_file_path , \"rb\" ) as f : # Read and update hash string value in blocks of 4K for byte_block in iter ( lambda : f . read ( 4096 ), b \"\" ): sha256_hash . update ( byte_block ) self . _file_hash = sha256_hash . hexdigest () return self . _file_hash def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine def _lock_db ( self ): self . _lock = True self . _invalidate () def _unlock_db ( self ): if self . _immutable : raise Exception ( \"Can't unlock db, it's immutable.\" ) self . _lock = False self . _invalidate () def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url ) def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate () def _invalidate ( self ): self . _cached_engine = None self . _cached_inspector = None self . _table_names = None self . _table_schemas = None self . _file_hash = None def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector @property def table_names ( self ) -> Iterable [ str ]: if self . _table_names is not None : return self . _table_names self . _table_names = self . get_sqlalchemy_inspector () . get_table_names () return self . _table_names def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ]","title":"KiaraDatabase"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.db_file_path","text":"The path to the sqlite database file.","title":"db_file_path"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.db_url","text":"","title":"db_url"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.file_hash","text":"","title":"file_hash"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.table_names","text":"","title":"table_names"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.copy_database_file","text":"Source code in tabular/models/db.py def copy_database_file ( self , target : str ): os . makedirs ( os . path . dirname ( target )) shutil . copy2 ( self . db_file_path , target ) new_db = KiaraDatabase ( db_file_path = target ) if self . _file_hash : new_db . _file_hash = self . _file_hash return new_db","title":"copy_database_file()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.create_if_not_exists","text":"Source code in tabular/models/db.py def create_if_not_exists ( self ): from sqlalchemy_utils import create_database , database_exists if not database_exists ( self . db_url ): create_database ( self . db_url )","title":"create_if_not_exists()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.create_in_temp_dir","text":"Source code in tabular/models/db.py @classmethod def create_in_temp_dir ( cls , init_statement : Union [ None , str , \"TextClause\" ] = None , init_data : Optional [ Mapping [ str , Any ]] = None , ): temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = cls ( db_file_path = db_path ) db . create_if_not_exists () if init_statement : db . _unlock_db () db . execute_sql ( statement = init_statement , data = init_data , invalidate = True ) db . _lock_db () return db","title":"create_in_temp_dir()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.ensure_absolute_path","text":"Source code in tabular/models/db.py @validator ( \"db_file_path\" , allow_reuse = True ) def ensure_absolute_path ( cls , path : str ): path = os . path . abspath ( path ) if not os . path . exists ( os . path . dirname ( path )): raise ValueError ( f \"Parent folder for database file does not exist: { path } \" ) return path","title":"ensure_absolute_path()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.execute_sql","text":"Execute an sql script. Parameters: Name Type Description Default statement Union[str, TextClause] the sql statement required data Optional[Mapping[str, Any]] (optional) data, to be bound to the statement None invalidate bool whether to invalidate cached values within this object False Source code in tabular/models/db.py def execute_sql ( self , statement : Union [ str , \"TextClause\" ], data : Optional [ Mapping [ str , Any ]] = None , invalidate : bool = False , ): \"\"\"Execute an sql script. Arguments: statement: the sql statement data: (optional) data, to be bound to the statement invalidate: whether to invalidate cached values within this object \"\"\" from sqlalchemy import text if isinstance ( statement , str ): statement = text ( statement ) if data : statement . bindparams ( ** data ) self . create_if_not_exists () conn = self . get_sqlalchemy_engine () . raw_connection () conn . execute ( statement ) conn . commit () conn . close () if invalidate : self . _invalidate ()","title":"execute_sql()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_schema_for_table","text":"Source code in tabular/models/db.py def get_schema_for_table ( self , table_name : str ): if self . _table_schemas is not None : if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ] ts : Dict [ str , Dict [ str , Any ]] = {} inspector = self . get_sqlalchemy_inspector () for tn in inspector . get_table_names (): columns = self . get_sqlalchemy_inspector () . get_columns ( tn ) ts [ tn ] = {} for c in columns : ts [ tn ][ c [ \"name\" ]] = c self . _table_schemas = ts if table_name not in self . _table_schemas . keys (): raise Exception ( f \"Can't get table schema, database does not contain table with name ' { table_name } '.\" ) return self . _table_schemas [ table_name ]","title":"get_schema_for_table()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_sqlalchemy_engine","text":"Source code in tabular/models/db.py def get_sqlalchemy_engine ( self ) -> \"Engine\" : if self . _cached_engine is not None : return self . _cached_engine from sqlalchemy import create_engine def _pragma_on_connect ( dbapi_con , con_record ): dbapi_con . execute ( \"PRAGMA query_only = ON\" ) self . _cached_engine = create_engine ( self . db_url , future = True ) if self . _lock : from sqlalchemy import event event . listen ( self . _cached_engine , \"connect\" , _pragma_on_connect ) return self . _cached_engine","title":"get_sqlalchemy_engine()"},{"location":"reference/kiara_plugin/tabular/models/db/#kiara_plugin.tabular.models.db.KiaraDatabase.get_sqlalchemy_inspector","text":"Source code in tabular/models/db.py def get_sqlalchemy_inspector ( self ) -> \"Inspector\" : if self . _cached_inspector is not None : return self . _cached_inspector from sqlalchemy.inspection import inspect self . _cached_inspector = inspect ( self . get_sqlalchemy_engine ()) return self . _cached_inspector","title":"get_sqlalchemy_inspector()"},{"location":"reference/kiara_plugin/tabular/models/table/","text":"Classes \u00b6 KiaraArray ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return ARRAY_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas () Attributes \u00b6 arrow_array : Array property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. create_array ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist () KiaraTable ( KiaraModel ) pydantic-model \u00b6 Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return TABLE_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: return self . arrow_table . column_names @property def num_rows ( self ) -> int : return self . arrow_table . num_rows def to_pydict ( self ): return self . arrow_table . to_pydict () def to_pylist ( self ): return self . arrow_table . to_pylist () def to_pandas ( self ): return self . arrow_table . to_pandas () Attributes \u00b6 arrow_table : Table property readonly \u00b6 column_names : Iterable [ str ] property readonly \u00b6 data_path : str pydantic-field \u00b6 The path to the (feather) file backing this array. num_rows : int property readonly \u00b6 create_table ( data ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj to_pandas ( self ) \u00b6 Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_table . to_pandas () to_pydict ( self ) \u00b6 Source code in tabular/models/table.py def to_pydict ( self ): return self . arrow_table . to_pydict () to_pylist ( self ) \u00b6 Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_table . to_pylist () KiaraTableMetadata ( ValueMetadata ) pydantic-model \u00b6 File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" ) Attributes \u00b6 table : TableMetadata pydantic-field required \u00b6 The table schema. create_value_metadata ( value ) classmethod \u00b6 Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) retrieve_supported_data_types () classmethod \u00b6 Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"table"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray","text":"Source code in tabular/models/table.py class KiaraArray ( KiaraModel ): # @classmethod # def create_in_temp_dir(cls, ): # # temp_f = tempfile.mkdtemp() # file_path = os.path.join(temp_f, \"array.feather\") # # def cleanup(): # shutil.rmtree(file_path, ignore_errors=True) # # atexit.register(cleanup) # # array_obj = cls(feather_path=file_path) # return array_obj @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _array_obj : pa . Array = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return ARRAY_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_array ( self ) -> pa . Array : if self . _array_obj is not None : return self . _array_obj if not self . data_path : raise Exception ( \"Can't retrieve array data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () if len ( table . columns ) != 1 : raise Exception ( f \"Invalid serialized array data, only a single-column Table is allowed. This value is a table with { len ( table . columns ) } columns.\" ) self . _array_obj = table . column ( 0 ) return self . _array_obj def to_pylist ( self ): return self . arrow_array . to_pylist () def to_pandas ( self ): return self . arrow_array . to_pandas ()","title":"KiaraArray"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray.arrow_array","text":"","title":"arrow_array"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray.data_path","text":"The path to the (feather) file backing this array.","title":"data_path"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray.create_array","text":"Source code in tabular/models/table.py @classmethod def create_array ( cls , data : Any ) -> \"KiaraArray\" : if isinstance ( data , KiaraArray ): return data array_obj = None if isinstance ( data , ( pa . Array , pa . ChunkedArray )): array_obj = data elif isinstance ( data , pa . Table ): if len ( data . columns ) != 1 : raise Exception ( f \"Invalid type, only Arrow Arrays or single-column Tables allowed. This value is a table with { len ( data . columns ) } columns.\" ) array_obj = data . column ( 0 ) else : try : array_obj = pa . array ( data ) except Exception : pass if array_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraArray () obj . _array_obj = array_obj return obj","title":"create_array()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray.to_pandas","text":"Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_array . to_pandas ()","title":"to_pandas()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraArray.to_pylist","text":"Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_array . to_pylist ()","title":"to_pylist()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable","text":"Source code in tabular/models/table.py class KiaraTable ( KiaraModel ): @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj data_path : Optional [ str ] = Field ( description = \"The path to the (feather) file backing this array.\" ) _table_obj : pa . Table = PrivateAttr ( default = None ) def _retrieve_id ( self ) -> str : return str ( uuid . uuid4 ()) def _retrieve_category_id ( self ) -> str : return TABLE_MODEL_CATEOGORY_ID def _retrieve_data_to_hash ( self ) -> Any : raise NotImplementedError () @property def arrow_table ( self ) -> pa . Table : if self . _table_obj is not None : return self . _table_obj if not self . data_path : raise Exception ( \"Can't retrieve table data, object not initialized (yet).\" ) with pa . memory_map ( self . data_path , \"r\" ) as source : table : pa . Table = pa . ipc . open_file ( source ) . read_all () self . _table_obj = table return self . _table_obj @property def column_names ( self ) -> Iterable [ str ]: return self . arrow_table . column_names @property def num_rows ( self ) -> int : return self . arrow_table . num_rows def to_pydict ( self ): return self . arrow_table . to_pydict () def to_pylist ( self ): return self . arrow_table . to_pylist () def to_pandas ( self ): return self . arrow_table . to_pandas ()","title":"KiaraTable"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.arrow_table","text":"","title":"arrow_table"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.column_names","text":"","title":"column_names"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.data_path","text":"The path to the (feather) file backing this array.","title":"data_path"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.num_rows","text":"","title":"num_rows"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.create_table","text":"Source code in tabular/models/table.py @classmethod def create_table ( cls , data : Any ) -> \"KiaraTable\" : table_obj = None if isinstance ( data , KiaraTable ): return data if isinstance ( data , ( pa . Table )): table_obj = data else : try : table_obj = pa . table ( data ) except Exception : pass if table_obj is None : raise Exception ( f \"Can't create array, invalid source data type: { type ( data ) } .\" ) obj = KiaraTable () obj . _table_obj = table_obj return obj","title":"create_table()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.to_pandas","text":"Source code in tabular/models/table.py def to_pandas ( self ): return self . arrow_table . to_pandas ()","title":"to_pandas()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.to_pydict","text":"Source code in tabular/models/table.py def to_pydict ( self ): return self . arrow_table . to_pydict ()","title":"to_pydict()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTable.to_pylist","text":"Source code in tabular/models/table.py def to_pylist ( self ): return self . arrow_table . to_pylist ()","title":"to_pylist()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata","text":"File stats. Source code in tabular/models/table.py class KiaraTableMetadata ( ValueMetadata ): \"\"\"File stats.\"\"\" _metadata_key = \"table\" @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ] @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md ) table : TableMetadata = Field ( description = \"The table schema.\" )","title":"KiaraTableMetadata"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata.table","text":"The table schema.","title":"table"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata.create_value_metadata","text":"Source code in tabular/models/table.py @classmethod def create_value_metadata ( cls , value : \"Value\" ) -> \"TableMetadata\" : kiara_table : KiaraTable = value . data table : pa . Table = kiara_table . arrow_table table_schema = {} for name in table . schema . names : field = table . schema . field ( name ) md = field . metadata _type = field . type if not md : md = { \"arrow_type_id\" : _type . id , } _d = { \"type_name\" : str ( _type ), \"metadata\" : md , } table_schema [ name ] = _d schema = { \"column_names\" : table . column_names , \"column_schema\" : table_schema , \"rows\" : table . num_rows , \"size\" : table . nbytes , } md = TableMetadata ( ** schema ) return KiaraTableMetadata . construct ( table = md )","title":"create_value_metadata()"},{"location":"reference/kiara_plugin/tabular/models/table/#kiara_plugin.tabular.models.table.KiaraTableMetadata.retrieve_supported_data_types","text":"Source code in tabular/models/table.py @classmethod def retrieve_supported_data_types ( cls ) -> Iterable [ str ]: return [ \"table\" ]","title":"retrieve_supported_data_types()"},{"location":"reference/kiara_plugin/tabular/pipelines/__init__/","text":"Default (empty) module that is used as a base path for pipelines contained in this package.","title":"pipelines"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/","text":"Modules \u00b6 db \u00b6 Classes \u00b6 CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"database.create\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. create__database__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/tabular/db.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/tabular/db.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( KiaraModule ) \u00b6 Source code in tabular/tabular/db.py class LoadDatabaseFromDiskModule ( KiaraModule ): _module_type_name = \"database.load_from.disk\" def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/db.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/db.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/db.py def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db ) SaveDatabaseModule ( PersistValueModule ) \u00b6 Source code in tabular/tabular/db.py class SaveDatabaseModule ( PersistValueModule ): _module_type_name = \"database.save_to.disk\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure data_type__database ( self , value , persistence_config ) \u00b6 Source code in tabular/tabular/db.py def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure get_persistence_format_name ( self ) \u00b6 Source code in tabular/tabular/db.py def get_persistence_format_name ( self ) -> str : return \"arrays\" get_persistence_target_name ( self ) \u00b6 Source code in tabular/tabular/db.py def get_persistence_target_name ( self ) -> str : return \"disk\" table \u00b6 EMPTY_COLUMN_NAME_MARKER \u00b6 Classes \u00b6 CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/tabular/table.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"table.create\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. create__table__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/tabular/table.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__table__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/tabular/table.py def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/tabular/table.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) LoadTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , ) Attributes \u00b6 only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table. LoadTableFromDiskModule ( KiaraModule ) \u00b6 Source code in tabular/tabular/table.py class LoadTableFromDiskModule ( KiaraModule ): _module_type_name = \"table.load_from.disk\" _config_cls = LoadTableConfig def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , ) Attributes \u00b6 only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array ) SaveTableToDiskModule ( PersistValueModule ) \u00b6 Source code in tabular/tabular/table.py class SaveTableToDiskModule ( PersistValueModule ): _module_type_name = \"table.save_to.disk.as.feather\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def _store_array ( self , array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch ) Methods \u00b6 data_type__array ( self , value , persistence_config ) \u00b6 Source code in tabular/tabular/table.py def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure data_type__table ( self , value , persistence_config ) \u00b6 Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Source code in tabular/tabular/table.py def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure get_persistence_format_name ( self ) \u00b6 Source code in tabular/tabular/table.py def get_persistence_format_name ( self ) -> str : return \"arrays\" get_persistence_target_name ( self ) \u00b6 Source code in tabular/tabular/table.py def get_persistence_target_name ( self ) -> str : return \"disk\"","title":"tabular"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular-modules","text":"","title":"Modules"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.db","text":"","title":"db"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.db.CreateDatabaseModule","text":"Source code in tabular/tabular/db.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"database.create\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path","title":"CreateDatabaseModule"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.db.CreateDatabaseModule-classes","text":"_config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. create__database__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/tabular/db.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/tabular/db.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path","title":"Classes"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.db.CreateDatabaseModuleConfig","text":"Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , )","title":"CreateDatabaseModuleConfig"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.db.CreateDatabaseModuleConfig-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.db.LoadDatabaseFromDiskModule","text":"Source code in tabular/tabular/db.py class LoadDatabaseFromDiskModule ( KiaraModule ): _module_type_name = \"database.load_from.disk\" def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db )","title":"LoadDatabaseFromDiskModule"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.db.LoadDatabaseFromDiskModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/db.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/db.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/db.py def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.db.SaveDatabaseModule","text":"Source code in tabular/tabular/db.py class SaveDatabaseModule ( PersistValueModule ): _module_type_name = \"database.save_to.disk\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure data_type__database ( self , value , persistence_config ) \u00b6 Source code in tabular/tabular/db.py def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure get_persistence_format_name ( self ) \u00b6 Source code in tabular/tabular/db.py def get_persistence_format_name ( self ) -> str : return \"arrays\" get_persistence_target_name ( self ) \u00b6 Source code in tabular/tabular/db.py def get_persistence_target_name ( self ) -> str : return \"disk\"","title":"SaveDatabaseModule"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table","text":"","title":"table"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.EMPTY_COLUMN_NAME_MARKER","text":"","title":"EMPTY_COLUMN_NAME_MARKER"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.CreateTableModule","text":"Source code in tabular/tabular/table.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"table.create\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"CreateTableModule"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.CreateTableModule-classes","text":"_config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. create__table__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/tabular/table.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__table__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/tabular/table.py def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"Classes"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.CreateTableModuleConfig","text":"Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , )","title":"CreateTableModuleConfig"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.CreateTableModuleConfig-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.CutColumnModule","text":"Cut off one column from a table, returning an array. Source code in tabular/tabular/table.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"CutColumnModule"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.CutColumnModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.LoadTableConfig","text":"Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , )","title":"LoadTableConfig"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.LoadTableConfig-attributes","text":"only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule","text":"Source code in tabular/tabular/table.py class LoadTableFromDiskModule ( KiaraModule ): _module_type_name = \"table.load_from.disk\" _config_cls = LoadTableConfig def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array )","title":"LoadTableFromDiskModule"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule-classes","text":"_config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , ) Attributes \u00b6 only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table.","title":"Classes"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule-methods","text":"create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array )","title":"Methods"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.SaveTableToDiskModule","text":"Source code in tabular/tabular/table.py class SaveTableToDiskModule ( PersistValueModule ): _module_type_name = \"table.save_to.disk.as.feather\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def _store_array ( self , array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch )","title":"SaveTableToDiskModule"},{"location":"reference/kiara_plugin/tabular/tabular/__init__/#kiara_plugin.tabular.tabular.table.SaveTableToDiskModule-methods","text":"data_type__array ( self , value , persistence_config ) \u00b6 Source code in tabular/tabular/table.py def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure data_type__table ( self , value , persistence_config ) \u00b6 Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Source code in tabular/tabular/table.py def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure get_persistence_format_name ( self ) \u00b6 Source code in tabular/tabular/table.py def get_persistence_format_name ( self ) -> str : return \"arrays\" get_persistence_target_name ( self ) \u00b6 Source code in tabular/tabular/table.py def get_persistence_target_name ( self ) -> str : return \"disk\"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/tabular/db/","text":"Classes \u00b6 CreateDatabaseModule ( CreateFromModule ) \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"database.create\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. create__database__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/tabular/db.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__database__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/tabular/db.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path CreateDatabaseModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table. LoadDatabaseFromDiskModule ( KiaraModule ) \u00b6 Source code in tabular/tabular/db.py class LoadDatabaseFromDiskModule ( KiaraModule ): _module_type_name = \"database.load_from.disk\" def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/db.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/db.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/db.py def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db ) SaveDatabaseModule ( PersistValueModule ) \u00b6 Source code in tabular/tabular/db.py class SaveDatabaseModule ( PersistValueModule ): _module_type_name = \"database.save_to.disk\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure data_type__database ( self , value , persistence_config ) \u00b6 Source code in tabular/tabular/db.py def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure get_persistence_format_name ( self ) \u00b6 Source code in tabular/tabular/db.py def get_persistence_format_name ( self ) -> str : return \"arrays\" get_persistence_target_name ( self ) \u00b6 Source code in tabular/tabular/db.py def get_persistence_target_name ( self ) -> str : return \"disk\"","title":"db"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModule","text":"Source code in tabular/tabular/db.py class CreateDatabaseModule ( CreateFromModule ): _module_type_name = \"database.create\" _config_cls = CreateDatabaseModuleConfig def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path","title":"CreateDatabaseModule"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModule._config_cls","text":"Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModule._config_cls-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. include_source_file_content : bool pydantic-field \u00b6 When including source metadata, whether to also include the original raw (string) content. include_source_metadata : bool pydantic-field \u00b6 Whether to include a table with metadata about the source files. merge_into_single_table : bool pydantic-field \u00b6 Whether to merge all csv files into a single table.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModule.create__database__from__csv_file","text":"Source code in tabular/tabular/db.py def create__database__from__csv_file ( self , source_value : Value ) -> Any : raise NotImplementedError () from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data","title":"create__database__from__csv_file()"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModule.create__database__from__csv_file_bundle","text":"Source code in tabular/tabular/db.py def create__database__from__csv_file_bundle ( self , source_value : Value ) -> Any : merge_into_single_table = self . get_config_value ( \"merge_into_single_table\" ) if merge_into_single_table : raise NotImplementedError ( \"Not supported (yet).\" ) include_raw_content_in_file_info : bool = self . get_config_value ( \"include_source_metadata\" ) temp_f = tempfile . mkdtemp () db_path = os . path . join ( temp_f , \"db.sqlite\" ) def cleanup (): shutil . rmtree ( db_path , ignore_errors = True ) atexit . register ( cleanup ) db = KiaraDatabase ( db_file_path = db_path ) db . create_if_not_exists () # TODO: check whether/how to add indexes bundle : FileBundle = source_value . data table_names : List [ str ] = [] for rel_path in sorted ( bundle . included_files . keys ()): file_item = bundle . included_files [ rel_path ] table_name = find_free_id ( stem = file_item . file_name_without_extension , current_ids = table_names ) try : table_names . append ( table_name ) create_sqlite_table_from_tabular_file ( target_db_file = db_path , file_item = file_item , table_name = table_name ) except Exception as e : if self . get_config_value ( \"ignore_errors\" ) is True or True : log_message ( \"ignore.import_file\" , file = rel_path , reason = str ( e )) continue raise KiaraProcessingException ( e ) if include_raw_content_in_file_info : include_content : bool = self . get_config_value ( \"include_source_file_content\" ) db . _unlock_db () insert_db_table_from_file_bundle ( database = db , file_bundle = source_value . data , table_name = \"source_files_metadata\" , include_content = include_content , ) db . _lock_db () return db_path","title":"create__database__from__csv_file_bundle()"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModuleConfig","text":"Source code in tabular/tabular/db.py class CreateDatabaseModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) merge_into_single_table : bool = Field ( description = \"Whether to merge all csv files into a single table.\" , default = False ) include_source_metadata : bool = Field ( description = \"Whether to include a table with metadata about the source files.\" , default = True , ) include_source_file_content : bool = Field ( description = \"When including source metadata, whether to also include the original raw (string) content.\" , default = False , )","title":"CreateDatabaseModuleConfig"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModuleConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModuleConfig.ignore_errors","text":"Whether to ignore convert errors and omit the failed items.","title":"ignore_errors"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModuleConfig.include_source_file_content","text":"When including source metadata, whether to also include the original raw (string) content.","title":"include_source_file_content"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModuleConfig.include_source_metadata","text":"Whether to include a table with metadata about the source files.","title":"include_source_metadata"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.CreateDatabaseModuleConfig.merge_into_single_table","text":"Whether to merge all csv files into a single table.","title":"merge_into_single_table"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.LoadDatabaseFromDiskModule","text":"Source code in tabular/tabular/db.py class LoadDatabaseFromDiskModule ( KiaraModule ): _module_type_name = \"database.load_from.disk\" def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db )","title":"LoadDatabaseFromDiskModule"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.LoadDatabaseFromDiskModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.LoadDatabaseFromDiskModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/tabular/db.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.LoadDatabaseFromDiskModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/tabular/db.py def create_outputs_schema ( self , ) -> ValueSetSchema : return { \"database\" : { \"type\" : \"database\" , \"doc\" : \"The database.\" }}","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.LoadDatabaseFromDiskModule.process","text":"Source code in tabular/tabular/db.py def process ( self , inputs : ValueMap , outputs : ValueMap ): bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) db_file = bytes_structure . chunk_map [ \"db.sqlite\" ] assert len ( db_file ) == 1 db = KiaraDatabase ( db_file_path = db_file [ 0 ]) db . _immutable = True outputs . set_value ( \"database\" , db )","title":"process()"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.SaveDatabaseModule","text":"Source code in tabular/tabular/db.py class SaveDatabaseModule ( PersistValueModule ): _module_type_name = \"database.save_to.disk\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure","title":"SaveDatabaseModule"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.SaveDatabaseModule.data_type__database","text":"Source code in tabular/tabular/db.py def data_type__database ( self , value : Value , persistence_config : Mapping [ str , Any ]): db : KiaraDatabase = value . data # type: ignore db . _lock_db () # TODO: assert type inherits from database? chunk_map = {} chunk_map [ \"db.sqlite\" ] = [ db . db_file_path ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"database.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig . construct ( ** load_config_data ) return load_config , bytes_structure","title":"data_type__database()"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.SaveDatabaseModule.get_persistence_format_name","text":"Source code in tabular/tabular/db.py def get_persistence_format_name ( self ) -> str : return \"arrays\"","title":"get_persistence_format_name()"},{"location":"reference/kiara_plugin/tabular/tabular/db/#kiara_plugin.tabular.tabular.db.SaveDatabaseModule.get_persistence_target_name","text":"Source code in tabular/tabular/db.py def get_persistence_target_name ( self ) -> str : return \"disk\"","title":"get_persistence_target_name()"},{"location":"reference/kiara_plugin/tabular/tabular/table/","text":"EMPTY_COLUMN_NAME_MARKER \u00b6 Classes \u00b6 CreateTableModule ( CreateFromModule ) \u00b6 Source code in tabular/tabular/table.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"table.create\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) Classes \u00b6 _config_cls ( CreateFromModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. create__table__from__csv_file ( self , source_value ) \u00b6 Source code in tabular/tabular/table.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data create__table__from__csv_file_bundle ( self , source_value ) \u00b6 Source code in tabular/tabular/table.py def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table ) CreateTableModuleConfig ( CreateFromModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , ) Attributes \u00b6 ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items. CutColumnModule ( KiaraModule ) \u00b6 Cut off one column from a table, returning an array. Source code in tabular/tabular/table.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column ) LoadTableConfig ( KiaraModuleConfig ) pydantic-model \u00b6 Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , ) Attributes \u00b6 only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table. LoadTableFromDiskModule ( KiaraModule ) \u00b6 Source code in tabular/tabular/table.py class LoadTableFromDiskModule ( KiaraModule ): _module_type_name = \"table.load_from.disk\" _config_cls = LoadTableConfig def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array ) Classes \u00b6 _config_cls ( KiaraModuleConfig ) private pydantic-model \u00b6 Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , ) Attributes \u00b6 only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table. Methods \u00b6 create_inputs_schema ( self ) \u00b6 Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs create_outputs_schema ( self ) \u00b6 Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} process ( self , inputs , outputs ) \u00b6 Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array ) SaveTableToDiskModule ( PersistValueModule ) \u00b6 Source code in tabular/tabular/table.py class SaveTableToDiskModule ( PersistValueModule ): _module_type_name = \"table.save_to.disk.as.feather\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def _store_array ( self , array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch ) Methods \u00b6 data_type__array ( self , value , persistence_config ) \u00b6 Source code in tabular/tabular/table.py def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure data_type__table ( self , value , persistence_config ) \u00b6 Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Source code in tabular/tabular/table.py def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure get_persistence_format_name ( self ) \u00b6 Source code in tabular/tabular/table.py def get_persistence_format_name ( self ) -> str : return \"arrays\" get_persistence_target_name ( self ) \u00b6 Source code in tabular/tabular/table.py def get_persistence_target_name ( self ) -> str : return \"disk\"","title":"table"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.EMPTY_COLUMN_NAME_MARKER","text":"","title":"EMPTY_COLUMN_NAME_MARKER"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CreateTableModule","text":"Source code in tabular/tabular/table.py class CreateTableModule ( CreateFromModule ): _module_type_name = \"table.create\" _config_cls = CreateTableModuleConfig def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"CreateTableModule"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CreateTableModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CreateTableModule._config_cls","text":"Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CreateTableModule._config_cls-attributes","text":"ignore_errors : bool pydantic-field \u00b6 Whether to ignore convert errors and omit the failed items.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CreateTableModule.create__table__from__csv_file","text":"Source code in tabular/tabular/table.py def create__table__from__csv_file ( self , source_value : Value ) -> Any : from pyarrow import csv input_file : FileModel = source_value . data imported_data = csv . read_csv ( input_file . path ) return imported_data","title":"create__table__from__csv_file()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CreateTableModule.create__table__from__csv_file_bundle","text":"Source code in tabular/tabular/table.py def create__table__from__csv_file_bundle ( self , source_value : Value ) -> Any : import pyarrow as pa bundle : FileBundle = source_value . data columns = FILE_BUNDLE_IMPORT_AVAILABLE_COLUMNS ignore_errors = self . get_config_value ( \"ignore_errors\" ) file_dict = bundle . read_text_file_contents ( ignore_errors = ignore_errors ) # TODO: use chunks to save on memory tabular : Dict [ str , List [ Any ]] = {} for column in columns : for index , rel_path in enumerate ( sorted ( file_dict . keys ())): if column == \"content\" : _value : Any = file_dict [ rel_path ] elif column == \"id\" : _value = index elif column == \"rel_path\" : _value = rel_path else : file_model = bundle . included_files [ rel_path ] _value = getattr ( file_model , column ) tabular . setdefault ( column , []) . append ( _value ) table = pa . Table . from_pydict ( tabular ) return KiaraTable . create_table ( table )","title":"create__table__from__csv_file_bundle()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CreateTableModuleConfig","text":"Source code in tabular/tabular/table.py class CreateTableModuleConfig ( CreateFromModuleConfig ): ignore_errors : bool = Field ( description = \"Whether to ignore convert errors and omit the failed items.\" , default = False , )","title":"CreateTableModuleConfig"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CreateTableModuleConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CreateTableModuleConfig.ignore_errors","text":"Whether to ignore convert errors and omit the failed items.","title":"ignore_errors"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CutColumnModule","text":"Cut off one column from a table, returning an array. Source code in tabular/tabular/table.py class CutColumnModule ( KiaraModule ): \"\"\"Cut off one column from a table, returning an array.\"\"\" _module_type_name = \"table.cut_column\" def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"CutColumnModule"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CutColumnModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CutColumnModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs : Mapping [ str , Any ] = { \"table\" : { \"type\" : \"table\" , \"doc\" : \"A table.\" }, \"column_name\" : { \"type\" : \"string\" , \"doc\" : \"The name of the column to extract.\" , }, } return inputs","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CutColumnModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : outputs : Mapping [ str , Any ] = { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The column.\" }} return outputs","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.CutColumnModule.process","text":"Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ) -> None : import pyarrow as pa column_name : str = inputs . get_value_data ( \"column_name\" ) table_value : Value = inputs . get_value_obj ( \"table\" ) table_metadata : KiaraTableMetadata = table_value . get_property_data ( \"metadata.table\" ) available = table_metadata . table . column_names if column_name not in available : raise KiaraProcessingException ( f \"Invalid column name ' { column_name } '. Available column names: { ', ' . join ( available ) } \" ) table : pa . Table = table_value . data . arrow_table column = table . column ( column_name ) outputs . set_value ( \"array\" , column )","title":"process()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableConfig","text":"Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , )","title":"LoadTableConfig"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableConfig-attributes","text":"","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableConfig.only_column","text":"Whether to only load a single column instead of the whole table.","title":"only_column"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule","text":"Source code in tabular/tabular/table.py class LoadTableFromDiskModule ( KiaraModule ): _module_type_name = \"table.load_from.disk\" _config_cls = LoadTableConfig def _retrieve_module_characteristics ( self ) -> ModuleCharacteristics : return ModuleCharacteristics ( is_internal = True ) def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }} def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array )","title":"LoadTableFromDiskModule"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule-classes","text":"","title":"Classes"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule._config_cls","text":"Source code in tabular/tabular/table.py class LoadTableConfig ( KiaraModuleConfig ): only_column : Optional [ str ] = Field ( description = \"Whether to only load a single column instead of the whole table.\" , default = None , )","title":"_config_cls"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule._config_cls-attributes","text":"only_column : str pydantic-field \u00b6 Whether to only load a single column instead of the whole table.","title":"Attributes"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule.create_inputs_schema","text":"Return the schema for this types' inputs. Source code in tabular/tabular/table.py def create_inputs_schema ( self , ) -> ValueSetSchema : inputs = { \"bytes_structure\" : { \"type\" : \"any\" , \"doc\" : \"The bytes.\" }} return inputs","title":"create_inputs_schema()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule.create_outputs_schema","text":"Return the schema for this types' outputs. Source code in tabular/tabular/table.py def create_outputs_schema ( self , ) -> ValueSetSchema : if not self . get_config_value ( \"only_column\" ): return { \"table\" : { \"type\" : \"table\" , \"doc\" : \"The table.\" }} else : return { \"array\" : { \"type\" : \"array\" , \"doc\" : \"The array.\" }}","title":"create_outputs_schema()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.LoadTableFromDiskModule.process","text":"Source code in tabular/tabular/table.py def process ( self , inputs : ValueMap , outputs : ValueMap ): import pyarrow as pa bytes_structure : BytesStructure = inputs . get_value_data ( \"bytes_structure\" ) if not self . get_config_value ( \"only_column\" ): columns = {} for column_name , chunks in bytes_structure . chunk_map . items (): assert len ( chunks ) == 1 with pa . memory_map ( chunks [ 0 ], \"r\" ) as column_chunk : loaded_arrays : pa . Table = pa . ipc . open_file ( column_chunk ) . read_all () column = loaded_arrays . column ( column_name ) if column_name == EMPTY_COLUMN_NAME_MARKER : columns [ \"\" ] = column else : columns [ column_name ] = column arrow_table = pa . table ( columns ) table = KiaraTable . create_table ( arrow_table ) outputs . set_value ( \"table\" , table ) else : chunks = bytes_structure . chunk_map [ \"array.arrow\" ] assert len ( chunks ) == 1 array_file = chunks [ 0 ] # with pa.memory_map(array_file, \"r\") as column_chunk: # loaded_arrays = pa.ipc.open_file(column_chunk).read_all() # column = loaded_arrays.column(\"array\") # # array = KiaraArray.create_array(column) array = KiaraArray ( data_path = array_file ) outputs . set_value ( \"array\" , array )","title":"process()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.SaveTableToDiskModule","text":"Source code in tabular/tabular/table.py class SaveTableToDiskModule ( PersistValueModule ): _module_type_name = \"table.save_to.disk.as.feather\" def get_persistence_target_name ( self ) -> str : return \"disk\" def get_persistence_format_name ( self ) -> str : return \"arrays\" def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure def _store_array ( self , array_obj : \"pa.Array\" , file_name : str , column_name : \"str\" = \"array\" ): import pyarrow as pa schema = pa . schema ([ pa . field ( column_name , array_obj . type )]) # TODO: support non-single chunk columns with pa . OSFile ( file_name , \"wb\" ) as sink : with pa . ipc . new_file ( sink , schema = schema ) as writer : batch = pa . record_batch ( array_obj . chunks , schema = schema ) writer . write ( batch )","title":"SaveTableToDiskModule"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.SaveTableToDiskModule-methods","text":"","title":"Methods"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.SaveTableToDiskModule.data_type__array","text":"Source code in tabular/tabular/table.py def data_type__array ( self , value : Value , persistence_config : Mapping [ str , Any ]): import pyarrow as pa kiara_array : KiaraArray = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) column : pa . Array = kiara_array . arrow_array file_name = os . path . join ( temp_f , \"array.arrow\" ) self . _store_array ( array_obj = column , file_name = file_name , column_name = \"array\" ) chunk_map [ \"array.arrow\" ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"module_config\" : { \"only_column\" : \"array\" }, \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER , }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure","title":"data_type__array()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.SaveTableToDiskModule.data_type__table","text":"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. Source code in tabular/tabular/table.py def data_type__table ( self , value : Value , persistence_config : Mapping [ str , Any ] ) -> Tuple [ LoadConfig , Optional [ BytesStructure ]]: \"\"\"Store the table as Apache Arrow feather file The table will be store with one feather file per column, to support de-duplicated storage of re-arranged tables. \"\"\" import pyarrow as pa table : KiaraTable = value . data chunk_map = {} # TODO: make sure temp dir is in the same partition as file store temp_f = tempfile . mkdtemp () def cleanup (): shutil . rmtree ( temp_f , ignore_errors = True ) atexit . register ( cleanup ) for column_name in table . arrow_table . column_names : column : pa . Array = table . arrow_table . column ( column_name ) if column_name == \"\" : file_name = os . path . join ( temp_f , EMPTY_COLUMN_NAME_MARKER ) else : file_name = os . path . join ( temp_f , column_name ) self . _store_array ( array_obj = column , file_name = file_name , column_name = column_name ) chunk_map [ column_name ] = [ file_name ] bytes_structure_data : Dict [ str , Any ] = { \"data_type\" : value . value_schema . type , \"data_type_config\" : value . value_schema . type_config , \"chunk_map\" : chunk_map , } bytes_structure = BytesStructure . construct ( ** bytes_structure_data ) load_config_data = { \"provisioning_strategy\" : ByteProvisioningStrategy . FILE_PATH_MAP , \"module_type\" : \"table.load_from.disk\" , \"inputs\" : { \"bytes_structure\" : LOAD_CONFIG_PLACEHOLDER }, \"output_name\" : value . value_schema . type , } load_config = LoadConfig ( ** load_config_data ) return load_config , bytes_structure","title":"data_type__table()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.SaveTableToDiskModule.get_persistence_format_name","text":"Source code in tabular/tabular/table.py def get_persistence_format_name ( self ) -> str : return \"arrays\"","title":"get_persistence_format_name()"},{"location":"reference/kiara_plugin/tabular/tabular/table/#kiara_plugin.tabular.tabular.table.SaveTableToDiskModule.get_persistence_target_name","text":"Source code in tabular/tabular/table.py def get_persistence_target_name ( self ) -> str : return \"disk\"","title":"get_persistence_target_name()"}]}